{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "En este notebook vas a ver un ejemplo de los procesos necesarios para realizar un análisis de sentimientos sobre Tweets. Para ello tendremos que seguir los siguientes pasos:\n",
    "1. Conseguir un Corpus: no es más que una base de datos de texto etiquetado\n",
    "2. Limpiar los datos\n",
    "3. Entrenar un modelo con el corpus\n",
    "4. Cargar un lexicon sentimental\n",
    "5. Hacer un modelo no_supervisado basado en el lexicon\n",
    "6. Compararlo con el modelo anteriormente entrenado\n",
    "\n",
    "**Estos programas son muy útiles en campañas de marketing, para monitorizar el lanzamiento de un nuevo producto, realizar seguimiento en Twitter de eventos, o simplemente tener monitorizadas ciertas cuentas o hashtags para tener un programa de análisis real time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "Para conseguir el corpus tendremos que registrarnos en la [página del TASS](http://tass.sepln.org/tass_data/download.php), que se trata de una asociación de análisis semántico que encargada de recopilar texto y mantenerlo etiquetado. \n",
    "\n",
    "Para datasets en ingles lo tenemos más fácil ya que con librerías como [TextBlob](https://textblob.readthedocs.io/en/dev/) podemos predecir directamente la polaridad del Tweet, con modelos ya preentrenados. En el caso del castellano necesitamos acudir a un corpus etiquetado para entrenar nuestro modelo.\n",
    "\n",
    "Registrate en el TASS y accede a sus corpus a través de un link que te llegará al correo tras el registro.\n",
    "\n",
    "![imagen](img/tass_register.png)\n",
    "\n",
    "\n",
    "Una vez estes registrado, descárgate el corpus de tweets en español de entrenamiento. En este punto lo ideal es coger un corpus que se adapte lo máximo posible a los tipos de tweets que intentamos predecir, es decir, si queremos predecir tweets sobre política, procurar elegir un corpus que tenga vocabulario de política.\n",
    "\n",
    "En este notebook se va a elegir un corpus genérico con no demasiados registros para aligerar la limpieza y entrenamiento de los modelos.\n",
    "\n",
    "![imagen](img/download_train_spanish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/general-train-tagged.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7219, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccifuentes</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>2011-12-02T00:47:55</td>\n",
       "      <td>es</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>2011-12-02T00:49:40</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Content  \\\n",
       "0      ccifuentes        Salgo de #VeoTV , que día más largoooooo...   \n",
       "1  CarmendelRiego  @PauladeLasHeras No te libraras de ayudar me/n...   \n",
       "2  CarmendelRiego                          @marodriguezb Gracias MAR   \n",
       "3    mgilguerrero  Off pensando en el regalito Sinde, la que se v...   \n",
       "4        paurubio  Conozco a alguien q es adicto al drama! Ja ja ...   \n",
       "\n",
       "                  Date Lang Polarity          Type  \n",
       "0  2011-12-02T00:47:55   es     NONE     AGREEMENT  \n",
       "1  2011-12-02T00:49:40   es      NEU  DISAGREEMENT  \n",
       "2  2011-12-02T00:57:40   es        P     AGREEMENT  \n",
       "3  2011-12-02T02:33:37   es       N+     AGREEMENT  \n",
       "4  2011-12-02T02:59:03   es       P+     AGREEMENT  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dict = {\n",
    "    'User': [],\n",
    "    'Content': [],\n",
    "    'Date': [],\n",
    "    'Lang': [],\n",
    "    'Polarity': [],\n",
    "    'Type': []\n",
    "}\n",
    "\n",
    "for i in root.iter('tweet'):\n",
    "    user = i.find('user').text\n",
    "    content = i.find('content').text\n",
    "    date = i.find('date').text\n",
    "    lang = i.find('lang').text\n",
    "    polarity = i.find('sentiments').find('polarity').find('value').text\n",
    "    tweet_type = i.find('sentiments').find('polarity').find('type').text\n",
    "    \n",
    "    raw_dict['User'].append(user)\n",
    "    raw_dict['Content'].append(content)\n",
    "    raw_dict['Date'].append(date)\n",
    "    raw_dict['Lang'].append(lang)\n",
    "    raw_dict['Polarity'].append(polarity)\n",
    "    raw_dict['Type'].append(tweet_type)\n",
    "    \n",
    "df = pd.DataFrame(raw_dict)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna de polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'NEU', 'P', 'N+', 'P+', 'N'], dtype=object)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos los valores unicos de la columna de polaridad\n",
    "df.Polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV9UlEQVR4nO3dfZBldX3n8fdHRkhMojxMizgzu0PJxBS6RrEX2RATDK6CUccYdaF8GAjZibv4EHVV1K1AjFRpjAGfwtZERhjXSNAYHVOzyyKKEleQRg0C6toFKjMF0gqi5RMZ/e4f9zdybbrn3Onpe283/X5V3epzvud3z/2eusCH83DPSVUhSdLePGDcDUiSlj7DQpLUybCQJHUyLCRJnQwLSVKnVeNuYBhWr15d69evH3cbkrSsXHfddd+uqom5lt0vw2L9+vVMTU2Nuw1JWlaSfGO+ZR6GkiR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHW6X/6CW9LS8q5XfWzcLeyzl7ztGeNuYUlxz0KS1MmwkCR1GlpYJNma5I4kN8yqvzTJV5LcmOQv++qvSzKd5KtJntpXP6nVppOcNax+JUnzG+Y5i4uAdwHb9hSSPAnYCPxmVf0kyUNb/WjgFOBRwMOBjyf59fa2dwP/EdgJXJtke1XdNMS+JUmzDC0squrTSdbPKv8X4M1V9ZM25o5W3whc0uq3JJkGjm3LpqvqZoAkl7SxhoUkjdCoz1n8OvDEJNck+VSSf9/qa4Bb+8btbLX56veRZHOSqSRTMzMzQ2hdklauUYfFKuBQ4Djg1cClSbIYK66qLVU1WVWTExNzPuhJkrRAo/6dxU7gw1VVwOeS/AxYDewC1vWNW9tq7KUuSRqRUe9ZfAR4EkA7gX0g8G1gO3BKkoOSHAlsAD4HXAtsSHJkkgPpnQTfPuKeJWnFG9qeRZIPACcAq5PsBM4GtgJb2+W09wCb2l7GjUkupXfiejdwZlX9tK3nJcBlwAHA1qq6cVg9S5LmNsyroU6dZ9EL5hl/LnDuHPUdwI5FbE2StI/8BbckqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSp06hvJDhWj3/1tu5BS8x1b33RuFuQJPcsJEndDAtJUifDQpLUybCQJHUyLCRJnQwLSVKnoYVFkq1J7mhPxZu97FVJKsnqNp8k70gyneT6JMf0jd2U5GvttWlY/UqS5jfMPYuLgJNmF5OsA54CfLOvfDK9525vADYDF7Sxh9J7HOsTgGOBs5McMsSeJUlzGFpYVNWngTvnWHQe8Bqg+mobgW3VczVwcJIjgKcCl1fVnVV1F3A5cwSQJGm4RnrOIslGYFdV/cusRWuAW/vmd7bafPW51r05yVSSqZmZmUXsWpI0srBI8iDg9cCfDWP9VbWlqiaranJiYmIYHyFJK9Yo9yweARwJ/EuSrwNrgc8neRiwC1jXN3Ztq81XlySN0MjCoqq+VFUPrar1VbWe3iGlY6rqdmA78KJ2VdRxwN1VdRtwGfCUJIe0E9tPaTVJ0ggN89LZDwCfBR6ZZGeSM/YyfAdwMzAN/C3wXwGq6k7gL4Br2+uNrSZJGqGh3aK8qk7tWL6+b7qAM+cZtxXYuqjNSZL2ib/gliR1MiwkSZ1W1JPyJGkYzn3Bc8bdwj57w//80D6Nd89CktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ28kqGXj+HceP+4W9slnXvqZcbcgLZphPilva5I7ktzQV3trkq8kuT7JPyY5uG/Z65JMJ/lqkqf21U9qtekkZw2rX0nS/IZ5GOoi4KRZtcuBR1fVY4D/B7wOIMnRwCnAo9p7/ibJAUkOAN4NnAwcDZzaxkqSRmhoYVFVnwbunFX7P1W1u81eDaxt0xuBS6rqJ1V1C71ncR/bXtNVdXNV3QNc0sZKkkZonCe4/wj4X216DXBr37KdrTZf/T6SbE4ylWRqZmZmCO1K0so1lrBI8gZgN/D+xVpnVW2pqsmqmpyYmFis1UqSGMPVUElOA54OnFhV1cq7gHV9w9a2GnupS5JGZKR7FklOAl4DPLOqfti3aDtwSpKDkhwJbAA+B1wLbEhyZJID6Z0E3z7KniVJQ9yzSPIB4ARgdZKdwNn0rn46CLg8CcDVVfXiqroxyaXATfQOT51ZVT9t63kJcBlwALC1qm4cVs+SpLkNLSyq6tQ5yhfuZfy5wLlz1HcAOxaxNUnSPvJ2H5KkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6DS0skmxNckeSG/pqhya5PMnX2t9DWj1J3pFkOsn1SY7pe8+mNv5rSTYNq19J0vyGuWdxEXDSrNpZwBVVtQG4os0DnEzvudsbgM3ABdALF3qPY30CcCxw9p6AkSSNztDCoqo+Ddw5q7wRuLhNXww8q6++rXquBg5OcgTwVODyqrqzqu4CLue+ASRJGrJRn7M4vKpua9O3A4e36TXArX3jdrbafPX7SLI5yVSSqZmZmcXtWpJWuLGd4K6qAmoR17elqiaranJiYmKxVitJYvRh8a12eIn2945W3wWs6xu3ttXmq0uSRmjUYbEd2HNF0ybgo331F7Wroo4D7m6Hqy4DnpLkkHZi+ymtJkkaoVXDWnGSDwAnAKuT7KR3VdObgUuTnAF8A3heG74DeBowDfwQOB2gqu5M8hfAtW3cG6tq9klzSdKQDS0squrUeRadOMfYAs6cZz1bga2L2JokaR8NdBgqyRWD1CRJ90973bNI8kvAg+gdSjoESFv0YOa5hFWSdP/TdRjqT4A/BR4OXMe9YfE94F3Da0uStJTsNSyq6u3A25O8tKreOaKeJElLzEAnuKvqnUl+C1jf/56q2jakviRJS8hAYZHkfcAjgC8CP23lAgwLSVoBBr10dhI4ul3iKklaYQb9BfcNwMOG2YgkaekadM9iNXBTks8BP9lTrKpnDqUrSdKSMmhYnDPMJiRJS9ugV0N9atiNSJKWrkGvhvo+9z574kDggcAPqurBw2pMkrR0DLpn8Wt7ppOE3mNQjxtWU5KkpWWfn2fRnpP9EXrPx5YkrQCDHoZ6dt/sA+j97uLHQ+lIWoE+9Tu/O+4W9tnvftpTmSvJoFdDPaNvejfwdXqHoiRJK8Cg5yxOX8wPTfIK4I/pnTT/Er0n4x0BXAIcRu8Oty+sqnuSHETvtiKPB74D/Keq+vpi9iNJ2rtBH360Nsk/Jrmjvf4hydqFfGCSNcDLgMmqejRwAHAK8BbgvKo6CrgLOKO95QzgrlY/r42TJI3QoCe43wtsp/dci4cDH2u1hVoF/HKSVfQernQb8HvAh9ryi4FntemNbZ62/MR2RZYkaUQGDYuJqnpvVe1ur4uAiYV8YFXtAv4K+Ca9kLib3mGn71bV7jZsJ/c+iW8NcGt77+42/rCFfLYkaWEGDYvvJHlBkgPa6wX0zh/ss/Z41o3AkfT2Un4FOGkh65q13s1JppJMzczM7O/qJEl9Bg2LPwKeB9xOb2/gOcBpC/zMJwO3VNVMVf0r8GHgeODgdlgKYC2wq03vAtYBtOUPYY6gqqotVTVZVZMTEwva6ZEkzWPQsHgjsKmqJqrqofTC488X+JnfBI5L8qB27uFE4Cbgk/RCCGAT8NE2vb3N05Z/wudqSNJoDRoWj6mqu/bMVNWdwOMW8oFVdQ29E9Wfp3fZ7AOALcBrgVcmmaZ3TuLC9pYLgcNa/ZXAWQv5XEnSwg36o7wHJDlkT2AkOXQf3nsfVXU2cPas8s3AsXOM/THw3IV+liRp/w36H/y3AZ9N8sE2/1zg3OG0JElaagb9Bfe2JFP0fgsB8Oyquml4bUmSlpKBDyW1cDAgJGkF2udblEuSVh7DQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0WfH8nLT3ffOO/G3cL++Tf/NmXxt2CpAG5ZyFJ6mRYSJI6GRaSpE6GhSSp01jCIsnBST6U5CtJvpzkPyQ5NMnlSb7W/h7SxibJO5JMJ7k+yTHj6FmSVrJx7Vm8HfjfVfUbwG8CX6b3uNQrqmoDcAX3Pj71ZGBDe20GLhh9u5K0so08LJI8BPgd2jO2q+qeqvousBG4uA27GHhWm94IbKueq4GDkxwx0qYlaYUbx57FkcAM8N4kX0jyniS/AhxeVbe1MbcDh7fpNcCtfe/f2WqSpBEZR1isAo4BLqiqxwE/4N5DTgBUVQG1LytNsjnJVJKpmZmZRWtWkjSesNgJ7Kyqa9r8h+iFx7f2HF5qf+9oy3cB6/rev7bVfkFVbamqyaqanJiYGFrzkrQSjTwsqup24NYkj2ylE+k923s7sKnVNgEfbdPbgRe1q6KOA+7uO1wlSRqBcd0b6qXA+5McCNwMnE4vuC5NcgbwDeB5bewO4GnANPDDNlaSNEJjCYuq+iIwOceiE+cYW8CZw+5JkjQ/f8EtSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqdPYwiLJAUm+kOSf2vyRSa5JMp3k79tT9EhyUJufbsvXj6tnSVqpxrln8XLgy33zbwHOq6qjgLuAM1r9DOCuVj+vjZMkjdBYwiLJWuD3gfe0+QC/B3yoDbkYeFab3tjmactPbOMlSSMyrj2L84HXAD9r84cB362q3W1+J7CmTa8BbgVoy+9u4yVJIzLysEjydOCOqrpukde7OclUkqmZmZnFXLUkrXjj2LM4Hnhmkq8Dl9A7/PR24OAkq9qYtcCuNr0LWAfQlj8E+M7slVbVlqqarKrJiYmJ4W6BJK0wIw+LqnpdVa2tqvXAKcAnqur5wCeB57Rhm4CPtuntbZ62/BNVVSNsWZJWvKX0O4vXAq9MMk3vnMSFrX4hcFirvxI4a0z9SdKKtap7yPBU1ZXAlW36ZuDYOcb8GHjuSBuTJP2CpbRnIUlaogwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ1GHhZJ1iX5ZJKbktyY5OWtfmiSy5N8rf09pNWT5B1JppNcn+SYUfcsSSvdOPYsdgOvqqqjgeOAM5McTe/Z2ldU1QbgCu591vbJwIb22gxcMPqWJWllG3lYVNVtVfX5Nv194MvAGmAjcHEbdjHwrDa9EdhWPVcDByc5YrRdS9LKNtZzFknWA48DrgEOr6rb2qLbgcPb9Brg1r637Wy12evanGQqydTMzMzwmpakFWhsYZHkV4F/AP60qr7Xv6yqCqh9WV9VbamqyaqanJiYWMROJUljCYskD6QXFO+vqg+38rf2HF5qf+9o9V3Aur63r201SdKIjONqqAAXAl+uqr/uW7Qd2NSmNwEf7au/qF0VdRxwd9/hKknSCKwaw2ceD7wQ+FKSL7ba64E3A5cmOQP4BvC8tmwH8DRgGvghcPpIu5UkjT4squqfgcyz+MQ5xhdw5lCbkiTtlb/gliR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktRp2YRFkpOSfDXJdJKzxt2PJK0kyyIskhwAvBs4GTgaODXJ0ePtSpJWjmURFsCxwHRV3VxV9wCXABvH3JMkrRjpPeJ6aUvyHOCkqvrjNv9C4AlV9ZK+MZuBzW32kcBXR9jiauDbI/y8UXP7lje3b/ka9bb926qamGvBqhE2MVRVtQXYMo7PTjJVVZPj+OxRcPuWN7dv+VpK27ZcDkPtAtb1za9tNUnSCCyXsLgW2JDkyCQHAqcA28fckyStGMviMFRV7U7yEuAy4ABga1XdOOa2+o3l8NcIuX3Lm9u3fC2ZbVsWJ7glSeO1XA5DSZLGyLCQJHUyLGZJUkne1jf/35Kc0ze/OclX2utzSX67b9mVSab65ieTXNmmT0hyd5Iv9r2ePJqtmt/etjfJOUl2zer54CSnJXnXrPVcmWRJXOI3qCQ/bdt0Q5IPJnnQuHtaqK5/bpe7+9N3NYil+H0aFvf1E+DZSVbPXpDk6cCfAL9dVb8BvBj4uyQP6xv20CQnz7Puq6rqsX2vjy969/tu3u1tzpvV83dH2Nuw/aht06OBe+h9n8tV1/cI/DzU14+mpUW11++q/c/YRWPpbDgG+j5HybC4r930rkB4xRzLXgu8uqq+DVBVnwcuBs7sG/NW4A3DbnIR7W17V5KrgKPG3cR+WEnf43L/rgax5L5Pw2Ju7waen+Qhs+qPAq6bVZtq9T0+C9yT5ElzrPeJsw7pPGLxWt4v820vwCv6+v3kqBsbhSSr6N2k8kvj7mU/7e17vF+4H31Xg1hS3+ey+J3FqFXV95JsA14G/GgBq3gT8N/p7Yn0u6qqnr6//S22ju09r6r+avZb5lvVojc3XL+c5Itt+irgwjH2st/m+x6TnA68vM0eBexIcg9wS1X9weg7XZA5v6sk1wAHAb8KHNo35rVVddmom1xMi/DfoUVlWMzvfODzwHv7ajcBjwc+0Vd7PPALPxCsqk8keRNw3JB7XEznc9/tnc93gENm1Q5l+d3M7UdV9dhxN7HIzmfW91hV790z3y64OK2qvj6G3vbHnN9VVT0Beucs6G3XaSPtavjOZ/B/L4fKw1DzqKo7gUuBM/rKfwm8JclhAEkeC5wG/M0cq3gT8Jrhdrl45tne+VwLHL/nxH67Cuog4NbhdahB7OP3qCVuKX2fhsXevY3eLYIBqKrtwFbg/yb5CvC3wAuq6rbZb6yqHcDMrPLscxbPGWLvC/EL29u8YlbP66vqW/QOa+xou/3nA6dW1c9G267mMdf3qOVrSXyf3u5DktTJPQtJUifDQpLUybCQJHUyLCRJnQwLSVInw0Ia0L7c+XSuO/MOsP7JJO9o0yck+a397VlaLIaFNLih3aU2yaqqmqqql7XSCYBhoSXDsJAW5irgqCSHJvlIkuuTXJ3kMbMHJnlGkmuSfCHJx5Mc3urnJHlfks8A72t7E//UbiH+Yu79QeQTk9yS5IHtfQ/un5dGwbCQ9tGsO5/+OfCFqnoM8Hpg2xxv+WfguKp6HHAJv3gbmKOBJ1fVqXsK7b5N/4N7nyVyFXAl8PttyCnAh6vqXxdzu6S98UaC0uDmuvPpNcAfws9vIHlYkgfPet9a4O+THAEcCNzSt2x7VQ1yR9H30AuZjwCnA/95oRshLYRhIQ3uPnc+TTLI+94J/HVVbW93Rz2nb9kPBllBVX0myfr2/gOq6oZB3ictFg9DSfvnKuD58PPbZH+7qr43a8xDgF1tetOA6/0+8GuzatuAv2MJ3K5aK49hIe2fc4DHJ7keeDNzh8E5wAeTXMfgz/z4GPAHe05wt9r76T1H5AP71bG0AN51Vlom2i3tN1bVC8fdi1Yez1lIy0CSd9K7Autp4+5FK5N7FpKkTp6zkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdfr/lkVb1DXEKWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x ='Polarity', data = df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columna de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x ='Type', data = df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de datos\n",
    "#### Polaridad\n",
    "Vamos a clasificar los Tweets como buenos o malos, por lo que haremos la siguiente agrupación de la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polaridad_fun(x):\n",
    "    if x in ('P', 'P+'):\n",
    "        return 0\n",
    "    elif x in ('N', 'N+'):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P', 'N+', 'P+', 'N'], dtype=object)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos cargamos los NONE y los neutros\n",
    "df = df[~df['Polarity'].isin(['NONE','NEU'])]\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasamos la columna a 1s y 0s. Y el tipo\n",
    "df.loc[:,'Polarity'] = df['Polarity'].apply(polaridad_fun)\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idioma\n",
    "Nos quedamos con los tweets en español. Si no tuviésemos esa columna podríamos acudir a librerías como `langid` o `langdetect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los tweets en español\n",
    "df = df[df['Lang'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5066, 6)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos con cuantos registros nos hemos quedado despues del filtrado\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5052, 6)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos los duplicados\n",
    "df.drop_duplicates(subset = 'Content', inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signos de puntuación\n",
    "Eliminamos signos de puntuación: puntos, comas, interrogaciones, paréntesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                            @marodriguezb Gracias MAR\n",
       "3    Off pensando en el regalito Sinde, la que se v...\n",
       "4    Conozco a alguien q es adicto al drama! Ja ja ...\n",
       "6    Toca @crackoviadeTV3 . Grabación dl especial N...\n",
       "8    Buen día todos! Lo primero mandar un abrazo gr...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                            @marodriguezb gracias mar\n",
       "3    off pensando en el regalito sinde la que se va...\n",
       "4    conozco a alguien q es adicto al drama ja ja j...\n",
       "6    toca @crackoviadetv  grabación dl especial nav...\n",
       "8    buen día todos lo primero mandar un abrazo gra...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "\n",
    "def signs_tweets(tweet):\n",
    "    return signos.sub('', tweet.lower())\n",
    "\n",
    "df['Content_bck'] = df['Content']\n",
    "df['Content'] = df['Content'].apply(signs_tweets)\n",
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "      <th>Content_bck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb gracias mar</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>off pensando en el regalito sinde la que se va...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>conozco a alguien q es adicto al drama ja ja j...</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>toca @crackoviadetv  grabación dl especial nav...</td>\n",
       "      <td>2011-12-02T07:00:50</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nacho_uriarte</td>\n",
       "      <td>buen día todos lo primero mandar un abrazo gra...</td>\n",
       "      <td>2011-12-02T07:45:05</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Content  \\\n",
       "2  CarmendelRiego                          @marodriguezb gracias mar   \n",
       "3    mgilguerrero  off pensando en el regalito sinde la que se va...   \n",
       "4        paurubio  conozco a alguien q es adicto al drama ja ja j...   \n",
       "6    Carlos_Latre  toca @crackoviadetv  grabación dl especial nav...   \n",
       "8   nacho_uriarte  buen día todos lo primero mandar un abrazo gra...   \n",
       "\n",
       "                  Date Lang  Polarity       Type  \\\n",
       "2  2011-12-02T00:57:40   es         0  AGREEMENT   \n",
       "3  2011-12-02T02:33:37   es         1  AGREEMENT   \n",
       "4  2011-12-02T02:59:03   es         0  AGREEMENT   \n",
       "6  2011-12-02T07:00:50   es         0  AGREEMENT   \n",
       "8  2011-12-02T07:45:05   es         0  AGREEMENT   \n",
       "\n",
       "                                         Content_bck  \n",
       "2                          @marodriguezb Gracias MAR  \n",
       "3  Off pensando en el regalito Sinde, la que se v...  \n",
       "4  Conozco a alguien q es adicto al drama! Ja ja ...  \n",
       "6  Toca @crackoviadeTV3 . Grabación dl especial N...  \n",
       "8  Buen día todos! Lo primero mandar un abrazo gr...  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(df):\n",
    "    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "\n",
    "df['Content'] = df['Content'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros\n",
    "Podríamos hacer un preprocesado mucho más fino:\n",
    "1. Hashtags\n",
    "2. Menciones\n",
    "3. Abreviaturas\n",
    "4. Faltas de ortografía\n",
    "5. Risas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "Para montar el modelo tendremos que seguir los siguientes pasos\n",
    "1. Eliminamos las stopwords\n",
    "2. Aplicamos un stemmer, SnowBall por ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "      <th>Content_bck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb gracias mar</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>off pensando regalito sinde va sgae van corrup...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>conozco alguien q adicto drama ja ja ja suena d</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>toca @crackoviadetv grabación dl especial navi...</td>\n",
       "      <td>2011-12-02T07:00:50</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nacho_uriarte</td>\n",
       "      <td>buen día primero mandar abrazo grande miguel f...</td>\n",
       "      <td>2011-12-02T07:45:05</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Content  \\\n",
       "2  CarmendelRiego                          @marodriguezb gracias mar   \n",
       "3    mgilguerrero  off pensando regalito sinde va sgae van corrup...   \n",
       "4        paurubio    conozco alguien q adicto drama ja ja ja suena d   \n",
       "6    Carlos_Latre  toca @crackoviadetv grabación dl especial navi...   \n",
       "8   nacho_uriarte  buen día primero mandar abrazo grande miguel f...   \n",
       "\n",
       "                  Date Lang  Polarity       Type  \\\n",
       "2  2011-12-02T00:57:40   es         0  AGREEMENT   \n",
       "3  2011-12-02T02:33:37   es         1  AGREEMENT   \n",
       "4  2011-12-02T02:59:03   es         0  AGREEMENT   \n",
       "6  2011-12-02T07:00:50   es         0  AGREEMENT   \n",
       "8  2011-12-02T07:45:05   es         0  AGREEMENT   \n",
       "\n",
       "                                         Content_bck  \n",
       "2                          @marodriguezb Gracias MAR  \n",
       "3  Off pensando en el regalito Sinde, la que se v...  \n",
       "4  Conozco a alguien q es adicto al drama! Ja ja ...  \n",
       "6  Toca @crackoviadeTV3 . Grabación dl especial N...  \n",
       "8  Buen día todos! Lo primero mandar un abrazo gr...  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    return \" \".join([word for word in df.split() if word not in spanish_stopwords])\n",
    "\n",
    "df['Content'] = df['Content'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_stemmer = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def spanish_stemmer(x):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return \" \".join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "df['Content'] = df['Content'].apply(spanish_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos columnas\n",
    "Nos quedamos con las columnas que nos interesan para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Content', 'Polarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montamos Pipeline\n",
    "Modelos que suelen funcionar bien con pocas observaciones y muchas features son la Regresión logística el LinearSVC o Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df['Content'],df['Polarity'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC() es el clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Aqui definimos el espacio de parámetros a explorar\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5,1),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__penalty': (None, \"l2\"),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                           parameters,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1 ,\n",
    "                           scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.55 s, sys: 279 ms, total: 5.83 s\n",
      "Wall time: 48.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "1620 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    % (all_penalties, penalty)\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1080 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\", line 355, in _fit\n",
      "    **fit_params_steps[name],\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72902893 0.72803975 0.71393298 0.71244783 0.64489683 0.64341076\n",
      " 0.73026686 0.73298933 0.71393298 0.71244783 0.64489683 0.64341076\n",
      " 0.73026686 0.73298933 0.71393298 0.71244783 0.64489683 0.64341076\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72902893 0.72803975 0.71393298 0.71244783 0.64489683 0.64341076\n",
      " 0.73026686 0.73298933 0.71393298 0.71244783 0.64489683 0.64341076\n",
      " 0.73026686 0.73298933 0.71393298 0.71244783 0.64489683 0.64341076\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72754409 0.72902924 0.71294411 0.71640792 0.64638167 0.65306392\n",
      " 0.72977089 0.73224584 0.71294411 0.71640792 0.64638167 0.65306392\n",
      " 0.72977089 0.73224584 0.71294411 0.71640792 0.64638167 0.65306392\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72754409 0.72902924 0.71294411 0.71640792 0.64638167 0.65306392\n",
      " 0.72977089 0.73224584 0.71294411 0.71640792 0.64638167 0.65306392\n",
      " 0.72977089 0.73224584 0.71294411 0.71640792 0.64638167 0.65306392\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.7307613  0.72803914 0.71393359 0.7161604  0.64910414 0.65405433\n",
      " 0.7295249  0.73026747 0.71393359 0.7161604  0.64910414 0.65405433\n",
      " 0.7295249  0.73026747 0.71393359 0.7161604  0.64910414 0.65405433\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.7307613  0.72803914 0.71393359 0.7161604  0.64910414 0.65405433\n",
      " 0.7295249  0.73026747 0.71393359 0.7161604  0.64910414 0.65405433\n",
      " 0.7295249  0.73026747 0.71393359 0.7161604  0.64910414 0.65405433\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cls__C': (0.2, 0.5, 0.7),\n",
       "                         'cls__max_iter': (500, 1000),\n",
       "                         'cls__penalty': (None, 'l2'), 'vect__max_df': (0.5, 1),\n",
       "                         'vect__max_features': (500, 1000, None),\n",
       "                         'vect__min_df': (10, 20, 50),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(X_train,\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'cls__C': 0.2, 'cls__max_iter': 500, 'cls__penalty': 'l2', 'vect__max_df': 0.5, 'vect__max_features': 1000, 'vect__min_df': 10, 'vect__ngram_range': (1, 2)}\n",
      "Best acc: 0.7329893279810056\n",
      "Best model: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.5, max_features=1000, min_df=10,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('cls', LogisticRegression(C=0.2, max_iter=500))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best acc:\", grid_search.best_score_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79       582\n",
      "           1       0.73      0.69      0.71       429\n",
      "\n",
      "    accuracy                           0.76      1011\n",
      "   macro avg       0.75      0.75      0.75      1011\n",
      "weighted avg       0.76      0.76      0.76      1011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AÑADIMOS OTRAS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonrisas = re.compile(\"([:;8X]-[\\)DP])\")\n",
    "def have_risas_emojis(row):\n",
    "    if len(sonrisas.findall(row)):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagrimas = re.compile(\"([:;8]-[\\(O]+)\")\n",
    "def have_lagrimas_emojis(row):\n",
    "    if len(lagrimas.findall(row)):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(row):\n",
    "    return len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tiene_sonrisas\"] = df_old.Content_bck.apply(have_risas_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tiene_lagrimas\"] = df_old.Content_bck.apply(have_lagrimas_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cuenta_tokens\"] = df.Content.apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = ColumnTransformer([\n",
    "    (\"vect\",CountVectorizer(),\"Content\")],\n",
    "    remainder = \"passthrough\")\n",
    "new_pipeline = Pipeline([\n",
    "    (\"preprocesado\",preprocessing),\n",
    "    (\"cls\",LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "   'vect__max_df': (0.5,1),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__penalty': (None, \"l2\"),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "new_parameters = {\n",
    "    'preprocesado__vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__penalty': (\"none\", \"l2\"),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_new_feats = GridSearchCV(new_pipeline,\n",
    "                           new_parameters,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1 ,\n",
    "                           scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df[[col for col in df.columns if \"Polari\" not in col]],df['Polarity'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 2.91 s, total: 5.57 s\n",
      "Wall time: 36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocesado',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('vect',\n",
       "                                                                         CountVectorizer(),\n",
       "                                                                         'Content')])),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cls__C': (0.2, 0.5, 0.7),\n",
       "                         'cls__max_iter': (500, 1000),\n",
       "                         'cls__penalty': ('none', 'l2'),\n",
       "                         'preprocesado__vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_new_feats.fit(X_train,\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'cls__C': 0.5, 'cls__max_iter': 500, 'cls__penalty': 'l2', 'preprocesado__vect__ngram_range': (1, 2)}\n",
      "Best acc: 0.7785179723163911\n",
      "Best model: Pipeline(steps=[('preprocesado',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('vect',\n",
      "                                                  CountVectorizer(ngram_range=(1,\n",
      "                                                                               2)),\n",
      "                                                  'Content')])),\n",
      "                ('cls', LogisticRegression(C=0.5, max_iter=500))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search_new_feats.best_params_)\n",
    "print(\"Best acc:\", grid_search_new_feats.best_score_)\n",
    "print(\"Best model:\", grid_search_new_feats.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_search_new_feats.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       568\n",
      "           1       0.77      0.65      0.71       443\n",
      "\n",
      "    accuracy                           0.76      1011\n",
      "   macro avg       0.76      0.75      0.75      1011\n",
      "weighted avg       0.76      0.76      0.76      1011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPONGAMOS QUE NO TENEMOS LABEL, \"MODELO\" NO SUPERVISADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un lexicon \"sentimental\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lexicon = pd.read_csv(\"./data/spanish_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a_buena_fé</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a_capella</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_cara_descubierta</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_gusto</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_la_defensiva</td>\n",
       "      <td>-0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>ósmosis</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>ótico</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11341</th>\n",
       "      <td>óxido_nítrico</td>\n",
       "      <td>-0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>único</td>\n",
       "      <td>0.3330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>útil</td>\n",
       "      <td>0.4065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11344 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    lemma  Polarity\n",
       "0              a_buena_fé    0.5000\n",
       "1               a_capella   -0.2500\n",
       "2      a_cara_descubierta    0.2500\n",
       "3                 a_gusto    0.7500\n",
       "4          a_la_defensiva   -0.3750\n",
       "...                   ...       ...\n",
       "11339             ósmosis   -0.2500\n",
       "11340               ótico   -0.2500\n",
       "11341       óxido_nítrico   -0.3750\n",
       "11342               único    0.3330\n",
       "11343                útil    0.4065\n",
       "\n",
       "[11344 rows x 2 columns]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo: Está indexado por Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo nos afectará?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos contruimos nuestro modelo basado en las puntuaciones del lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score_lexicon(text,threshold = 0.2):\n",
    "    score = 0\n",
    "    cuenta = 0\n",
    "    for word in text.split():\n",
    "        #print(word)\n",
    "        try:\n",
    "            score += df_lexicon[df_lexicon.lemma == word].Polarity.values[0]\n",
    "            cuenta += 1\n",
    "        except:\n",
    "            #print(\"No esta en el lexicon\")\n",
    "            continue\n",
    "    score = score/cuenta if cuenta > 0 else -99999\n",
    "    return \"P\" if score > threshold else \"N\" if score > -99999 else \"N/S\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero no lo podemos aplicar al contenido procesado porque le hemos aplicado el stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                 @marodriguezb graci mar\n",
       "3       off pens regalit sind va sga van corrupt inten...\n",
       "4               conozc algui q adict dram ja ja ja suen d\n",
       "6       toc @crackoviadetv grabacion dl especial navid...\n",
       "8       buen dia primer mand abraz grand miguel famili...\n",
       "                              ...                        \n",
       "7214    indign si nadi rep ello hoy aqui graci rt @lol...\n",
       "7215                            pobr discrimin muj {link}\n",
       "7216               cre banc product mujer canc mam {link}\n",
       "7217                sorprendent huid hoy sen rajoy {link}\n",
       "7218    #corremarianocorr bien version galleg #correma...\n",
       "Name: Content, Length: 5052, dtype: object"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 984 ms, sys: 15.7 ms, total: 1e+03 ms\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"Polarity_\"] = df.iloc[0:100].Content.apply(score_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N/S    74\n",
       "P      26\n",
       "Name: Polarity_, dtype: int64"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Polarity_.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo aplicamos al contenido original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 22.3 ms, total: 1.53 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_old[\"Polarity_\"] = df_old.iloc[0:100].Content_bck.apply(score_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N/S    45\n",
       "P      37\n",
       "N      18\n",
       "Name: Polarity_, dtype: int64"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old.iloc[0:100].Polarity_.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale algo mejor, pero todavía hay muchos N/S, ¿por qué? Porque espera lemmas. Tenemos que lemmatizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N/S    1\n",
       "Name: Content_bck, dtype: int64"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    N/S\n",
       "Name: Content_bck, dtype: object"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lemmatizar aprovecharemos las capacidades de una biblioteca muy potente para NLP, spacy, que además tiene modelos pre-entrenados en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'es_core_news_lg' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\") # Cargamos el modelo en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Esto es la prueba 23\") #Le metemos un texto y además de tokenizarlo, nos va a dar bastantes más features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto -> Lema:  este -> PoS Tag PRON -> Puede ser un numero? False\n",
      "es -> Lema:  ser -> PoS Tag AUX -> Puede ser un numero? False\n",
      "la -> Lema:  el -> PoS Tag DET -> Puede ser un numero? False\n",
      "prueba -> Lema:  prueba -> PoS Tag NOUN -> Puede ser un numero? False\n",
      "23 -> Lema:  23 -> PoS Tag NUM -> Puede ser un numero? True\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"-> Lema: \", token.lemma_, \"-> PoS Tag\", token.pos_, \"-> Puede ser un numero?\", token.like_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizemos su lemmatizador para poder aprovechar el lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scorer(doc):\n",
    "    parsed_doc = nlp(doc)\n",
    "    for token in parsed_doc:\n",
    "        new_doc = \" \".join([token.lemma_ for token in parsed_doc])\n",
    "    return score_lexicon(new_doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_before_stemmer[\"New_Pol\"] = df_before_stemmer.Content.apply(new_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P      2400\n",
       "N/S    1570\n",
       "N      1082\n",
       "Name: New_Pol, dtype: int64"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_stemmer.New_Pol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P      0.475059\n",
       "N/S    0.310768\n",
       "N      0.214173\n",
       "Name: New_Pol, dtype: float64"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_stemmer.New_Pol.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_stemmer[\"Aciertos\"] = df_before_stemmer[[\"New_Pol\",\"Polarity\"]].apply(lambda row: 1 if (row[0] == \"P\" and row[1] == 0) or (row[0] == \"N\" and row[1] == 1) else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "      <th>Content_bck</th>\n",
       "      <th>New_Pol</th>\n",
       "      <th>Aciertos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb gracias mar</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>N/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>off pensando regalito sinde va sgae van corrup...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>conozco alguien q adicto drama ja ja ja suena d</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>toca @crackoviadetv grabación dl especial navi...</td>\n",
       "      <td>2011-12-02T07:00:50</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nacho_uriarte</td>\n",
       "      <td>buen día primero mandar abrazo grande miguel f...</td>\n",
       "      <td>2011-12-02T07:45:05</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>mariviromero</td>\n",
       "      <td>indignante si nadie repara ello hoy aqui graci...</td>\n",
       "      <td>2012-04-10T22:19:42</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Muy indignante si ...nadie repara en ello hoy ...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7215</th>\n",
       "      <td>mariviromero</td>\n",
       "      <td>pobres discriminar mujer {link}</td>\n",
       "      <td>2012-04-10T22:39:22</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Más pobres por discriminar a la mujer http://t...</td>\n",
       "      <td>N/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>mariviromero</td>\n",
       "      <td>crean banco productos mujeres cáncer mama {link}</td>\n",
       "      <td>2012-04-10T22:41:30</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Crean un banco de productos para mujeres con c...</td>\n",
       "      <td>N/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>Tonicanto1</td>\n",
       "      <td>sorprendente huída hoy senado rajoy {link}</td>\n",
       "      <td>2012-04-10T23:16:49</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>Sobre la sorprendente huída hoy en el Senado d...</td>\n",
       "      <td>N/S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7218</th>\n",
       "      <td>RodolfoIrago</td>\n",
       "      <td>#corremarianocorre bien versión gallega #corre...</td>\n",
       "      <td>2012-04-10T23:40:36</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>#CorreMarianoCorre está muy bien pero la versi...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5052 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                User                                            Content  \\\n",
       "2     CarmendelRiego                          @marodriguezb gracias mar   \n",
       "3       mgilguerrero  off pensando regalito sinde va sgae van corrup...   \n",
       "4           paurubio    conozco alguien q adicto drama ja ja ja suena d   \n",
       "6       Carlos_Latre  toca @crackoviadetv grabación dl especial navi...   \n",
       "8      nacho_uriarte  buen día primero mandar abrazo grande miguel f...   \n",
       "...              ...                                                ...   \n",
       "7214    mariviromero  indignante si nadie repara ello hoy aqui graci...   \n",
       "7215    mariviromero                    pobres discriminar mujer {link}   \n",
       "7216    mariviromero   crean banco productos mujeres cáncer mama {link}   \n",
       "7217      Tonicanto1         sorprendente huída hoy senado rajoy {link}   \n",
       "7218    RodolfoIrago  #corremarianocorre bien versión gallega #corre...   \n",
       "\n",
       "                     Date Lang  Polarity       Type  \\\n",
       "2     2011-12-02T00:57:40   es         0  AGREEMENT   \n",
       "3     2011-12-02T02:33:37   es         1  AGREEMENT   \n",
       "4     2011-12-02T02:59:03   es         0  AGREEMENT   \n",
       "6     2011-12-02T07:00:50   es         0  AGREEMENT   \n",
       "8     2011-12-02T07:45:05   es         0  AGREEMENT   \n",
       "...                   ...  ...       ...        ...   \n",
       "7214  2012-04-10T22:19:42   es         1  AGREEMENT   \n",
       "7215  2012-04-10T22:39:22   es         1  AGREEMENT   \n",
       "7216  2012-04-10T22:41:30   es         0  AGREEMENT   \n",
       "7217  2012-04-10T23:16:49   es         1  AGREEMENT   \n",
       "7218  2012-04-10T23:40:36   es         0  AGREEMENT   \n",
       "\n",
       "                                            Content_bck New_Pol  Aciertos  \n",
       "2                             @marodriguezb Gracias MAR     N/S         0  \n",
       "3     Off pensando en el regalito Sinde, la que se v...       N         1  \n",
       "4     Conozco a alguien q es adicto al drama! Ja ja ...       P         1  \n",
       "6     Toca @crackoviadeTV3 . Grabación dl especial N...       P         1  \n",
       "8     Buen día todos! Lo primero mandar un abrazo gr...       P         1  \n",
       "...                                                 ...     ...       ...  \n",
       "7214  Muy indignante si ...nadie repara en ello hoy ...       N         1  \n",
       "7215  Más pobres por discriminar a la mujer http://t...     N/S         0  \n",
       "7216  Crean un banco de productos para mujeres con c...     N/S         0  \n",
       "7217  Sobre la sorprendente huída hoy en el Senado d...     N/S         0  \n",
       "7218  #CorreMarianoCorre está muy bien pero la versi...       N         0  \n",
       "\n",
       "[5052 rows x 9 columns]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.506928\n",
       "1    0.493072\n",
       "Name: Aciertos, dtype: float64"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_stemmer.Aciertos.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probar descontando los N/S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
