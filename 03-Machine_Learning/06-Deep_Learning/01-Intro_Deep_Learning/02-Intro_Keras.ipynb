{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pabma\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pabma\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pabma\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: keras in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (3.10.0)\n",
      "Requirement already satisfied: optree in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optree->keras) (4.10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pabma\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pabma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPor defecto, keras no tira de GPU\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "'''\n",
    "Por defecto, keras no tira de GPU\n",
    "'''\n",
    "#https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar y abrir VS Code si hace falta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[25000,12,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays). Aplano a una dimension cada imagen.\n",
    "# Escalamos ya que vamos a usar gradient descent, y le afecta mucho la escala de las features.\n",
    "# Ejecutar esta celda solo una vez. Sino reescalará\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10,000 samples for validation. Entraran dentro del modelo para validar. No es validacion cruzada\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICION/CONSTRUCCION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax.  \n",
    "\n",
    "Es decir vamos a volver a montar esta arquitectura:  \n",
    "<img src=\"./img/mlp_clasification.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Un poco más sobre la activación softmax:    \n",
    "\n",
    "Fórmula:  \n",
    "<img src=\"./img/softmax_function.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Función de transferencia:  \n",
    "<img src=\"./img/softmax_activation.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Ejemplo de funcionamiento:  \n",
    "<img src=\"./img/softmax_example.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una manera de declarar la red neuronal\n",
    "\n",
    "# Siempre hay que declarar la capa sequential para empezar a declarar la red\n",
    "# Se trata de la API sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Flatten, aplana en un unico vector. Y especificamos el tamaño de la entrada\n",
    "# Es como si hiciese un .reshape(-1, 28*28)\n",
    "# \"kernel_initializer\" o \"bias_initializer\" No lo usamos pero seria para inicializar los pesos de otra manera\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Capas de la red. Dense es la capa de neuronas. Necesitamos numero y activacion\n",
    "model.add(keras.layers.Dense(units = 300, # Numero de neuronas de la capa\n",
    "                             activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(100,\n",
    "                             activation='relu'))\n",
    "\n",
    "# Capa de salida, con tamaño del número de clases\n",
    "# Suele ir aqui un softmax. Para multiclase guay. Si es binaria -> sigmoide\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "]\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y una forma mediante encademaniento de funciones (usando lo que se denomina la Functional API)\n",
    "input_layer = keras.layers.Input(shape = (28,28))\n",
    "flatten_layer = keras.layers.Flatten()(input_layer)\n",
    "hidden_1 = keras.layers.Dense(300, activation = \"relu\")(flatten_layer)\n",
    "hidden_2 = keras.layers.Dense(100, activation = \"relu\")(hidden_1)\n",
    "output = keras.layers.Dense(10, activation = \"softmax\")(hidden_2)\n",
    "model = keras.Model(inputs = [input_layer], outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Flatten name=flatten_5, built=True>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<InputLayer name=input_layer_5, built=True>,\n",
       " <Flatten name=flatten_5, built=True>,\n",
       " <Dense name=dense_15, built=True>,\n",
       " <Dense name=dense_16, built=True>,\n",
       " <Dense name=dense_17, built=True>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[1])\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[2]\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "# 784 features (pixeles de las imagenes) x 300 neuronas\n",
    "# Los pesos están inicializados aleatoriamente\n",
    "weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializadores:  \n",
    "- Los pesos inicializados a cero -> No aprendizaje\n",
    "- Desde siempre se inicializan \"aleatoriamente\", pero no sólo de forma uniforme (todos los valores con la misma probabilidad), sino que se emplean diferentes distribuciones de probabilidad con parámetros que dependen del número de entradas y salidas de la capa. El objetivo esintentar que las varianzas de las entradas sean similares a las de las salidas y evitar el problema del gradiente que se desvanece (\"Vanishing Gradient\" problem):  \n",
    "    *   Glorot inizialization (por defecto la de Keras, con función uniforme de distribución) -> Para cuando tienes funciones de activación (ninguna, tanh, sigmoid, softmax, aunque también se usa por defecto :-) para casi todo) [Xavier Glorot & Yoshua Bengio]\n",
    "    *   He inizialization, -> Para cuando tienes ReLU, Leaky ReLU, ELU, GELU, Swish, Mish [He Kaiming et al.]\n",
    "    *   LeCunn inizialization -> Para cuando tienes SELU [Jean LeCunn]\n",
    "- Es un hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica la configuración del entrenamiento (optimizador, pérdida, métricas):\n",
    "model.compile(\n",
    "    # Stocastic gradient descent. El algoritmo para minimizar la loss function\n",
    "    # El stocastic va haciendo muestreo en cada evaluacion, no usa todo el dataset\n",
    "    # Podemos modificar el learning rate(0.01 por defecto) mediante el parametro lr\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer, con parámetros por defecto\n",
    "    \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    # sparse_categorical_crossentropy cuando tenemos un label en nuna columna\n",
    "    # Si lo tuviesemos en varias tipo dummy, cogeriamos categorical_crossentropy\n",
    "    # binary_crossentropy si es una neurona, clasi binario\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente (... casi, los parámetros del optimizador serán los que tenga por defecto)\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary\n",
    "# La primera hidden layer tiene 784 entradas x 300 salidas\n",
    "# Son los 235500 params = 783x300 + 300 (bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAPAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos construyendo nuestra __lista de capas__ (para guardar en el \"Toolbox\"):\n",
    "\n",
    "__Entrenables__:  \n",
    "__* Dense__ -> Capa completamente conectada a las neuronas de la capa anterior y a la posterior  \n",
    "    Hiperparámetros asociados:     \n",
    "        * units: Number of neurons, dimensionality of the output space  \n",
    "        * activation: Activation function to use. If you don't specify anything, no activation is applied  \n",
    "        * kernel_initializer: Initializer for the kernel weights matrix.  \n",
    "        * bias_initializer: Initializer for the bias vector. (Suelen inicializarse a cero)\n",
    "        * Kernel_regularizar: Los clásicos (L1,L2,...)\n",
    " \n",
    "__Funcionales__:       \n",
    "__* Input__ -> Capa para definir la forma de la entrada (shape), que se puede pasar como input_shape\n",
    "__* Flatten__ -> Capa que aplana (convierte su entrada en un vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones de activación interesantes: SELU (1.67*ELU) y Swish (también SiLU, o Sigmoid linear unit)... No entrar en pánico, vais a usar ReLU, Softmax y no-activation, y en algunos casos (quizás): sigmoid, tanh y las (x)LU (SELU, Siwsh,etc)\n",
    "\n",
    "<img src=\"./img/activation_functions.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMIZADORES\n",
    "\n",
    "##### Ejemplo \"sencillo\" https://medium.com/@axegggl/newtons-method-for-optimization-in-python-11ce261fcf98"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también vamos completando lista de hiperparámetros, estos asociados al \"Optimizador\"/\"Modelo\":  \n",
    "Tipo de optimizador:  \n",
    "* __SGD__, Gradient descent \"genérico\" (puedes añadirle \"momento\", es decir que a la hora de descontar el gradiente tenga en cuenta el vector medio de gradientes pasados)\n",
    "  \n",
    "\n",
    "* __Adagrad__, Hace gradient descent pero ajusta el gradiente para compensar las componentes de mayor valor numérico (es como evitar irse por las pendientes más inclinadas)... Es decir evita irse a mínimos locales al precio de enlentecer el entrenamiento.    \n",
    "\n",
    "* __RMSprop__, Versión de AdaGrad, pero considera principalmente los últimos valores del gradiente. Es decir, busca lo bueno de Adagrad reduciendo sus peligros.    \n",
    "\n",
    "* __Adam__, _Adaptative Moment Estimation_, combina RMSProp y el uso de momento. Es el rey actual (junto con sus versiones) para grandes cantidades de datos.    \n",
    "\n",
    "* __AdamW, Nadam, AdaMax__, variantes del anterior. \n",
    "\n",
    "Comparativa, donde * es malo y *** bueno (extraído del \"Hands-on Machine Learning with....\" de Aurelien Geron, 3a Edicion)\n",
    "\n",
    "<img src=\"./img/Comparativa-optimizadores.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Keras también permite:\n",
    "__Adadelta__ _(variante de Adagrad)_, __Adafactor__ y __Ftrl__\n",
    "\n",
    "      \n",
    "Hiperparámetros Genéricos:\n",
    "Learning Rate: Coeficiente aplicado al descenso de gradiente, como en otros modelos que ya hemos visto\n",
    "Asociados al Gradient Clipping: clipnorm, clipvalue, global_clipnorm\n",
    "\n",
    "Cada optimizador además puede tener sus propios hiperparámetros (ver: https://keras.io/api/optimizers/)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuciones de pérdida y métricas\n",
    "__Función de perdida__: La función a minimizar durante el entrenamiento (son las mismas que en otros modelos no Deep)  \n",
    "- Clasificación: En clases Keras -> __BinaryCrossEntropy, CategoricalCrossEntropy, SparseCategoricalCrossEntropy__  \n",
    "- Regresión: En clases Keras -> __MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, CosineSimilarity__   \n",
    "\n",
    "__Métricas__:  \n",
    "- Regresión: __MAE, MSE, MAPE__ :-)  \n",
    "- Clasificación: __Accuracy, Precision, Recall, f1, AuRoC__  \n",
    "\n",
    "¿Cuál es la diferencia entre Categorical y Sparse? ¿Por qué las funciones de pérdida son diferentes a las métricas en Clasificación? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El __batch_size__ es la cantidad de muestras que utiliza el SGD, y las __epochs__ son las iteraciones que realiza en el entrenamiento. (Son hiperparámetros de entrenamiento)    \n",
    "\n",
    "En una epoch se entrenan tantos batches como sea necesario para recorrer todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6272 - loss: 1.4000 - val_accuracy: 0.8979 - val_loss: 0.3984\n",
      "Epoch 2/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8918 - loss: 0.4004 - val_accuracy: 0.9146 - val_loss: 0.3057\n",
      "Epoch 3/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9081 - loss: 0.3265 - val_accuracy: 0.9248 - val_loss: 0.2662\n",
      "Epoch 4/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9208 - loss: 0.2830 - val_accuracy: 0.9309 - val_loss: 0.2414\n",
      "Epoch 5/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9278 - loss: 0.2549 - val_accuracy: 0.9379 - val_loss: 0.2214\n",
      "Epoch 6/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9326 - loss: 0.2327 - val_accuracy: 0.9422 - val_loss: 0.2061\n",
      "Epoch 7/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9398 - loss: 0.2159 - val_accuracy: 0.9489 - val_loss: 0.1895\n",
      "Epoch 8/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1989 - val_accuracy: 0.9506 - val_loss: 0.1816\n",
      "Epoch 9/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1848 - val_accuracy: 0.9533 - val_loss: 0.1709\n",
      "Epoch 10/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1824 - val_accuracy: 0.9566 - val_loss: 0.1626\n",
      "Epoch 11/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9524 - loss: 0.1633 - val_accuracy: 0.9576 - val_loss: 0.1558\n",
      "Epoch 12/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1519 - val_accuracy: 0.9608 - val_loss: 0.1480\n",
      "Epoch 13/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.1486 - val_accuracy: 0.9614 - val_loss: 0.1430\n",
      "Epoch 14/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1388 - val_accuracy: 0.9620 - val_loss: 0.1385\n",
      "Epoch 15/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.1272 - val_accuracy: 0.9633 - val_loss: 0.1339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEn el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\\nSi vemos que ya no baja mas, no serán necesarias tantas epochs.\\nImprimera tantas lineas como epochs hayamos puesto\\n\\nTampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\\nUtil para datasets desbalanceados.\\n\\nEl loss que muestra es el categoricalcrossentropy\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64, # numero de muestras empleadas en el entrenamiento de SGD\n",
    "    epochs=15, # 1 por defecto. Insuficiente. Numero de vueltas del backpropagation\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    # En vez de validation data podemos usar el argumento validation_split=0.1\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1281 - val_accuracy: 0.9641 - val_loss: 0.1289\n",
      "Epoch 2/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1193 - val_accuracy: 0.9650 - val_loss: 0.1251\n",
      "Epoch 3/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.1126 - val_accuracy: 0.9660 - val_loss: 0.1213\n",
      "Epoch 4/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9691 - loss: 0.1100 - val_accuracy: 0.9669 - val_loss: 0.1186\n",
      "Epoch 5/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.1063 - val_accuracy: 0.9669 - val_loss: 0.1164\n",
      "Epoch 6/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.1007 - val_accuracy: 0.9684 - val_loss: 0.1152\n",
      "Epoch 7/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.0987 - val_accuracy: 0.9694 - val_loss: 0.1123\n",
      "Epoch 8/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0925 - val_accuracy: 0.9697 - val_loss: 0.1090\n",
      "Epoch 9/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.0887 - val_accuracy: 0.9686 - val_loss: 0.1100\n",
      "Epoch 10/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.0860 - val_accuracy: 0.9726 - val_loss: 0.1058\n",
      "Epoch 11/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0829 - val_accuracy: 0.9717 - val_loss: 0.1040\n",
      "Epoch 12/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0809 - val_accuracy: 0.9713 - val_loss: 0.1023\n",
      "Epoch 13/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0739 - val_accuracy: 0.9723 - val_loss: 0.0996\n",
      "Epoch 14/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0719 - val_accuracy: 0.9724 - val_loss: 0.0975\n",
      "Epoch 15/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0732 - val_accuracy: 0.9737 - val_loss: 0.0967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e04ef51f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 'auto', 'epochs': 15, 'steps': 782}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.7829599976539612,\n",
       "  0.896120011806488,\n",
       "  0.9128400087356567,\n",
       "  0.9225199818611145,\n",
       "  0.9292799830436707,\n",
       "  0.9353799819946289,\n",
       "  0.9405999779701233,\n",
       "  0.9438999891281128,\n",
       "  0.9477400183677673,\n",
       "  0.950439989566803,\n",
       "  0.9525600075721741,\n",
       "  0.9561399817466736,\n",
       "  0.9583600163459778,\n",
       "  0.9601200222969055,\n",
       "  0.9627400040626526],\n",
       " 'loss': [0.8974539637565613,\n",
       "  0.37788301706314087,\n",
       "  0.31123602390289307,\n",
       "  0.2747615575790405,\n",
       "  0.24883617460727692,\n",
       "  0.22856050729751587,\n",
       "  0.21128809452056885,\n",
       "  0.19654913246631622,\n",
       "  0.18410569429397583,\n",
       "  0.17303436994552612,\n",
       "  0.16326501965522766,\n",
       "  0.15448084473609924,\n",
       "  0.14595746994018555,\n",
       "  0.13866876065731049,\n",
       "  0.1317824423313141],\n",
       " 'val_accuracy': [0.8978999853134155,\n",
       "  0.9146000146865845,\n",
       "  0.9247999787330627,\n",
       "  0.930899977684021,\n",
       "  0.9379000067710876,\n",
       "  0.9422000050544739,\n",
       "  0.9488999843597412,\n",
       "  0.9506000280380249,\n",
       "  0.9532999992370605,\n",
       "  0.95660001039505,\n",
       "  0.9575999975204468,\n",
       "  0.9607999920845032,\n",
       "  0.9613999724388123,\n",
       "  0.9620000123977661,\n",
       "  0.9632999897003174],\n",
       " 'val_loss': [0.39838695526123047,\n",
       "  0.30570879578590393,\n",
       "  0.26618054509162903,\n",
       "  0.24142169952392578,\n",
       "  0.22144268453121185,\n",
       "  0.20610658824443817,\n",
       "  0.18945255875587463,\n",
       "  0.18164058029651642,\n",
       "  0.1708560287952423,\n",
       "  0.16255098581314087,\n",
       "  0.15582002699375153,\n",
       "  0.14801043272018433,\n",
       "  0.1429913491010666,\n",
       "  0.1384904831647873,\n",
       "  0.13392286002635956]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Podemos ver como evoluciona el entrenamiento, en funcion de los epochs\n",
    "# Validacion y training estan muy cerca, no hay overfitting!\n",
    "# Todavia no ha acabado de coverger ya que el loss en validacion sigue bajando,\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - accuracy: 0.9665 - loss: 0.1125\n",
      "test loss, test acc: [0.0955900028347969, 0.9714999794960022]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el \"score\" a partir del conjunto de test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "\n",
    "# Metodo evaluate para que nos de el error vs las metricas elegidas en la funcion compile\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pabma\\AppData\\Local\\Temp\\ipykernel_11748\\1468152043.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "predictions shape: (1, 10)\n",
      "[0.    0.    0.    0.003 0.    0.    0.    0.996 0.    0.   ]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ojo aqui viene slicing xq presupone que le entran varios inputs\n",
    "Nos da las probabilidades de pertenecer a una clase u otra.\n",
    "'''\n",
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona.\n",
    "     \n",
    "Vamos a configurar una red como esta:  \n",
    "<img src=\"./img/mlp_regresion.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pabma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - RootMeanSquaredError: 1.0670 - loss: 1.2044 - val_RootMeanSquaredError: 0.7428 - val_loss: 0.5518\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - RootMeanSquaredError: 0.7111 - loss: 0.5061 - val_RootMeanSquaredError: 0.6633 - val_loss: 0.4400\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - RootMeanSquaredError: 0.6762 - loss: 0.4577 - val_RootMeanSquaredError: 0.6493 - val_loss: 0.4216\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - RootMeanSquaredError: 0.6374 - loss: 0.4066 - val_RootMeanSquaredError: 0.6383 - val_loss: 0.4074\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - RootMeanSquaredError: 0.6369 - loss: 0.4058 - val_RootMeanSquaredError: 0.6314 - val_loss: 0.3988\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - RootMeanSquaredError: 0.6263 - loss: 0.3925 - val_RootMeanSquaredError: 0.6289 - val_loss: 0.3956\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - RootMeanSquaredError: 0.6412 - loss: 0.4116 - val_RootMeanSquaredError: 0.6307 - val_loss: 0.3979\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - RootMeanSquaredError: 0.6302 - loss: 0.3973 - val_RootMeanSquaredError: 0.6228 - val_loss: 0.3879\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - RootMeanSquaredError: 0.6245 - loss: 0.3901 - val_RootMeanSquaredError: 0.6204 - val_loss: 0.3849\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - RootMeanSquaredError: 0.6314 - loss: 0.3990 - val_RootMeanSquaredError: 0.6172 - val_loss: 0.3809\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - RootMeanSquaredError: 0.6238 - loss: 0.3892 - val_RootMeanSquaredError: 0.6158 - val_loss: 0.3793\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - RootMeanSquaredError: 0.6222 - loss: 0.3873 - val_RootMeanSquaredError: 0.6221 - val_loss: 0.3870\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - RootMeanSquaredError: 0.6100 - loss: 0.3723 - val_RootMeanSquaredError: 0.6174 - val_loss: 0.3812\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - RootMeanSquaredError: 0.6287 - loss: 0.3956 - val_RootMeanSquaredError: 0.6153 - val_loss: 0.3786\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - RootMeanSquaredError: 0.6197 - loss: 0.3843 - val_RootMeanSquaredError: 0.6300 - val_loss: 0.3969\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - RootMeanSquaredError: 0.6246 - loss: 0.3904 - val_RootMeanSquaredError: 0.6103 - val_loss: 0.3725\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - RootMeanSquaredError: 0.6219 - loss: 0.3870 - val_RootMeanSquaredError: 0.6092 - val_loss: 0.3711\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - RootMeanSquaredError: 0.6037 - loss: 0.3646 - val_RootMeanSquaredError: 0.6029 - val_loss: 0.3636\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - RootMeanSquaredError: 0.6105 - loss: 0.3729 - val_RootMeanSquaredError: 0.6077 - val_loss: 0.3693\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - RootMeanSquaredError: 0.6155 - loss: 0.3790 - val_RootMeanSquaredError: 0.6025 - val_loss: 0.3630\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - RootMeanSquaredError: 0.6255 - loss: 0.3914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = [\"RootMeanSquaredError\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.37658920884132385, 0.6137462258338928]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pabma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\normalization.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.8556 - RootMeanSquaredError: 1.4114 - loss: 2.0538 - val_MeanAbsoluteError: 0.5327 - val_RootMeanSquaredError: 0.7504 - val_loss: 0.5632\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - MeanAbsoluteError: 0.5209 - RootMeanSquaredError: 0.7264 - loss: 0.5279 - val_MeanAbsoluteError: 0.5025 - val_RootMeanSquaredError: 0.7089 - val_loss: 0.5027\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - MeanAbsoluteError: 0.4833 - RootMeanSquaredError: 0.6690 - loss: 0.4480 - val_MeanAbsoluteError: 0.4958 - val_RootMeanSquaredError: 0.6887 - val_loss: 0.4743\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - MeanAbsoluteError: 0.4717 - RootMeanSquaredError: 0.6608 - loss: 0.4370 - val_MeanAbsoluteError: 0.4817 - val_RootMeanSquaredError: 0.6666 - val_loss: 0.4443\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - MeanAbsoluteError: 0.4745 - RootMeanSquaredError: 0.6623 - loss: 0.4389 - val_MeanAbsoluteError: 0.4702 - val_RootMeanSquaredError: 0.6608 - val_loss: 0.4366\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - MeanAbsoluteError: 0.4572 - RootMeanSquaredError: 0.6459 - loss: 0.4173 - val_MeanAbsoluteError: 0.4688 - val_RootMeanSquaredError: 0.6521 - val_loss: 0.4252\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - MeanAbsoluteError: 0.4487 - RootMeanSquaredError: 0.6285 - loss: 0.3951 - val_MeanAbsoluteError: 0.4661 - val_RootMeanSquaredError: 0.6525 - val_loss: 0.4257\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - MeanAbsoluteError: 0.4474 - RootMeanSquaredError: 0.6247 - loss: 0.3903 - val_MeanAbsoluteError: 0.4578 - val_RootMeanSquaredError: 0.6453 - val_loss: 0.4164\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - MeanAbsoluteError: 0.4448 - RootMeanSquaredError: 0.6261 - loss: 0.3922 - val_MeanAbsoluteError: 0.4558 - val_RootMeanSquaredError: 0.6424 - val_loss: 0.4127\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - MeanAbsoluteError: 0.4482 - RootMeanSquaredError: 0.6268 - loss: 0.3930 - val_MeanAbsoluteError: 0.4556 - val_RootMeanSquaredError: 0.6415 - val_loss: 0.4115\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - MeanAbsoluteError: 0.4401 - RootMeanSquaredError: 0.6214 - loss: 0.3863 - val_MeanAbsoluteError: 0.4520 - val_RootMeanSquaredError: 0.6383 - val_loss: 0.4074\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - MeanAbsoluteError: 0.4331 - RootMeanSquaredError: 0.6074 - loss: 0.3691 - val_MeanAbsoluteError: 0.4552 - val_RootMeanSquaredError: 0.6340 - val_loss: 0.4019\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - MeanAbsoluteError: 0.4455 - RootMeanSquaredError: 0.6237 - loss: 0.3893 - val_MeanAbsoluteError: 0.4478 - val_RootMeanSquaredError: 0.6315 - val_loss: 0.3988\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - MeanAbsoluteError: 0.4372 - RootMeanSquaredError: 0.6177 - loss: 0.3817 - val_MeanAbsoluteError: 0.4478 - val_RootMeanSquaredError: 0.6367 - val_loss: 0.4055\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - MeanAbsoluteError: 0.4394 - RootMeanSquaredError: 0.6169 - loss: 0.3808 - val_MeanAbsoluteError: 0.4476 - val_RootMeanSquaredError: 0.6313 - val_loss: 0.3985\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - MeanAbsoluteError: 0.4381 - RootMeanSquaredError: 0.6175 - loss: 0.3817 - val_MeanAbsoluteError: 0.4435 - val_RootMeanSquaredError: 0.6292 - val_loss: 0.3959\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - MeanAbsoluteError: 0.4294 - RootMeanSquaredError: 0.6092 - loss: 0.3713 - val_MeanAbsoluteError: 0.4464 - val_RootMeanSquaredError: 0.6262 - val_loss: 0.3922\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - MeanAbsoluteError: 0.4458 - RootMeanSquaredError: 0.6429 - loss: 0.4137 - val_MeanAbsoluteError: 0.4442 - val_RootMeanSquaredError: 0.6303 - val_loss: 0.3972\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - MeanAbsoluteError: 0.4304 - RootMeanSquaredError: 0.6081 - loss: 0.3701 - val_MeanAbsoluteError: 0.4475 - val_RootMeanSquaredError: 0.6281 - val_loss: 0.3946\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - MeanAbsoluteError: 0.4359 - RootMeanSquaredError: 0.6189 - loss: 0.3832 - val_MeanAbsoluteError: 0.4493 - val_RootMeanSquaredError: 0.6235 - val_loss: 0.3888\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - MeanAbsoluteError: 0.4408 - RootMeanSquaredError: 0.6208 - loss: 0.3857\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    }
   ],
   "source": [
    "#Otra forma pure-Keras:\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "\n",
    "norm_layer = keras.layers.Normalization(input_shape = X_train.shape[1:]) # Es una Standardization\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    norm_layer,\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=optimizer, metrics = [\"RootMeanSquaredError\",\"MeanAbsoluteError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.38239651918411255, 0.4397435784339905, 0.6177532076835632]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva capa al toolbox:\n",
    "\n",
    "Funcionales:  \n",
    "__Normalize__: keras.layers.Normalization -> Nos hace la standardizacion de la entrada \n",
    "Hay que ejecutar el metodo Adapt antes de llamar al fit del modelo que incluya la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es emplear el formato TensorFlow. En este caso crea un directorio con varios ficheros que facilita el despliegue en algunas aplicaciones (ojo, que habría que llevar a producción todo el directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model_otro.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model_k= keras.models.load_model(\"my_keras_model_otro.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - MeanAbsoluteError: 0.4259 - RootMeanSquaredError: 0.5939 - loss: 0.3530\n",
      "Epoch 2/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - MeanAbsoluteError: 0.4317 - RootMeanSquaredError: 0.6227 - loss: 0.3879\n",
      "Epoch 3/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - MeanAbsoluteError: 0.4263 - RootMeanSquaredError: 0.5999 - loss: 0.3603\n",
      "Epoch 4/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - MeanAbsoluteError: 0.4233 - RootMeanSquaredError: 0.6010 - loss: 0.3620\n",
      "Epoch 5/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - MeanAbsoluteError: 0.4292 - RootMeanSquaredError: 0.6060 - loss: 0.3673\n",
      "Epoch 6/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - MeanAbsoluteError: 0.4216 - RootMeanSquaredError: 0.5962 - loss: 0.3557\n",
      "Epoch 7/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - MeanAbsoluteError: 0.4188 - RootMeanSquaredError: 0.5911 - loss: 0.3497\n",
      "Epoch 8/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - MeanAbsoluteError: 0.4251 - RootMeanSquaredError: 0.5986 - loss: 0.3584\n",
      "Epoch 9/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - MeanAbsoluteError: 0.4251 - RootMeanSquaredError: 0.6029 - loss: 0.3636\n",
      "Epoch 10/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - MeanAbsoluteError: 0.4331 - RootMeanSquaredError: 0.6165 - loss: 0.3804\n",
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4271 - RootMeanSquaredError: 0.6099 - loss: 0.3726 - val_MeanAbsoluteError: 0.4305 - val_RootMeanSquaredError: 0.6149 - val_loss: 0.3781\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - MeanAbsoluteError: 0.4108 - RootMeanSquaredError: 0.5779 - loss: 0.3343 - val_MeanAbsoluteError: 0.4392 - val_RootMeanSquaredError: 0.6127 - val_loss: 0.3754\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - MeanAbsoluteError: 0.4246 - RootMeanSquaredError: 0.6034 - loss: 0.3649 - val_MeanAbsoluteError: 0.4309 - val_RootMeanSquaredError: 0.6151 - val_loss: 0.3784\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - MeanAbsoluteError: 0.4219 - RootMeanSquaredError: 0.5984 - loss: 0.3583 - val_MeanAbsoluteError: 0.4325 - val_RootMeanSquaredError: 0.6215 - val_loss: 0.3863\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - MeanAbsoluteError: 0.4300 - RootMeanSquaredError: 0.6324 - loss: 0.4001 - val_MeanAbsoluteError: 0.4292 - val_RootMeanSquaredError: 0.6154 - val_loss: 0.3787\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - MeanAbsoluteError: 0.4248 - RootMeanSquaredError: 0.6307 - loss: 0.3986 - val_MeanAbsoluteError: 0.4328 - val_RootMeanSquaredError: 0.6117 - val_loss: 0.3742\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - MeanAbsoluteError: 0.4221 - RootMeanSquaredError: 0.6020 - loss: 0.3626 - val_MeanAbsoluteError: 0.4321 - val_RootMeanSquaredError: 0.6145 - val_loss: 0.3776\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - MeanAbsoluteError: 0.4232 - RootMeanSquaredError: 0.6125 - loss: 0.3765 - val_MeanAbsoluteError: 0.4264 - val_RootMeanSquaredError: 0.6166 - val_loss: 0.3802\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - MeanAbsoluteError: 0.4135 - RootMeanSquaredError: 0.5879 - loss: 0.3458 - val_MeanAbsoluteError: 0.4286 - val_RootMeanSquaredError: 0.6078 - val_loss: 0.3694\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - MeanAbsoluteError: 0.4264 - RootMeanSquaredError: 0.6018 - loss: 0.3625 - val_MeanAbsoluteError: 0.4356 - val_RootMeanSquaredError: 0.6366 - val_loss: 0.4052\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - MeanAbsoluteError: 0.4142 - RootMeanSquaredError: 0.5872 - loss: 0.3450 - val_MeanAbsoluteError: 0.4236 - val_RootMeanSquaredError: 0.6106 - val_loss: 0.3728\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - MeanAbsoluteError: 0.4173 - RootMeanSquaredError: 0.5983 - loss: 0.3583 - val_MeanAbsoluteError: 0.4255 - val_RootMeanSquaredError: 0.6073 - val_loss: 0.3688\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - MeanAbsoluteError: 0.4185 - RootMeanSquaredError: 0.5981 - loss: 0.3578 - val_MeanAbsoluteError: 0.4229 - val_RootMeanSquaredError: 0.6082 - val_loss: 0.3698\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - MeanAbsoluteError: 0.4181 - RootMeanSquaredError: 0.5955 - loss: 0.3549 - val_MeanAbsoluteError: 0.4286 - val_RootMeanSquaredError: 0.6063 - val_loss: 0.3676\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - MeanAbsoluteError: 0.4133 - RootMeanSquaredError: 0.5873 - loss: 0.3451 - val_MeanAbsoluteError: 0.4219 - val_RootMeanSquaredError: 0.6046 - val_loss: 0.3656\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - MeanAbsoluteError: 0.4060 - RootMeanSquaredError: 0.5739 - loss: 0.3296 - val_MeanAbsoluteError: 0.4208 - val_RootMeanSquaredError: 0.6046 - val_loss: 0.3656\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - MeanAbsoluteError: 0.4111 - RootMeanSquaredError: 0.5804 - loss: 0.3369 - val_MeanAbsoluteError: 0.4287 - val_RootMeanSquaredError: 0.6117 - val_loss: 0.3741\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - MeanAbsoluteError: 0.4084 - RootMeanSquaredError: 0.5823 - loss: 0.3392 - val_MeanAbsoluteError: 0.4365 - val_RootMeanSquaredError: 0.6088 - val_loss: 0.3706\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - MeanAbsoluteError: 0.4122 - RootMeanSquaredError: 0.5876 - loss: 0.3454 - val_MeanAbsoluteError: 0.4200 - val_RootMeanSquaredError: 0.5993 - val_loss: 0.3592\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - MeanAbsoluteError: 0.4112 - RootMeanSquaredError: 0.5849 - loss: 0.3421 - val_MeanAbsoluteError: 0.4190 - val_RootMeanSquaredError: 0.5988 - val_loss: 0.3586\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.keras\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - MeanAbsoluteError: 0.4053 - RootMeanSquaredError: 0.5766 - loss: 0.3326 - val_MeanAbsoluteError: 0.4208 - val_RootMeanSquaredError: 0.5991 - val_loss: 0.3589\n",
      "Epoch 2/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - MeanAbsoluteError: 0.4018 - RootMeanSquaredError: 0.5758 - loss: 0.3317 - val_MeanAbsoluteError: 0.4221 - val_RootMeanSquaredError: 0.6033 - val_loss: 0.3639\n",
      "Epoch 3/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - MeanAbsoluteError: 0.4082 - RootMeanSquaredError: 0.5844 - loss: 0.3418 - val_MeanAbsoluteError: 0.4237 - val_RootMeanSquaredError: 0.5984 - val_loss: 0.3581\n",
      "Epoch 4/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - MeanAbsoluteError: 0.4087 - RootMeanSquaredError: 0.5813 - loss: 0.3380 - val_MeanAbsoluteError: 0.4213 - val_RootMeanSquaredError: 0.6009 - val_loss: 0.3610\n",
      "Epoch 5/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - MeanAbsoluteError: 0.4066 - RootMeanSquaredError: 0.5773 - loss: 0.3334 - val_MeanAbsoluteError: 0.4191 - val_RootMeanSquaredError: 0.5975 - val_loss: 0.3570\n",
      "Epoch 6/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - MeanAbsoluteError: 0.4081 - RootMeanSquaredError: 0.5877 - loss: 0.3456 - val_MeanAbsoluteError: 0.4160 - val_RootMeanSquaredError: 0.5991 - val_loss: 0.3589\n",
      "Epoch 7/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - MeanAbsoluteError: 0.4088 - RootMeanSquaredError: 0.5835 - loss: 0.3408 - val_MeanAbsoluteError: 0.4148 - val_RootMeanSquaredError: 0.5940 - val_loss: 0.3528\n",
      "Epoch 8/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - MeanAbsoluteError: 0.4081 - RootMeanSquaredError: 0.5790 - loss: 0.3353 - val_MeanAbsoluteError: 0.4142 - val_RootMeanSquaredError: 0.5939 - val_loss: 0.3527\n",
      "Epoch 9/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - MeanAbsoluteError: 0.3977 - RootMeanSquaredError: 0.5670 - loss: 0.3216 - val_MeanAbsoluteError: 0.4169 - val_RootMeanSquaredError: 0.5943 - val_loss: 0.3532\n",
      "Epoch 10/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - MeanAbsoluteError: 0.4024 - RootMeanSquaredError: 0.5755 - loss: 0.3314 - val_MeanAbsoluteError: 0.4192 - val_RootMeanSquaredError: 0.5963 - val_loss: 0.3555\n",
      "Epoch 11/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - MeanAbsoluteError: 0.4111 - RootMeanSquaredError: 0.5898 - loss: 0.3482 - val_MeanAbsoluteError: 0.4208 - val_RootMeanSquaredError: 0.5941 - val_loss: 0.3529\n",
      "Epoch 12/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - MeanAbsoluteError: 0.4112 - RootMeanSquaredError: 0.5869 - loss: 0.3446 - val_MeanAbsoluteError: 0.4415 - val_RootMeanSquaredError: 0.6074 - val_loss: 0.3689\n",
      "Epoch 13/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - MeanAbsoluteError: 0.3991 - RootMeanSquaredError: 0.5646 - loss: 0.3189 - val_MeanAbsoluteError: 0.4138 - val_RootMeanSquaredError: 0.5923 - val_loss: 0.3508\n",
      "Epoch 14/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - MeanAbsoluteError: 0.3970 - RootMeanSquaredError: 0.5689 - loss: 0.3237 - val_MeanAbsoluteError: 0.4082 - val_RootMeanSquaredError: 0.5939 - val_loss: 0.3527\n",
      "Epoch 15/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - MeanAbsoluteError: 0.3941 - RootMeanSquaredError: 0.5633 - loss: 0.3174 - val_MeanAbsoluteError: 0.4154 - val_RootMeanSquaredError: 0.5904 - val_loss: 0.3486\n",
      "Epoch 16/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - MeanAbsoluteError: 0.4000 - RootMeanSquaredError: 0.5694 - loss: 0.3243 - val_MeanAbsoluteError: 0.4181 - val_RootMeanSquaredError: 0.6039 - val_loss: 0.3647\n",
      "Epoch 17/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - MeanAbsoluteError: 0.4014 - RootMeanSquaredError: 0.5780 - loss: 0.3345 - val_MeanAbsoluteError: 0.4107 - val_RootMeanSquaredError: 0.5910 - val_loss: 0.3493\n",
      "Epoch 18/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - MeanAbsoluteError: 0.3991 - RootMeanSquaredError: 0.5700 - loss: 0.3250 - val_MeanAbsoluteError: 0.4219 - val_RootMeanSquaredError: 0.6047 - val_loss: 0.3656\n",
      "Epoch 19/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - MeanAbsoluteError: 0.3966 - RootMeanSquaredError: 0.5654 - loss: 0.3200 - val_MeanAbsoluteError: 0.4099 - val_RootMeanSquaredError: 0.5886 - val_loss: 0.3464\n",
      "Epoch 20/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - MeanAbsoluteError: 0.3925 - RootMeanSquaredError: 0.5563 - loss: 0.3099 - val_MeanAbsoluteError: 0.4105 - val_RootMeanSquaredError: 0.5866 - val_loss: 0.3441\n",
      "Epoch 21/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4027 - RootMeanSquaredError: 0.5769 - loss: 0.3329 - val_MeanAbsoluteError: 0.4117 - val_RootMeanSquaredError: 0.5884 - val_loss: 0.3462\n",
      "Epoch 22/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - MeanAbsoluteError: 0.4038 - RootMeanSquaredError: 0.5834 - loss: 0.3409 - val_MeanAbsoluteError: 0.4109 - val_RootMeanSquaredError: 0.5893 - val_loss: 0.3473\n",
      "Epoch 23/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - MeanAbsoluteError: 0.3893 - RootMeanSquaredError: 0.5575 - loss: 0.3112 - val_MeanAbsoluteError: 0.4075 - val_RootMeanSquaredError: 0.5887 - val_loss: 0.3465\n",
      "Epoch 24/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - MeanAbsoluteError: 0.3921 - RootMeanSquaredError: 0.5622 - loss: 0.3162 - val_MeanAbsoluteError: 0.4056 - val_RootMeanSquaredError: 0.5885 - val_loss: 0.3463\n",
      "Epoch 25/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - MeanAbsoluteError: 0.3908 - RootMeanSquaredError: 0.5630 - loss: 0.3172 - val_MeanAbsoluteError: 0.4637 - val_RootMeanSquaredError: 0.8075 - val_loss: 0.6520\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "10 esta bien. Lo pondemos a 5 para el ejercicio\n",
    "¿Qué considera como dejar de mejorar? parametros min_delta y baseline\n",
    "'''\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, \n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guía \"casera\":\n",
    "\n",
    "1- Recetas (adaptada de \"Hands-on...\")  PARA MLPs!!!!    \n",
    "    * Capas:   \n",
    "        - Empezar con una capa oculta e ir añadiendo (dependiendo de la complejidad del problema, probar wide & deep)\n",
    "        - Si pocas features -> más neuronas  (aumentar la combinación de features) (num_features < 100) [Orientativo]  \n",
    "        - Si muchas features  -> menos neuronas (proyección tipo PCA) (num_features > 1000) [Orientativo] e ir aumentando en capas sucesivas\n",
    "        - O empezar con muchas (doble de tus features e ir \"estrechando los pantalones\")  \n",
    "        - Construcción en prisma o pirámide (para empezar)  \n",
    "        - Inicialización: Empezar con Glorot, cambiar a He  \n",
    "        - Activación: ReLU salvo la última, si muchas capas probar -> SELU o Swish (con el inicializador a LeCunn) \n",
    "    * Optimizadores:   \n",
    "        - Si muchos datos*features -> Adam o AdamW con sus valores por defecto  \n",
    "        - Si no, SGD con Nesterov activado, y momento a 0.9  \n",
    "        - Learning rate -> 0.001-0.0001 para empezar e ir creciendo (learning-rate warm-up) (Si te atreves, buscar adaptative learning rate y optimizar con esto)  \n",
    "    * Entrenamiento:  \n",
    "        - Epoch, probar con pocas para ver duración -> Epochs altas y Callback de Early Stop activado  \n",
    "        - Batch_Size -> 32, si tienes muchos datos y una GPU a mano puedes subir mucho 64,128,256...\n",
    "    * Regularización (lo veremos):  \n",
    "        - Dropout al 0.25-0.5 (sin SELU)\n",
    "\n",
    "\n",
    "\n",
    "2- Pasos  \n",
    "    - Si overfitting -> Regularizar: Earlystopping, Dropout (lo veremos en la siguiente sección)  \n",
    "    - Comprobar underfitting -> Aumentar epochs, aumentar batch_size  \n",
    "    - Jugar con optimizador: learning rate (de pequeño a grande), tipo de optimizador   \n",
    "    - Jugar con número de capas (ojo al overfitting) y las funciones de activación y la inicialización de pesos  \n",
    "    - Jugar con el número de neuronas por capa (suele ser piramide o prisma, pero puedes jugar a expandir dimensiones)  \n",
    "    - Combinar los dos anteriores  \n",
    "\n",
    "3- Keras Tuner:\n",
    "    https://keras.io/guides/keras_tuner/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Keras tiene implementado un dashboard para monitorizar las ejecuciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crea este directorio\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# Guarda una carpeta nueva con la fecha de la ejecucion\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3995 - RootMeanSquaredError: 0.5683 - loss: 0.3231 - val_MeanAbsoluteError: 0.4072 - val_RootMeanSquaredError: 0.5902 - val_loss: 0.3483\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4029 - RootMeanSquaredError: 0.5970 - loss: 0.3575 - val_MeanAbsoluteError: 0.4144 - val_RootMeanSquaredError: 0.5973 - val_loss: 0.3567\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3977 - RootMeanSquaredError: 0.5668 - loss: 0.3213 - val_MeanAbsoluteError: 0.4079 - val_RootMeanSquaredError: 0.5923 - val_loss: 0.3508\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3947 - RootMeanSquaredError: 0.5671 - loss: 0.3217 - val_MeanAbsoluteError: 0.4117 - val_RootMeanSquaredError: 0.5900 - val_loss: 0.3481\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3961 - RootMeanSquaredError: 0.5640 - loss: 0.3183 - val_MeanAbsoluteError: 0.4075 - val_RootMeanSquaredError: 0.5877 - val_loss: 0.3454\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3940 - RootMeanSquaredError: 0.5622 - loss: 0.3162 - val_MeanAbsoluteError: 0.4078 - val_RootMeanSquaredError: 0.5945 - val_loss: 0.3534\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3930 - RootMeanSquaredError: 0.5608 - loss: 0.3146 - val_MeanAbsoluteError: 0.4068 - val_RootMeanSquaredError: 0.5920 - val_loss: 0.3505\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3946 - RootMeanSquaredError: 0.5690 - loss: 0.3241 - val_MeanAbsoluteError: 0.4072 - val_RootMeanSquaredError: 0.5862 - val_loss: 0.3436\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3927 - RootMeanSquaredError: 0.5594 - loss: 0.3131 - val_MeanAbsoluteError: 0.4016 - val_RootMeanSquaredError: 0.5847 - val_loss: 0.3419\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3905 - RootMeanSquaredError: 0.5590 - loss: 0.3126 - val_MeanAbsoluteError: 0.4035 - val_RootMeanSquaredError: 0.5855 - val_loss: 0.3428\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3948 - RootMeanSquaredError: 0.5642 - loss: 0.3184 - val_MeanAbsoluteError: 0.4002 - val_RootMeanSquaredError: 0.5817 - val_loss: 0.3383\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3843 - RootMeanSquaredError: 0.5476 - loss: 0.3003 - val_MeanAbsoluteError: 0.3999 - val_RootMeanSquaredError: 0.5840 - val_loss: 0.3411\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3849 - RootMeanSquaredError: 0.5513 - loss: 0.3041 - val_MeanAbsoluteError: 0.4041 - val_RootMeanSquaredError: 0.5955 - val_loss: 0.3547\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3905 - RootMeanSquaredError: 0.5661 - loss: 0.3209 - val_MeanAbsoluteError: 0.4190 - val_RootMeanSquaredError: 0.6143 - val_loss: 0.3774\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3897 - RootMeanSquaredError: 0.5633 - loss: 0.3177 - val_MeanAbsoluteError: 0.4003 - val_RootMeanSquaredError: 0.5897 - val_loss: 0.3478\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3869 - RootMeanSquaredError: 0.5547 - loss: 0.3080 - val_MeanAbsoluteError: 0.4076 - val_RootMeanSquaredError: 0.5845 - val_loss: 0.3417\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3949 - RootMeanSquaredError: 0.5748 - loss: 0.3307 - val_MeanAbsoluteError: 0.4027 - val_RootMeanSquaredError: 0.5824 - val_loss: 0.3392\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3966 - RootMeanSquaredError: 0.5771 - loss: 0.3333 - val_MeanAbsoluteError: 0.3983 - val_RootMeanSquaredError: 0.5814 - val_loss: 0.3381\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3961 - RootMeanSquaredError: 0.5704 - loss: 0.3255 - val_MeanAbsoluteError: 0.3998 - val_RootMeanSquaredError: 0.5796 - val_loss: 0.3360\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3873 - RootMeanSquaredError: 0.5620 - loss: 0.3161 - val_MeanAbsoluteError: 0.4006 - val_RootMeanSquaredError: 0.5808 - val_loss: 0.3373\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3862 - RootMeanSquaredError: 0.5518 - loss: 0.3046 - val_MeanAbsoluteError: 0.4061 - val_RootMeanSquaredError: 0.5823 - val_loss: 0.3391\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4223 - RootMeanSquaredError: 0.7353 - loss: 0.5569 - val_MeanAbsoluteError: 0.4049 - val_RootMeanSquaredError: 0.5915 - val_loss: 0.3499\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3937 - RootMeanSquaredError: 0.5751 - loss: 0.3310 - val_MeanAbsoluteError: 0.4069 - val_RootMeanSquaredError: 0.5846 - val_loss: 0.3417\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3906 - RootMeanSquaredError: 0.5591 - loss: 0.3130 - val_MeanAbsoluteError: 0.4004 - val_RootMeanSquaredError: 0.5872 - val_loss: 0.3447\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3838 - RootMeanSquaredError: 0.5488 - loss: 0.3013 - val_MeanAbsoluteError: 0.4026 - val_RootMeanSquaredError: 0.5927 - val_loss: 0.3513\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3887 - RootMeanSquaredError: 0.5608 - loss: 0.3147 - val_MeanAbsoluteError: 0.4059 - val_RootMeanSquaredError: 0.5790 - val_loss: 0.3352\n",
      "Epoch 27/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3938 - RootMeanSquaredError: 0.5716 - loss: 0.3279 - val_MeanAbsoluteError: 0.4197 - val_RootMeanSquaredError: 0.6109 - val_loss: 0.3731\n",
      "Epoch 28/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4079 - RootMeanSquaredError: 0.5865 - loss: 0.3447 - val_MeanAbsoluteError: 0.4247 - val_RootMeanSquaredError: 0.5991 - val_loss: 0.3588\n",
      "Epoch 29/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4046 - RootMeanSquaredError: 0.5767 - loss: 0.3326 - val_MeanAbsoluteError: 0.4121 - val_RootMeanSquaredError: 0.5910 - val_loss: 0.3493\n",
      "Epoch 30/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3996 - RootMeanSquaredError: 0.5716 - loss: 0.3268 - val_MeanAbsoluteError: 0.4080 - val_RootMeanSquaredError: 0.5898 - val_loss: 0.3479\n",
      "Epoch 31/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3995 - RootMeanSquaredError: 0.5705 - loss: 0.3256 - val_MeanAbsoluteError: 0.4123 - val_RootMeanSquaredError: 0.5894 - val_loss: 0.3473\n",
      "Epoch 32/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3948 - RootMeanSquaredError: 0.5642 - loss: 0.3186 - val_MeanAbsoluteError: 0.4069 - val_RootMeanSquaredError: 0.5863 - val_loss: 0.3437\n",
      "Epoch 33/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3952 - RootMeanSquaredError: 0.5646 - loss: 0.3189 - val_MeanAbsoluteError: 0.4146 - val_RootMeanSquaredError: 0.5862 - val_loss: 0.3436\n",
      "Epoch 34/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4028 - RootMeanSquaredError: 0.5732 - loss: 0.3287 - val_MeanAbsoluteError: 0.4205 - val_RootMeanSquaredError: 0.5915 - val_loss: 0.3499\n",
      "Epoch 35/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4025 - RootMeanSquaredError: 0.5791 - loss: 0.3358 - val_MeanAbsoluteError: 0.4048 - val_RootMeanSquaredError: 0.5917 - val_loss: 0.3501\n",
      "Epoch 36/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3968 - RootMeanSquaredError: 0.5719 - loss: 0.3272 - val_MeanAbsoluteError: 0.4072 - val_RootMeanSquaredError: 0.5841 - val_loss: 0.3411\n",
      "Epoch 37/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3908 - RootMeanSquaredError: 0.5543 - loss: 0.3076 - val_MeanAbsoluteError: 0.4043 - val_RootMeanSquaredError: 0.5865 - val_loss: 0.3440\n",
      "Epoch 38/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3899 - RootMeanSquaredError: 0.5548 - loss: 0.3080 - val_MeanAbsoluteError: 0.4023 - val_RootMeanSquaredError: 0.5856 - val_loss: 0.3429\n",
      "Epoch 39/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3927 - RootMeanSquaredError: 0.5551 - loss: 0.3083 - val_MeanAbsoluteError: 0.4056 - val_RootMeanSquaredError: 0.5839 - val_loss: 0.3410\n",
      "Epoch 40/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3884 - RootMeanSquaredError: 0.5523 - loss: 0.3051 - val_MeanAbsoluteError: 0.4021 - val_RootMeanSquaredError: 0.5802 - val_loss: 0.3366\n",
      "Epoch 41/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3801 - RootMeanSquaredError: 0.5432 - loss: 0.2953 - val_MeanAbsoluteError: 0.4041 - val_RootMeanSquaredError: 0.5835 - val_loss: 0.3405\n",
      "Epoch 42/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3903 - RootMeanSquaredError: 0.5559 - loss: 0.3092 - val_MeanAbsoluteError: 0.4079 - val_RootMeanSquaredError: 0.5826 - val_loss: 0.3394\n",
      "Epoch 43/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3912 - RootMeanSquaredError: 0.5641 - loss: 0.3185 - val_MeanAbsoluteError: 0.4027 - val_RootMeanSquaredError: 0.5785 - val_loss: 0.3347\n",
      "Epoch 44/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3918 - RootMeanSquaredError: 0.5608 - loss: 0.3146 - val_MeanAbsoluteError: 0.4089 - val_RootMeanSquaredError: 0.5878 - val_loss: 0.3454\n",
      "Epoch 45/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3960 - RootMeanSquaredError: 0.5674 - loss: 0.3223 - val_MeanAbsoluteError: 0.4042 - val_RootMeanSquaredError: 0.5796 - val_loss: 0.3359\n",
      "Epoch 46/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3921 - RootMeanSquaredError: 0.5621 - loss: 0.3160 - val_MeanAbsoluteError: 0.4005 - val_RootMeanSquaredError: 0.5778 - val_loss: 0.3339\n",
      "Epoch 47/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3948 - RootMeanSquaredError: 0.5637 - loss: 0.3179 - val_MeanAbsoluteError: 0.3997 - val_RootMeanSquaredError: 0.5802 - val_loss: 0.3366\n",
      "Epoch 48/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3913 - RootMeanSquaredError: 0.5619 - loss: 0.3159 - val_MeanAbsoluteError: 0.3996 - val_RootMeanSquaredError: 0.5817 - val_loss: 0.3384\n",
      "Epoch 49/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3849 - RootMeanSquaredError: 0.5532 - loss: 0.3062 - val_MeanAbsoluteError: 0.4029 - val_RootMeanSquaredError: 0.5790 - val_loss: 0.3353\n",
      "Epoch 50/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3861 - RootMeanSquaredError: 0.5518 - loss: 0.3046 - val_MeanAbsoluteError: 0.3970 - val_RootMeanSquaredError: 0.5790 - val_loss: 0.3353\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPara lanzarlo desde el jupyter notebook\\n%load_ext tensorboard\\n%tensorboard --logdir=./my_logs --port=6006\\n\\nPara lanzarlo desde el terminal, hay que estar en la carpeta de los logs\\ntensorboard --logdir=./my_logs --port=6006\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Para lanzarlo desde el jupyter notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "Para lanzarlo desde el terminal, hay que estar en la carpeta de los logs\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "167a7833a0358ac30a26ad970c5914014f41a5348f3dc652232a762d6e3283fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
