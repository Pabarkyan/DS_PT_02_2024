{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPor defecto, keras no tira de GPU\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "# !pip install keras (NO NECESARIO YA INTEGRADO EN TENSORFLOW)\n",
    "'''\n",
    "Por defecto, keras no tira de GPU\n",
    "'''\n",
    "#https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar y abrir VS Code si hace falta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[25000,12,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays). Aplano a una dimension cada imagen.\n",
    "# Escalamos ya que vamos a usar gradient descent, y le afecta mucho la escala de las features.\n",
    "# Ejecutar esta celda solo una vez. Sino reescalará\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10,000 samples for validation. Entraran dentro del modelo para validar. No es validacion cruzada\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICION/CONSTRUCCION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax.  \n",
    "\n",
    "Es decir vamos a volver a montar esta arquitectura:  \n",
    "<img src=\"./img/mlp_clasification.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Un poco más sobre la activación softmax:    \n",
    "\n",
    "Fórmula:  \n",
    "<img src=\"./img/softmax_function.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Función de transferencia:  \n",
    "<img src=\"./img/softmax_activation.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Ejemplo de funcionamiento:  \n",
    "<img src=\"./img/softmax_example.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonatan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Una manera de declarar la red neuronal\n",
    "\n",
    "# Siempre hay que declarar la capa sequential para empezar a declarar la red\n",
    "# Se trata de la API sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Flatten, aplana en un unico vector. Y especificamos el tamaño de la entrada\n",
    "# Es como si hiciese un .reshape(-1, 28*28)\n",
    "# \"kernel_initializer\" o \"bias_initializer\" No lo usamos pero seria para inicializar los pesos de otra manera\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Capas de la red. Dense es la capa de neuronas. Necesitamos numero y activacion\n",
    "model.add(keras.layers.Dense(units = 300, # Numero de neuronas de la capa\n",
    "                             activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(100,\n",
    "                             activation='relu'))\n",
    "\n",
    "# Capa de salida, con tamaño del número de clases\n",
    "# Suele ir aqui un softmax. Para multiclase guay. Si es binaria -> sigmoide\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "]\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y una forma mediante encademaniento de funciones (usando lo que se denomina la Functional API)\n",
    "input_layer = keras.layers.Input(shape = (28,28))\n",
    "flatten_layer = keras.layers.Flatten()(input_layer)\n",
    "hidden_1 = keras.layers.Dense(300, activation = \"relu\")(flatten_layer)\n",
    "hidden_2 = keras.layers.Dense(100, activation = \"relu\")(hidden_1)\n",
    "output = keras.layers.Dense(10, activation = \"softmax\")(hidden_2)\n",
    "model = keras.Model(inputs = [input_layer], outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Flatten name=flatten_2, built=True>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<InputLayer name=input_layer_2, built=True>,\n",
       " <Flatten name=flatten_2, built=True>,\n",
       " <Dense name=dense_6, built=True>,\n",
       " <Dense name=dense_7, built=True>,\n",
       " <Dense name=dense_8, built=True>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[1])\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[2]\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "# 784 features (pixeles de las imagenes) x 300 neuronas\n",
    "# Los pesos están inicializados aleatoriamente\n",
    "weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializadores:  \n",
    "- Los pesos inicializados a cero -> No aprendizaje\n",
    "- Desde siempre se inicializan \"aleatoriamente\", pero no sólo de forma uniforme (todos los valores con la misma probabilidad), sino que se emplean diferentes distribuciones de probabilidad con parámetros que dependen del número de entradas y salidas de la capa. El objetivo esintentar que las varianzas de las entradas sean similares a las de las salidas y evitar el problema del gradiente que se desvanece (\"Vanishing Gradient\" problem):  \n",
    "    *   Glorot inizialization (por defecto la de Keras, con función uniforme de distribución) -> Para cuando tienes funciones de activación (ninguna, tanh, sigmoid, softmax, aunque también se usa por defecto :-) para casi todo) [Xavier Glorot & Yoshua Bengio]\n",
    "    *   He inizialization, -> Para cuando tienes ReLU, Leaky ReLU, ELU, GELU, Swish, Mish [He Kaiming et al.]\n",
    "    *   LeCunn inizialization -> Para cuando tienes SELU [Jean LeCunn]\n",
    "- Es un hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica la configuración del entrenamiento (optimizador, pérdida, métricas):\n",
    "model.compile(\n",
    "    # Stocastic gradient descent. El algoritmo para minimizar la loss function\n",
    "    # El stocastic va haciendo muestreo en cada evaluacion, no usa todo el dataset\n",
    "    # Podemos modificar el learning rate(0.01 por defecto) mediante el parametro lr\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer, con parámetros por defecto\n",
    "    \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    # sparse_categorical_crossentropy cuando tenemos un label en nuna columna\n",
    "    # Si lo tuviesemos en varias tipo dummy, cogeriamos categorical_crossentropy\n",
    "    # binary_crossentropy si es una neurona, clasi binario\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente (... casi, los parámetros del optimizador serán los que tenga por defecto)\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary\n",
    "# La primera hidden layer tiene 784 entradas x 300 salidas\n",
    "# Son los 235500 params = 783x300 + 300 (bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAPAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos construyendo nuestra __lista de capas__ (para guardar en el \"Toolbox\"):\n",
    "\n",
    "__Entrenables__:  \n",
    "__* Dense__ -> Capa completamente conectada a las neuronas de la capa anterior y a la posterior  \n",
    "    Hiperparámetros asociados:     \n",
    "        * units: Number of neurons, dimensionality of the output space  \n",
    "        * activation: Activation function to use. If you don't specify anything, no activation is applied  \n",
    "        * kernel_initializer: Initializer for the kernel weights matrix.  \n",
    "        * bias_initializer: Initializer for the bias vector. (Suelen inicializarse a cero)\n",
    "        * Kernel_regularizar: Los clásicos (L1,L2,...)\n",
    " \n",
    "__Funcionales__:       \n",
    "__* Input__ -> Capa para definir la forma de la entrada (shape), que se puede pasar como input_shape\n",
    "__* Flatten__ -> Capa que aplana (convierte su entrada en un vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones de activación interesantes: SELU (1.67*ELU) y Swish (también SiLU, o Sigmoid linear unit)... No entrar en pánico, vais a usar ReLU, Softmax y no-activation, y en algunos casos (quizás): sigmoid, tanh y las (x)LU (SELU, Siwsh,etc)\n",
    "\n",
    "<img src=\"./img/activation_functions.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMIZADORES\n",
    "\n",
    "##### Ejemplo \"sencillo\" https://medium.com/@axegggl/newtons-method-for-optimization-in-python-11ce261fcf98"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también vamos completando lista de hiperparámetros, estos asociados al \"Optimizador\"/\"Modelo\":  \n",
    "Tipo de optimizador:  \n",
    "* __SGD__, Gradient descent \"genérico\" (puedes añadirle \"momento\", es decir que a la hora de descontar el gradiente tenga en cuenta el vector medio de gradientes pasados)\n",
    "  \n",
    "\n",
    "* __Adagrad__, Hace gradient descent pero ajusta el gradiente para compensar las componentes de mayor valor numérico (es como evitar irse por las pendientes más inclinadas)... Es decir evita irse a mínimos locales al precio de enlentecer el entrenamiento.    \n",
    "\n",
    "* __RMSprop__, Versión de AdaGrad, pero considera principalmente los últimos valores del gradiente. Es decir, busca lo bueno de Adagrad reduciendo sus peligros.    \n",
    "\n",
    "* __Adam__, _Adaptative Moment Estimation_, combina RMSProp y el uso de momento. Es el rey actual (junto con sus versiones) para grandes cantidades de datos.    \n",
    "\n",
    "* __AdamW, Nadam, AdaMax__, variantes del anterior. \n",
    "\n",
    "Comparativa, donde * es malo y *** bueno (extraído del \"Hands-on Machine Learning with....\" de Aurelien Geron, 3a Edicion)\n",
    "\n",
    "<img src=\"./img/Comparativa-optimizadores.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Keras también permite:\n",
    "__Adadelta__ _(variante de Adagrad)_, __Adafactor__ y __Ftrl__\n",
    "\n",
    "      \n",
    "Hiperparámetros Genéricos:\n",
    "Learning Rate: Coeficiente aplicado al descenso de gradiente, como en otros modelos que ya hemos visto\n",
    "Asociados al Gradient Clipping: clipnorm, clipvalue, global_clipnorm\n",
    "\n",
    "Cada optimizador además puede tener sus propios hiperparámetros (ver: https://keras.io/api/optimizers/)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuciones de pérdida y métricas\n",
    "__Función de perdida__: La función a minimizar durante el entrenamiento (son las mismas que en otros modelos no Deep)  \n",
    "- Clasificación: En clases Keras -> __BinaryCrossEntropy, CategoricalCrossEntropy, SparseCategoricalCrossEntropy__  \n",
    "- Regresión: En clases Keras -> __MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, CosineSimilarity__   \n",
    "\n",
    "__Métricas__:  \n",
    "- Regresión: __MAE, MSE, MAPE__ :-)  \n",
    "- Clasificación: __Accuracy, Precision, Recall, f1, AuRoC__  \n",
    "\n",
    "¿Cuál es la diferencia entre Categorical y Sparse? ¿Por qué las funciones de pérdida son diferentes a las métricas en Clasificación? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El __batch_size__ es la cantidad de muestras que utiliza el SGD, y las __epochs__ son las iteraciones que realiza en el entrenamiento. (Son hiperparámetros de entrenamiento)    \n",
    "\n",
    "En una epoch se entrenan tantos batches como sea necesario para recorrer todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6308 - loss: 1.4006 - val_accuracy: 0.8911 - val_loss: 0.3980\n",
      "Epoch 2/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8897 - loss: 0.3995 - val_accuracy: 0.9164 - val_loss: 0.3044\n",
      "Epoch 3/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9076 - loss: 0.3247 - val_accuracy: 0.9263 - val_loss: 0.2648\n",
      "Epoch 4/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9171 - loss: 0.2858 - val_accuracy: 0.9310 - val_loss: 0.2444\n",
      "Epoch 5/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9267 - loss: 0.2553 - val_accuracy: 0.9358 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.2324 - val_accuracy: 0.9424 - val_loss: 0.2123\n",
      "Epoch 7/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9389 - loss: 0.2163 - val_accuracy: 0.9441 - val_loss: 0.2049\n",
      "Epoch 8/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9421 - loss: 0.2030 - val_accuracy: 0.9495 - val_loss: 0.1895\n",
      "Epoch 9/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9456 - loss: 0.1880 - val_accuracy: 0.9536 - val_loss: 0.1759\n",
      "Epoch 10/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.1815 - val_accuracy: 0.9542 - val_loss: 0.1691\n",
      "Epoch 11/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.1678 - val_accuracy: 0.9570 - val_loss: 0.1610\n",
      "Epoch 12/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9537 - loss: 0.1619 - val_accuracy: 0.9569 - val_loss: 0.1531\n",
      "Epoch 13/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1504 - val_accuracy: 0.9590 - val_loss: 0.1484\n",
      "Epoch 14/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9612 - loss: 0.1377 - val_accuracy: 0.9614 - val_loss: 0.1440\n",
      "Epoch 15/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9597 - loss: 0.1392 - val_accuracy: 0.9628 - val_loss: 0.1370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEn el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\\nSi vemos que ya no baja mas, no serán necesarias tantas epochs.\\nImprimera tantas lineas como epochs hayamos puesto\\n\\nTampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\\nUtil para datasets desbalanceados.\\n\\nEl loss que muestra es el categoricalcrossentropy\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64, # numero de muestras empleadas en el entrenamiento de SGD\n",
    "    epochs=15, # 1 por defecto. Insuficiente. Numero de vueltas del backpropagation\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    # En vez de validation data podemos usar el argumento validation_split=0.1\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9628 - loss: 0.1295 - val_accuracy: 0.9636 - val_loss: 0.1339\n",
      "Epoch 2/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1204 - val_accuracy: 0.9640 - val_loss: 0.1292\n",
      "Epoch 3/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.1184 - val_accuracy: 0.9661 - val_loss: 0.1243\n",
      "Epoch 4/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.1131 - val_accuracy: 0.9654 - val_loss: 0.1245\n",
      "Epoch 5/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.1054 - val_accuracy: 0.9675 - val_loss: 0.1182\n",
      "Epoch 6/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9723 - loss: 0.1012 - val_accuracy: 0.9690 - val_loss: 0.1148\n",
      "Epoch 7/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9730 - loss: 0.0984 - val_accuracy: 0.9692 - val_loss: 0.1124\n",
      "Epoch 8/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0937 - val_accuracy: 0.9694 - val_loss: 0.1106\n",
      "Epoch 9/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0948 - val_accuracy: 0.9695 - val_loss: 0.1093\n",
      "Epoch 10/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0883 - val_accuracy: 0.9692 - val_loss: 0.1099\n",
      "Epoch 11/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0827 - val_accuracy: 0.9711 - val_loss: 0.1049\n",
      "Epoch 12/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0797 - val_accuracy: 0.9704 - val_loss: 0.1051\n",
      "Epoch 13/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0789 - val_accuracy: 0.9728 - val_loss: 0.0993\n",
      "Epoch 14/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0745 - val_accuracy: 0.9723 - val_loss: 0.1001\n",
      "Epoch 15/15\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.0697 - val_accuracy: 0.9726 - val_loss: 0.0977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24d58ed5670>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 'auto', 'epochs': 15, 'steps': 782}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.7841799855232239,\n",
       "  0.895799994468689,\n",
       "  0.9119200110435486,\n",
       "  0.9210000038146973,\n",
       "  0.9286800026893616,\n",
       "  0.9340599775314331,\n",
       "  0.9387999773025513,\n",
       "  0.9431800246238708,\n",
       "  0.9462599754333496,\n",
       "  0.9495999813079834,\n",
       "  0.9526000022888184,\n",
       "  0.9549000263214111,\n",
       "  0.9569600224494934,\n",
       "  0.9597600102424622,\n",
       "  0.9618399739265442],\n",
       " 'loss': [0.895813524723053,\n",
       "  0.3739772140979767,\n",
       "  0.30946680903434753,\n",
       "  0.2744063138961792,\n",
       "  0.24985529482364655,\n",
       "  0.2304764837026596,\n",
       "  0.2141077220439911,\n",
       "  0.1997520476579666,\n",
       "  0.18756356835365295,\n",
       "  0.17625410854816437,\n",
       "  0.16616183519363403,\n",
       "  0.15724296867847443,\n",
       "  0.1489691138267517,\n",
       "  0.14156195521354675,\n",
       "  0.1342805027961731],\n",
       " 'val_accuracy': [0.8910999894142151,\n",
       "  0.9164000153541565,\n",
       "  0.9262999892234802,\n",
       "  0.9309999942779541,\n",
       "  0.9358000159263611,\n",
       "  0.9423999786376953,\n",
       "  0.944100022315979,\n",
       "  0.9495000243186951,\n",
       "  0.9535999894142151,\n",
       "  0.954200029373169,\n",
       "  0.9570000171661377,\n",
       "  0.9569000005722046,\n",
       "  0.9589999914169312,\n",
       "  0.9613999724388123,\n",
       "  0.9628000259399414],\n",
       " 'val_loss': [0.3979733884334564,\n",
       "  0.3044237494468689,\n",
       "  0.2648281157016754,\n",
       "  0.24438032507896423,\n",
       "  0.2320060282945633,\n",
       "  0.21229177713394165,\n",
       "  0.20489005744457245,\n",
       "  0.1894899606704712,\n",
       "  0.17587219178676605,\n",
       "  0.16910426318645477,\n",
       "  0.16095511615276337,\n",
       "  0.1530623584985733,\n",
       "  0.1484019011259079,\n",
       "  0.14396832883358002,\n",
       "  0.13696062564849854]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Podemos ver como evoluciona el entrenamiento, en funcion de los epochs\n",
    "# Validacion y training estan muy cerca, no hay overfitting!\n",
    "# Todavia no ha acabado de coverger ya que el loss en validacion sigue bajando,\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1163\n",
      "test loss, test acc: [0.09840267896652222, 0.9714000225067139]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el \"score\" a partir del conjunto de test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "\n",
    "# Metodo evaluate para que nos de el error vs las metricas elegidas en la funcion compile\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonatan\\AppData\\Local\\Temp\\ipykernel_25656\\1468152043.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "predictions shape: (1, 10)\n",
      "[0.    0.    0.    0.006 0.    0.    0.    0.993 0.    0.   ]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ojo aqui viene slicing xq presupone que le entran varios inputs\n",
    "Nos da las probabilidades de pertenecer a una clase u otra.\n",
    "'''\n",
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona.\n",
    "     \n",
    "Vamos a configurar una red como esta:  \n",
    "<img src=\"./img/mlp_regresion.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonatan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - RootMeanSquaredError: 1.2839 - loss: 1.6936 - val_RootMeanSquaredError: 0.8277 - val_loss: 0.6850\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - RootMeanSquaredError: 2.7455 - loss: 8.4114 - val_RootMeanSquaredError: 0.6996 - val_loss: 0.4894\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - RootMeanSquaredError: 0.6559 - loss: 0.4304 - val_RootMeanSquaredError: 0.6544 - val_loss: 0.4283\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6338 - loss: 0.4019 - val_RootMeanSquaredError: 0.6435 - val_loss: 0.4141\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - RootMeanSquaredError: 0.6309 - loss: 0.3982 - val_RootMeanSquaredError: 0.6349 - val_loss: 0.4032\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6140 - loss: 0.3775 - val_RootMeanSquaredError: 0.6314 - val_loss: 0.3986\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - RootMeanSquaredError: 0.6063 - loss: 0.3677 - val_RootMeanSquaredError: 0.6349 - val_loss: 0.4031\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - RootMeanSquaredError: 0.6169 - loss: 0.3809 - val_RootMeanSquaredError: 0.6262 - val_loss: 0.3921\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - RootMeanSquaredError: 0.6234 - loss: 0.3891 - val_RootMeanSquaredError: 0.6251 - val_loss: 0.3907\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - RootMeanSquaredError: 0.6044 - loss: 0.3654 - val_RootMeanSquaredError: 0.6244 - val_loss: 0.3898\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - RootMeanSquaredError: 0.6023 - loss: 0.3629 - val_RootMeanSquaredError: 0.6218 - val_loss: 0.3866\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - RootMeanSquaredError: 0.5984 - loss: 0.3582 - val_RootMeanSquaredError: 0.6260 - val_loss: 0.3919\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - RootMeanSquaredError: 0.6033 - loss: 0.3642 - val_RootMeanSquaredError: 0.6175 - val_loss: 0.3813\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - RootMeanSquaredError: 0.5969 - loss: 0.3564 - val_RootMeanSquaredError: 0.6186 - val_loss: 0.3826\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - RootMeanSquaredError: 0.6031 - loss: 0.3639 - val_RootMeanSquaredError: 0.6183 - val_loss: 0.3823\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 910us/step - RootMeanSquaredError: 0.5871 - loss: 0.3449 - val_RootMeanSquaredError: 0.6141 - val_loss: 0.3771\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - RootMeanSquaredError: 0.5847 - loss: 0.3422 - val_RootMeanSquaredError: 0.6172 - val_loss: 0.3810\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - RootMeanSquaredError: 0.5939 - loss: 0.3529 - val_RootMeanSquaredError: 0.6140 - val_loss: 0.3770\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - RootMeanSquaredError: 0.5869 - loss: 0.3451 - val_RootMeanSquaredError: 0.6132 - val_loss: 0.3761\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5891 - loss: 0.3472 - val_RootMeanSquaredError: 0.6117 - val_loss: 0.3741\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - RootMeanSquaredError: 0.5800 - loss: 0.3370\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = [\"RootMeanSquaredError\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.3657071590423584, 0.6047372817993164]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonatan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.7842 - RootMeanSquaredError: 1.1815 - loss: 1.4353 - val_MeanAbsoluteError: 0.5372 - val_RootMeanSquaredError: 0.7513 - val_loss: 0.5644\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.5372 - RootMeanSquaredError: 0.7365 - loss: 0.5428 - val_MeanAbsoluteError: 0.4997 - val_RootMeanSquaredError: 0.6996 - val_loss: 0.4894\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4940 - RootMeanSquaredError: 0.6863 - loss: 0.4713 - val_MeanAbsoluteError: 0.4779 - val_RootMeanSquaredError: 0.6814 - val_loss: 0.4643\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.4945 - RootMeanSquaredError: 0.6958 - loss: 0.4845 - val_MeanAbsoluteError: 0.4852 - val_RootMeanSquaredError: 0.6775 - val_loss: 0.4590\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4783 - RootMeanSquaredError: 0.6747 - loss: 0.4557 - val_MeanAbsoluteError: 0.4748 - val_RootMeanSquaredError: 0.6646 - val_loss: 0.4417\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - MeanAbsoluteError: 0.4717 - RootMeanSquaredError: 0.6600 - loss: 0.4357 - val_MeanAbsoluteError: 0.4684 - val_RootMeanSquaredError: 0.6571 - val_loss: 0.4318\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4708 - RootMeanSquaredError: 0.6596 - loss: 0.4353 - val_MeanAbsoluteError: 0.4674 - val_RootMeanSquaredError: 0.6563 - val_loss: 0.4308\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4741 - RootMeanSquaredError: 0.6989 - loss: 0.4900 - val_MeanAbsoluteError: 0.4623 - val_RootMeanSquaredError: 0.6527 - val_loss: 0.4261\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4578 - RootMeanSquaredError: 0.6433 - loss: 0.4140 - val_MeanAbsoluteError: 0.4579 - val_RootMeanSquaredError: 0.6454 - val_loss: 0.4165\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4554 - RootMeanSquaredError: 0.6453 - loss: 0.4165 - val_MeanAbsoluteError: 0.4618 - val_RootMeanSquaredError: 0.6398 - val_loss: 0.4094\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - MeanAbsoluteError: 0.4525 - RootMeanSquaredError: 0.6421 - loss: 0.4124 - val_MeanAbsoluteError: 0.4496 - val_RootMeanSquaredError: 0.6346 - val_loss: 0.4027\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - MeanAbsoluteError: 0.4630 - RootMeanSquaredError: 0.6657 - loss: 0.4435 - val_MeanAbsoluteError: 0.4517 - val_RootMeanSquaredError: 0.6354 - val_loss: 0.4037\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - MeanAbsoluteError: 0.4536 - RootMeanSquaredError: 0.6348 - loss: 0.4034 - val_MeanAbsoluteError: 0.4511 - val_RootMeanSquaredError: 0.6280 - val_loss: 0.3943\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - MeanAbsoluteError: 0.4458 - RootMeanSquaredError: 0.6262 - loss: 0.3922 - val_MeanAbsoluteError: 0.4619 - val_RootMeanSquaredError: 0.6335 - val_loss: 0.4013\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - MeanAbsoluteError: 0.4443 - RootMeanSquaredError: 0.6243 - loss: 0.3898 - val_MeanAbsoluteError: 0.4445 - val_RootMeanSquaredError: 0.6374 - val_loss: 0.4063\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - MeanAbsoluteError: 0.4500 - RootMeanSquaredError: 0.6324 - loss: 0.4005 - val_MeanAbsoluteError: 0.4448 - val_RootMeanSquaredError: 0.6295 - val_loss: 0.3963\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - MeanAbsoluteError: 0.4487 - RootMeanSquaredError: 0.6319 - loss: 0.3995 - val_MeanAbsoluteError: 0.4415 - val_RootMeanSquaredError: 0.6169 - val_loss: 0.3805\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4354 - RootMeanSquaredError: 0.6096 - loss: 0.3717 - val_MeanAbsoluteError: 0.4378 - val_RootMeanSquaredError: 0.6182 - val_loss: 0.3822\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - MeanAbsoluteError: 0.4470 - RootMeanSquaredError: 0.6371 - loss: 0.4062 - val_MeanAbsoluteError: 0.4417 - val_RootMeanSquaredError: 0.6254 - val_loss: 0.3911\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4384 - RootMeanSquaredError: 0.6202 - loss: 0.3847 - val_MeanAbsoluteError: 0.4319 - val_RootMeanSquaredError: 0.6159 - val_loss: 0.3793\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - MeanAbsoluteError: 0.4221 - RootMeanSquaredError: 0.6080 - loss: 0.3699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "#Otra forma pure-Keras:\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "\n",
    "norm_layer = keras.layers.Normalization(input_shape = X_train.shape[1:]) # Es una Standardization\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    norm_layer,\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=optimizer, metrics = [\"RootMeanSquaredError\",\"MeanAbsoluteError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.38163068890571594, 0.6177626252174377, 0.4289900064468384]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva capa al toolbox:\n",
    "\n",
    "Funcionales:  \n",
    "__Normalize__: keras.layers.Normalization -> Nos hace la standardizacion de la entrada \n",
    "Hay que ejecutar el metodo Adapt antes de llamar al fit del modelo que incluya la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es emplear el formato TensorFlow. En este caso crea un directorio con varios ficheros que facilita el despliegue en algunas aplicaciones (ojo, que habría que llevar a producción todo el directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model_otro.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model_k= keras.models.load_model(\"my_keras_model_otro.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step - MeanAbsoluteError: 0.4328 - RootMeanSquaredError: 0.6126 - loss: 0.3754\n",
      "Epoch 2/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - MeanAbsoluteError: 0.4338 - RootMeanSquaredError: 0.6126 - loss: 0.3756\n",
      "Epoch 3/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - MeanAbsoluteError: 0.4266 - RootMeanSquaredError: 0.5977 - loss: 0.3579\n",
      "Epoch 4/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - MeanAbsoluteError: 0.4241 - RootMeanSquaredError: 0.6017 - loss: 0.3622\n",
      "Epoch 5/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - MeanAbsoluteError: 0.4326 - RootMeanSquaredError: 0.6155 - loss: 0.3790\n",
      "Epoch 6/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - MeanAbsoluteError: 0.4268 - RootMeanSquaredError: 0.6046 - loss: 0.3657\n",
      "Epoch 7/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - MeanAbsoluteError: 0.4297 - RootMeanSquaredError: 0.6060 - loss: 0.3674\n",
      "Epoch 8/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4243 - RootMeanSquaredError: 0.6035 - loss: 0.3644\n",
      "Epoch 9/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4225 - RootMeanSquaredError: 0.6009 - loss: 0.3612  \n",
      "Epoch 10/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - MeanAbsoluteError: 0.4183 - RootMeanSquaredError: 0.5937 - loss: 0.3531\n",
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4213 - RootMeanSquaredError: 0.5983 - loss: 0.3581 - val_MeanAbsoluteError: 0.4212 - val_RootMeanSquaredError: 0.6020 - val_loss: 0.3623\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4184 - RootMeanSquaredError: 0.5912 - loss: 0.3497 - val_MeanAbsoluteError: 0.4213 - val_RootMeanSquaredError: 0.5917 - val_loss: 0.3501\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - MeanAbsoluteError: 0.4266 - RootMeanSquaredError: 0.6008 - loss: 0.3613 - val_MeanAbsoluteError: 0.4190 - val_RootMeanSquaredError: 0.5986 - val_loss: 0.3584\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4160 - RootMeanSquaredError: 0.5949 - loss: 0.3540 - val_MeanAbsoluteError: 0.4345 - val_RootMeanSquaredError: 0.6052 - val_loss: 0.3663\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - MeanAbsoluteError: 0.4265 - RootMeanSquaredError: 0.6080 - loss: 0.3697 - val_MeanAbsoluteError: 0.4176 - val_RootMeanSquaredError: 0.5962 - val_loss: 0.3555\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4242 - RootMeanSquaredError: 0.6030 - loss: 0.3640 - val_MeanAbsoluteError: 0.4185 - val_RootMeanSquaredError: 0.5918 - val_loss: 0.3502\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - MeanAbsoluteError: 0.4105 - RootMeanSquaredError: 0.5824 - loss: 0.3394 - val_MeanAbsoluteError: 0.4259 - val_RootMeanSquaredError: 0.6021 - val_loss: 0.3625\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - MeanAbsoluteError: 0.4246 - RootMeanSquaredError: 0.6062 - loss: 0.3680 - val_MeanAbsoluteError: 0.4258 - val_RootMeanSquaredError: 0.5920 - val_loss: 0.3505\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4190 - RootMeanSquaredError: 0.5936 - loss: 0.3525 - val_MeanAbsoluteError: 0.4361 - val_RootMeanSquaredError: 0.5963 - val_loss: 0.3555\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - MeanAbsoluteError: 0.4232 - RootMeanSquaredError: 0.5991 - loss: 0.3592 - val_MeanAbsoluteError: 0.4166 - val_RootMeanSquaredError: 0.6136 - val_loss: 0.3765\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - MeanAbsoluteError: 0.4111 - RootMeanSquaredError: 0.5844 - loss: 0.3416 - val_MeanAbsoluteError: 0.4142 - val_RootMeanSquaredError: 0.5990 - val_loss: 0.3588\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - MeanAbsoluteError: 0.4133 - RootMeanSquaredError: 0.5913 - loss: 0.3498 - val_MeanAbsoluteError: 0.4174 - val_RootMeanSquaredError: 0.5870 - val_loss: 0.3445\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - MeanAbsoluteError: 0.4145 - RootMeanSquaredError: 0.5909 - loss: 0.3492 - val_MeanAbsoluteError: 0.4108 - val_RootMeanSquaredError: 0.5876 - val_loss: 0.3453\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - MeanAbsoluteError: 0.4125 - RootMeanSquaredError: 0.5896 - loss: 0.3478 - val_MeanAbsoluteError: 0.4153 - val_RootMeanSquaredError: 0.5872 - val_loss: 0.3448\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4138 - RootMeanSquaredError: 0.5858 - loss: 0.3433 - val_MeanAbsoluteError: 0.4256 - val_RootMeanSquaredError: 0.5904 - val_loss: 0.3485\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - MeanAbsoluteError: 0.4134 - RootMeanSquaredError: 0.5909 - loss: 0.3492 - val_MeanAbsoluteError: 0.4208 - val_RootMeanSquaredError: 0.5861 - val_loss: 0.3435\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4152 - RootMeanSquaredError: 0.5902 - loss: 0.3484 - val_MeanAbsoluteError: 0.4218 - val_RootMeanSquaredError: 0.5857 - val_loss: 0.3431\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - MeanAbsoluteError: 0.4051 - RootMeanSquaredError: 0.5781 - loss: 0.3345 - val_MeanAbsoluteError: 0.4167 - val_RootMeanSquaredError: 0.5950 - val_loss: 0.3540\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - MeanAbsoluteError: 0.4034 - RootMeanSquaredError: 0.5736 - loss: 0.3291 - val_MeanAbsoluteError: 0.4126 - val_RootMeanSquaredError: 0.5861 - val_loss: 0.3435\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - MeanAbsoluteError: 0.4101 - RootMeanSquaredError: 0.5801 - loss: 0.3366 - val_MeanAbsoluteError: 0.4093 - val_RootMeanSquaredError: 0.5767 - val_loss: 0.3326\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.keras\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4037 - RootMeanSquaredError: 0.5764 - loss: 0.3325 - val_MeanAbsoluteError: 0.4143 - val_RootMeanSquaredError: 0.5858 - val_loss: 0.3431\n",
      "Epoch 2/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4038 - RootMeanSquaredError: 0.5747 - loss: 0.3305 - val_MeanAbsoluteError: 0.4164 - val_RootMeanSquaredError: 0.5786 - val_loss: 0.3348\n",
      "Epoch 3/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4036 - RootMeanSquaredError: 0.5746 - loss: 0.3302 - val_MeanAbsoluteError: 0.4094 - val_RootMeanSquaredError: 0.6057 - val_loss: 0.3668\n",
      "Epoch 4/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - MeanAbsoluteError: 0.4093 - RootMeanSquaredError: 0.5837 - loss: 0.3410 - val_MeanAbsoluteError: 0.4118 - val_RootMeanSquaredError: 0.5820 - val_loss: 0.3387\n",
      "Epoch 5/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - MeanAbsoluteError: 0.4084 - RootMeanSquaredError: 0.5821 - loss: 0.3390 - val_MeanAbsoluteError: 0.4038 - val_RootMeanSquaredError: 0.5803 - val_loss: 0.3367\n",
      "Epoch 6/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - MeanAbsoluteError: 0.4040 - RootMeanSquaredError: 0.5781 - loss: 0.3345 - val_MeanAbsoluteError: 0.4127 - val_RootMeanSquaredError: 0.5772 - val_loss: 0.3331\n",
      "Epoch 7/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - MeanAbsoluteError: 0.4088 - RootMeanSquaredError: 0.5825 - loss: 0.3394 - val_MeanAbsoluteError: 0.4087 - val_RootMeanSquaredError: 0.5792 - val_loss: 0.3355\n",
      "Epoch 8/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - MeanAbsoluteError: 0.3989 - RootMeanSquaredError: 0.5662 - loss: 0.3208 - val_MeanAbsoluteError: 0.4179 - val_RootMeanSquaredError: 0.5803 - val_loss: 0.3367\n",
      "Epoch 9/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - MeanAbsoluteError: 0.4031 - RootMeanSquaredError: 0.5790 - loss: 0.3354 - val_MeanAbsoluteError: 0.4062 - val_RootMeanSquaredError: 0.5765 - val_loss: 0.3323\n",
      "Epoch 10/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - MeanAbsoluteError: 0.3986 - RootMeanSquaredError: 0.5693 - loss: 0.3243 - val_MeanAbsoluteError: 0.4098 - val_RootMeanSquaredError: 0.5821 - val_loss: 0.3388\n",
      "Epoch 11/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - MeanAbsoluteError: 0.4037 - RootMeanSquaredError: 0.5734 - loss: 0.3289 - val_MeanAbsoluteError: 0.4176 - val_RootMeanSquaredError: 0.5786 - val_loss: 0.3347\n",
      "Epoch 12/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - MeanAbsoluteError: 0.4098 - RootMeanSquaredError: 0.5867 - loss: 0.3445 - val_MeanAbsoluteError: 0.3990 - val_RootMeanSquaredError: 0.5758 - val_loss: 0.3315\n",
      "Epoch 13/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - MeanAbsoluteError: 0.3967 - RootMeanSquaredError: 0.5652 - loss: 0.3196 - val_MeanAbsoluteError: 0.3999 - val_RootMeanSquaredError: 0.5772 - val_loss: 0.3332\n",
      "Epoch 14/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - MeanAbsoluteError: 0.4022 - RootMeanSquaredError: 0.5770 - loss: 0.3330 - val_MeanAbsoluteError: 0.4037 - val_RootMeanSquaredError: 0.5714 - val_loss: 0.3265\n",
      "Epoch 15/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4042 - RootMeanSquaredError: 0.5738 - loss: 0.3293 - val_MeanAbsoluteError: 0.4006 - val_RootMeanSquaredError: 0.5724 - val_loss: 0.3276\n",
      "Epoch 16/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - MeanAbsoluteError: 0.3997 - RootMeanSquaredError: 0.5691 - loss: 0.3241 - val_MeanAbsoluteError: 0.4041 - val_RootMeanSquaredError: 0.5749 - val_loss: 0.3305\n",
      "Epoch 17/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4073 - RootMeanSquaredError: 0.5826 - loss: 0.3396 - val_MeanAbsoluteError: 0.4002 - val_RootMeanSquaredError: 0.5751 - val_loss: 0.3308\n",
      "Epoch 18/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - MeanAbsoluteError: 0.4010 - RootMeanSquaredError: 0.5728 - loss: 0.3284 - val_MeanAbsoluteError: 0.4024 - val_RootMeanSquaredError: 0.5808 - val_loss: 0.3374\n",
      "Epoch 19/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - MeanAbsoluteError: 0.4011 - RootMeanSquaredError: 0.5739 - loss: 0.3294 - val_MeanAbsoluteError: 0.4101 - val_RootMeanSquaredError: 0.5717 - val_loss: 0.3268\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "10 esta bien. Lo pondemos a 5 para el ejercicio\n",
    "¿Qué considera como dejar de mejorar? parametros min_delta y baseline\n",
    "'''\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, \n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guía \"casera\":\n",
    "\n",
    "1- Recetas (adaptada de \"Hands-on...\")  PARA MLPs!!!!    \n",
    "    * Capas:   \n",
    "        - Empezar con una capa oculta e ir añadiendo (dependiendo de la complejidad del problema, probar wide & deep)\n",
    "        - Si pocas features -> más neuronas  (aumentar la combinación de features) (num_features < 100) [Orientativo]  \n",
    "        - Si muchas features  -> menos neuronas (proyección tipo PCA) (num_features > 1000) [Orientativo] e ir aumentando en capas sucesivas\n",
    "        - O empezar con muchas (doble de tus features e ir \"estrechando los pantalones\")  \n",
    "        - Construcción en prisma o pirámide (para empezar)  \n",
    "        - Inicialización: Empezar con Glorot, cambiar a He  \n",
    "        - Activación: ReLU salvo la última, si muchas capas probar -> SELU o Swish (con el inicializador a LeCunn) \n",
    "    * Optimizadores:   \n",
    "        - Si muchos datos*features -> Adam o AdamW con sus valores por defecto  \n",
    "        - Si no, SGD con Nesterov activado, y momento a 0.9  \n",
    "        - Learning rate -> 0.001-0.0001 para empezar e ir creciendo (learning-rate warm-up) (Si te atreves, buscar adaptative learning rate y optimizar con esto)  \n",
    "    * Entrenamiento:  \n",
    "        - Epoch, probar con pocas para ver duración -> Epochs altas y Callback de Early Stop activado  \n",
    "        - Batch_Size -> 32, si tienes muchos datos y una GPU a mano puedes subir mucho 64,128,256...\n",
    "    * Regularización (lo veremos):  \n",
    "        - Dropout al 0.25-0.5 (sin SELU)\n",
    "\n",
    "\n",
    "\n",
    "2- Pasos  \n",
    "    - Si overfitting -> Regularizar: Earlystopping, Dropout (lo veremos en la siguiente sección)  \n",
    "    - Comprobar underfitting -> Aumentar epochs, aumentar batch_size  \n",
    "    - Jugar con optimizador: learning rate (de pequeño a grande), tipo de optimizador   \n",
    "    - Jugar con número de capas (ojo al overfitting) y las funciones de activación y la inicialización de pesos  \n",
    "    - Jugar con el número de neuronas por capa (suele ser piramide o prisma, pero puedes jugar a expandir dimensiones)  \n",
    "    - Combinar los dos anteriores  \n",
    "\n",
    "3- Keras Tuner:\n",
    "    https://keras.io/guides/keras_tuner/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Keras tiene implementado un dashboard para monitorizar las ejecuciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crea este directorio\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# Guarda una carpeta nueva con la fecha de la ejecucion\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4051 - RootMeanSquaredError: 0.5776 - loss: 0.3338 - val_MeanAbsoluteError: 0.4000 - val_RootMeanSquaredError: 0.5729 - val_loss: 0.3282\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.4000 - RootMeanSquaredError: 0.5746 - loss: 0.3302 - val_MeanAbsoluteError: 0.4055 - val_RootMeanSquaredError: 0.5799 - val_loss: 0.3363\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4038 - RootMeanSquaredError: 0.5756 - loss: 0.3315 - val_MeanAbsoluteError: 0.4120 - val_RootMeanSquaredError: 0.5780 - val_loss: 0.3341\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4028 - RootMeanSquaredError: 0.5702 - loss: 0.3254 - val_MeanAbsoluteError: 0.4046 - val_RootMeanSquaredError: 0.5745 - val_loss: 0.3300\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4059 - RootMeanSquaredError: 0.5865 - loss: 0.3442 - val_MeanAbsoluteError: 0.4026 - val_RootMeanSquaredError: 0.5717 - val_loss: 0.3268\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3982 - RootMeanSquaredError: 0.5685 - loss: 0.3234 - val_MeanAbsoluteError: 0.4056 - val_RootMeanSquaredError: 0.5753 - val_loss: 0.3310\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3952 - RootMeanSquaredError: 0.5643 - loss: 0.3186 - val_MeanAbsoluteError: 0.4091 - val_RootMeanSquaredError: 0.5770 - val_loss: 0.3330\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.4010 - RootMeanSquaredError: 0.5751 - loss: 0.3309 - val_MeanAbsoluteError: 0.4223 - val_RootMeanSquaredError: 0.5803 - val_loss: 0.3367\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3997 - RootMeanSquaredError: 0.5698 - loss: 0.3248 - val_MeanAbsoluteError: 0.3998 - val_RootMeanSquaredError: 0.5722 - val_loss: 0.3274\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4021 - RootMeanSquaredError: 0.5748 - loss: 0.3306 - val_MeanAbsoluteError: 0.4074 - val_RootMeanSquaredError: 0.5727 - val_loss: 0.3280\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4004 - RootMeanSquaredError: 0.5746 - loss: 0.3304 - val_MeanAbsoluteError: 0.3998 - val_RootMeanSquaredError: 0.5805 - val_loss: 0.3370\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3948 - RootMeanSquaredError: 0.5677 - loss: 0.3224 - val_MeanAbsoluteError: 0.3994 - val_RootMeanSquaredError: 0.5738 - val_loss: 0.3292\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3979 - RootMeanSquaredError: 0.5745 - loss: 0.3302 - val_MeanAbsoluteError: 0.3997 - val_RootMeanSquaredError: 0.5764 - val_loss: 0.3322\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3957 - RootMeanSquaredError: 0.5642 - loss: 0.3184 - val_MeanAbsoluteError: 0.4074 - val_RootMeanSquaredError: 0.5725 - val_loss: 0.3278\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3975 - RootMeanSquaredError: 0.5650 - loss: 0.3193 - val_MeanAbsoluteError: 0.4027 - val_RootMeanSquaredError: 0.5720 - val_loss: 0.3272\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3973 - RootMeanSquaredError: 0.5731 - loss: 0.3286 - val_MeanAbsoluteError: 0.3948 - val_RootMeanSquaredError: 0.5677 - val_loss: 0.3223\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3975 - RootMeanSquaredError: 0.5722 - loss: 0.3275 - val_MeanAbsoluteError: 0.4220 - val_RootMeanSquaredError: 0.5771 - val_loss: 0.3331\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4075 - RootMeanSquaredError: 0.5869 - loss: 0.3448 - val_MeanAbsoluteError: 0.4059 - val_RootMeanSquaredError: 0.5824 - val_loss: 0.3391\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4100 - RootMeanSquaredError: 0.5901 - loss: 0.3489 - val_MeanAbsoluteError: 0.4029 - val_RootMeanSquaredError: 0.5801 - val_loss: 0.3366\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3934 - RootMeanSquaredError: 0.5696 - loss: 0.3250 - val_MeanAbsoluteError: 0.4049 - val_RootMeanSquaredError: 0.5795 - val_loss: 0.3358\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.4000 - RootMeanSquaredError: 0.5751 - loss: 0.3309 - val_MeanAbsoluteError: 0.3974 - val_RootMeanSquaredError: 0.5747 - val_loss: 0.3303\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3970 - RootMeanSquaredError: 0.5732 - loss: 0.3290 - val_MeanAbsoluteError: 0.4011 - val_RootMeanSquaredError: 0.5718 - val_loss: 0.3270\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3987 - RootMeanSquaredError: 0.5652 - loss: 0.3195 - val_MeanAbsoluteError: 0.4053 - val_RootMeanSquaredError: 0.5767 - val_loss: 0.3326\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3944 - RootMeanSquaredError: 0.5639 - loss: 0.3183 - val_MeanAbsoluteError: 0.4001 - val_RootMeanSquaredError: 0.5704 - val_loss: 0.3254\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.4018 - RootMeanSquaredError: 0.5853 - loss: 0.3427 - val_MeanAbsoluteError: 0.3970 - val_RootMeanSquaredError: 0.5705 - val_loss: 0.3255\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3914 - RootMeanSquaredError: 0.5670 - loss: 0.3216 - val_MeanAbsoluteError: 0.3967 - val_RootMeanSquaredError: 0.5694 - val_loss: 0.3242\n",
      "Epoch 27/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3937 - RootMeanSquaredError: 0.5663 - loss: 0.3209 - val_MeanAbsoluteError: 0.3952 - val_RootMeanSquaredError: 0.5688 - val_loss: 0.3235\n",
      "Epoch 28/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3996 - RootMeanSquaredError: 0.5755 - loss: 0.3313 - val_MeanAbsoluteError: 0.3935 - val_RootMeanSquaredError: 0.5696 - val_loss: 0.3244\n",
      "Epoch 29/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3976 - RootMeanSquaredError: 0.5822 - loss: 0.3394 - val_MeanAbsoluteError: 0.3977 - val_RootMeanSquaredError: 0.5670 - val_loss: 0.3215\n",
      "Epoch 30/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3931 - RootMeanSquaredError: 0.5677 - loss: 0.3223 - val_MeanAbsoluteError: 0.3961 - val_RootMeanSquaredError: 0.5699 - val_loss: 0.3248\n",
      "Epoch 31/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3914 - RootMeanSquaredError: 0.5620 - loss: 0.3160 - val_MeanAbsoluteError: 0.3969 - val_RootMeanSquaredError: 0.5663 - val_loss: 0.3207\n",
      "Epoch 32/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.4163 - RootMeanSquaredError: 0.6277 - loss: 0.3957 - val_MeanAbsoluteError: 0.4010 - val_RootMeanSquaredError: 0.5787 - val_loss: 0.3349\n",
      "Epoch 33/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3992 - RootMeanSquaredError: 0.5729 - loss: 0.3286 - val_MeanAbsoluteError: 0.4010 - val_RootMeanSquaredError: 0.5723 - val_loss: 0.3275\n",
      "Epoch 34/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3894 - RootMeanSquaredError: 0.5522 - loss: 0.3053 - val_MeanAbsoluteError: 0.4076 - val_RootMeanSquaredError: 0.5729 - val_loss: 0.3282\n",
      "Epoch 35/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3979 - RootMeanSquaredError: 0.5726 - loss: 0.3280 - val_MeanAbsoluteError: 0.4018 - val_RootMeanSquaredError: 0.5691 - val_loss: 0.3238\n",
      "Epoch 36/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3952 - RootMeanSquaredError: 0.5645 - loss: 0.3188 - val_MeanAbsoluteError: 0.4037 - val_RootMeanSquaredError: 0.5723 - val_loss: 0.3275\n",
      "Epoch 37/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3939 - RootMeanSquaredError: 0.5657 - loss: 0.3201 - val_MeanAbsoluteError: 0.4018 - val_RootMeanSquaredError: 0.5683 - val_loss: 0.3230\n",
      "Epoch 38/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3959 - RootMeanSquaredError: 0.5666 - loss: 0.3212 - val_MeanAbsoluteError: 0.4055 - val_RootMeanSquaredError: 0.5718 - val_loss: 0.3270\n",
      "Epoch 39/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3879 - RootMeanSquaredError: 0.5565 - loss: 0.3099 - val_MeanAbsoluteError: 0.3999 - val_RootMeanSquaredError: 0.5655 - val_loss: 0.3198\n",
      "Epoch 40/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3907 - RootMeanSquaredError: 0.5581 - loss: 0.3117 - val_MeanAbsoluteError: 0.3927 - val_RootMeanSquaredError: 0.5656 - val_loss: 0.3199\n",
      "Epoch 41/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3983 - RootMeanSquaredError: 0.5680 - loss: 0.3227 - val_MeanAbsoluteError: 0.3984 - val_RootMeanSquaredError: 0.5641 - val_loss: 0.3182\n",
      "Epoch 42/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3922 - RootMeanSquaredError: 0.5642 - loss: 0.3185 - val_MeanAbsoluteError: 0.4019 - val_RootMeanSquaredError: 0.5783 - val_loss: 0.3345\n",
      "Epoch 43/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3872 - RootMeanSquaredError: 0.5569 - loss: 0.3102 - val_MeanAbsoluteError: 0.4122 - val_RootMeanSquaredError: 0.5706 - val_loss: 0.3256\n",
      "Epoch 44/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3887 - RootMeanSquaredError: 0.5575 - loss: 0.3110 - val_MeanAbsoluteError: 0.4066 - val_RootMeanSquaredError: 0.5732 - val_loss: 0.3286\n",
      "Epoch 45/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3963 - RootMeanSquaredError: 0.5742 - loss: 0.3300 - val_MeanAbsoluteError: 0.3924 - val_RootMeanSquaredError: 0.5646 - val_loss: 0.3188\n",
      "Epoch 46/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3928 - RootMeanSquaredError: 0.5680 - loss: 0.3230 - val_MeanAbsoluteError: 0.3997 - val_RootMeanSquaredError: 0.5654 - val_loss: 0.3197\n",
      "Epoch 47/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3909 - RootMeanSquaredError: 0.5612 - loss: 0.3150 - val_MeanAbsoluteError: 0.3944 - val_RootMeanSquaredError: 0.5663 - val_loss: 0.3207\n",
      "Epoch 48/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - MeanAbsoluteError: 0.3867 - RootMeanSquaredError: 0.5558 - loss: 0.3091 - val_MeanAbsoluteError: 0.3911 - val_RootMeanSquaredError: 0.5655 - val_loss: 0.3198\n",
      "Epoch 49/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3870 - RootMeanSquaredError: 0.5584 - loss: 0.3118 - val_MeanAbsoluteError: 0.3968 - val_RootMeanSquaredError: 0.5635 - val_loss: 0.3176\n",
      "Epoch 50/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - MeanAbsoluteError: 0.3855 - RootMeanSquaredError: 0.5572 - loss: 0.3107 - val_MeanAbsoluteError: 0.4052 - val_RootMeanSquaredError: 0.5703 - val_loss: 0.3252\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPara lanzarlo desde el jupyter notebook\\n%load_ext tensorboard\\n%tensorboard --logdir=./my_logs --port=6006\\n\\nPara lanzarlo desde el terminal, hay que estar en la carpeta de los logs\\ntensorboard --logdir=./my_logs --port=6006\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Para lanzarlo desde el jupyter notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "Para lanzarlo desde el terminal, hay que estar en la carpeta de los logs\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "167a7833a0358ac30a26ad970c5914014f41a5348f3dc652232a762d6e3283fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
