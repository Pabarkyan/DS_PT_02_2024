{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZBRUaiBBEpa"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### Download the IMDb Dataset\n",
    "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](../load_data/text.ipynb).  \n",
    "\n",
    "Download the dataset using Keras file utility and take a look at the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 85s 1us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", origin = 'file:/./aclImdb_v1.tar.gz',\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\aclImdb_v1\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glezr\\TheBridge\\77_Lunes_1303\\77_Lunes_1303\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"./aclImdb\"\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(dataset_dir + \"/train/unsup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "The `train` directory also has additional folders which should be removed before creating training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about using this utility in this [text classification tutorial](https://www.tensorflow.org/tutorials/keras/text_classification). \n",
    "\n",
    "Use the `train` directory to create both train and validation datasets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "'''\n",
    "Busca en un directorio todas las carpetas. Cada carpeta es una etiqueta\n",
    "Y cada archivo una review. Podemos especificar si es para el subset\n",
    "de training o validation y cuanto dejamos para validacion.\n",
    "Esto crea un tf.data.Dataset\n",
    "ELIMINAR UNA CARPETA EN TRAIN, QUE SOBRA\n",
    "'''\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation', # Esto y la semilla permiten que las muestras con train no se superpongan\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aTCbSkvkYmTT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have st' ...\n",
      "Label: 0 (Negative)\n",
      "b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams' ...\n",
      "Label: 1 (Positive)\n",
      "b'Alex D. Linz replaces Macaulay Culkin as the central figure in the third movie in the Home Alone empire. Four industrial spies acquire a missile guida' ...\n",
      "Label: 0 (Negative)\n",
      "b\"There's a good movie lurking here, but this isn't it. The basic idea is good: to explore the moral issues that would face a group of young survivors o\" ...\n",
      "Label: 0 (Negative)\n",
      "b'I saw this movie at an actual movie theater (probably the $2.00 one) with my cousin and uncle. We were around 11 and 12, I guess, and really into scar' ...\n",
      "Label: 0 (Negative)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1): # Cogemos el primer batch\n",
    "    for i in range(5): # 1025 da error xq no hay mas en este batch\n",
    "        print(text_batch.numpy()[i][:150],\"...\")\n",
    "        print(\"Label:\",label_batch[i].numpy(),\"(%s)\" %(\"Positive\" if label_batch.numpy()[i] == 1 else \"Negative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "\n",
    "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## Using the Embedding layer with sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, lo que ya hemos visto con el ejemplo \"Me llamo Iñigo Montoya\", aunque en este caso supongamos que vamos a introducir un batch de 2 secuencias: \"Me llamo Iñigo Montoya\" y \"tú mataste a mi padre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias_ejemplo = [\"Me\",\"llamo\",\"Iñigo\",\"Montoya\",\"soy\",\"tú\",\"mataste\",\"a\",\"mi\",\"padre\"]\n",
    "pre_conversion = tf.keras.layers.StringLookup() # Hay que convertir nuestro vocabulario a indices\n",
    "pre_conversion.adapt(categorias_ejemplo)\n",
    "lookup_y_embedding = tf.keras.Sequential([\\\n",
    "                                          tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string), \n",
    "                                          pre_conversion,\n",
    "                                          tf.keras.layers.Embedding(input_dim = pre_conversion.vocabulary_size(),\n",
    "                                                                   output_dim = 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2array(texto):\n",
    "    return np.array([texto.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vwSYepRjyRGy",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m lookup_y_embedding(tf\u001b[39m.\u001b[39;49mconstant([text2array(\u001b[39m\"\u001b[39;49m\u001b[39mMe llamo Iñigo Montoya\u001b[39;49m\u001b[39m\"\u001b[39;49m), text2array(\u001b[39m\"\u001b[39;49m\u001b[39mtú mataste a mi padre\u001b[39;49m\u001b[39m\"\u001b[39;49m)]))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m result\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "result = lookup_y_embedding(tf.constant([text2array(\"Me llamo Iñigo Montoya\"), text2array(\"tú mataste a mi padre\")]))\n",
    "print(result.shape)\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores son estructuras \"rectangulares\" eso de meterle secuencias de longitud variable nos lleva a un primer punto que debéis conocer si vais a trabajar con NLP en DL... __Es necesario hacer Padding__, es decir hacer secuencias de un tamaño fijo rellenando con \"ceros\" las secuencias más cortas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.01446323, -0.00073646],\n",
       "         [ 0.0494634 ,  0.04787635],\n",
       "         [-0.0160408 , -0.01600071],\n",
       "         [ 0.00278103,  0.00609075],\n",
       "         [ 0.0482927 ,  0.03427731]]],\n",
       "\n",
       "\n",
       "       [[[ 0.03200665, -0.01884188],\n",
       "         [ 0.01349134, -0.02253004],\n",
       "         [-0.04695269, -0.02307154],\n",
       "         [ 0.02485181,  0.01351038],\n",
       "         [ 0.04982268,  0.02805033]]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lookup_y_embedding(tf.constant([text2array(\"<relleno> Me llamo Iñigo Montoya\"), text2array(\"tú mataste a mi padre\")]))\n",
    "print(result.shape)\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso nos llevará a otro problema: las secuencias muy cortas estarán llenas de ceros y serán mal interpretadas, para corregir esto utilizaremos el concepto de __mask__ (lo veremos más adelante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest. The [Text Classification with an RNN](text_classification_rnn.ipynb) tutorial is a good next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. You can learn more about using this layer in the [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data) # lo convierte a mayúsculas\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # le quita los codigos de cambio de línea\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '') #le quita los signos de puntuación\n",
    "\n",
    "\n",
    "sequence_length = 8\n",
    "\n",
    "# Veamos como la capa TextVectorization nos permite darle secuenccias de entrada y \n",
    "# nos devuelve el formato lista de índices, CON PADDING\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    output_mode='int', # el modo int eso hace que devuelva una secuencia con índices al vocabulario IMPORTANTE!!!\n",
    "    output_sequence_length=sequence_length) # Aquí le indicamos el tamaño de la secuencia de salida y \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = [\"Me llamo Iñigo Monotya\", \"Tú mataste a mi padre\",\"Prepárate a morir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 8), dtype=int64, numpy=\n",
       "array([[ 9, 11, 12,  7,  0,  0,  0,  0],\n",
       "       [ 3, 10,  2,  8,  5,  0,  0,  0],\n",
       "       [ 4,  2,  6,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> \n",
      "1 -> [UNK]\n",
      "2 -> a\n",
      "3 -> tú\n",
      "4 -> prepárate\n",
      "5 -> padre\n",
      "6 -> morir\n",
      "7 -> monotya\n",
      "8 -> mi\n",
      "9 -> me\n",
      "10 -> mataste\n",
      "11 -> llamo\n",
      "12 -> iñigo\n"
     ]
    }
   ],
   "source": [
    "for indice,word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    print(\"%d -> %s\" %(indice,word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice 0 se suele reservar para hacer el padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a la salida del TextVectorization le aplicaremos la capa de Embeddings y luego a eso nuestras capas densas para hacer el modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data) # lo convierte a mayúsculas\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # le quita los codigos de cambio de línea\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '') #le quita los signos de puntuación\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000 # Vamos a permitir que tenga un vocabulario de 10000, las de mayor frecuencia el resto se codificará como UNK, o si hemos empleado OOV como esas categorías extra\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "# Nos quitamos los labels de esta manera, que van en conjunto con features en train_ds\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## Create a classification model\n",
    "\n",
    "Use the [Keras Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model.\n",
    "* The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) layer transforms strings into vocabulary indices. You have already initialized `vectorize_layer` as a TextVectorization layer and built it's vocabulary by calling `adapt` on `text_ds`. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
    "* The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
    "\n",
    "* The [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible. \n",
    "\n",
    "* The fixed-length output vector is piped through a fully-connected ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer with 16 hidden units.\n",
    "\n",
    "* The last layer is densely connected with a single output node. \n",
    "\n",
    "Caution: This model doesn't use masking, so the zero-padding is used as part of the input and hence the padding length may affect the output.  To fix this, see the [masking and padding guide](https://www.tensorflow.org/guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "'''\n",
    "GlobalAveragePooling1D\n",
    "Cada palabra tiene asociado un embedding. El ouput es la media de cada\n",
    "coordenada del embedding, por tanto, si hay 16 embeddings, hará un \n",
    "flatten a 16, siendo cada valor la media de la coordenada de ese \n",
    "embedding para todas las palabras de la review\n",
    "'''\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer, # 100 [1, 3, 4, 4, 90, ...]\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # 10.000 x 16 --> [[], [], [] ...] 100x16\n",
    "  GlobalAveragePooling1D(), # [] 16\n",
    "  Dense(16, activation='relu'), # \n",
    "  Dense(1) # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": [
    "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              # binary_crossentropy\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "5mQehiQyv8rP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 8s 359ms/step - loss: 0.6922 - accuracy: 0.5028 - val_loss: 0.6904 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 2s 81ms/step - loss: 0.6878 - accuracy: 0.5028 - val_loss: 0.6845 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.6794 - accuracy: 0.5028 - val_loss: 0.6741 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 0.6659 - accuracy: 0.5028 - val_loss: 0.6583 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 0.6459 - accuracy: 0.5028 - val_loss: 0.6366 - val_accuracy: 0.4886\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.6196 - accuracy: 0.5218 - val_loss: 0.6099 - val_accuracy: 0.5446\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.5882 - accuracy: 0.6016 - val_loss: 0.5801 - val_accuracy: 0.6162\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.5537 - accuracy: 0.6790 - val_loss: 0.5495 - val_accuracy: 0.6642\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.5185 - accuracy: 0.7306 - val_loss: 0.5203 - val_accuracy: 0.7050\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.4848 - accuracy: 0.7631 - val_loss: 0.4940 - val_accuracy: 0.7342\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.4541 - accuracy: 0.7883 - val_loss: 0.4714 - val_accuracy: 0.7518\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.4267 - accuracy: 0.8074 - val_loss: 0.4525 - val_accuracy: 0.7646\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 0.4026 - accuracy: 0.8219 - val_loss: 0.4368 - val_accuracy: 0.7752\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.3813 - accuracy: 0.8342 - val_loss: 0.4240 - val_accuracy: 0.7830\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.3625 - accuracy: 0.8432 - val_loss: 0.4134 - val_accuracy: 0.7912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f9e5d09db0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "With this approach the model reaches a validation accuracy of around 84% (note that the model is overfitting since training accuracy is higher).\n",
    "\n",
    "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    dataset_dir + \"/test/\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 23s 693ms/step - loss: 0.4343 - accuracy: 0.7764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4342634677886963, 0.776449978351593]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y esto es bueno o malo? Compáralo con alguna baseline..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indpendientemente de lo anterior, veamos algunas cosas más sobre embeddings mirando el resumen del modelo... Por ejemplo el número de parámetros de la capa de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "mDCgjWyq_0dc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al limitar el tamaño de secuencia a 100, el modelo está intencionadamente evitando problemas de excesivo padding (las reviews seguramente son de más de 100 palabras), pero...  \n",
    "el coste es perder capacidad, porque hace como la generación Z, a las 100 palabras se aburre y toma una decisión.  \n",
    "Si quisieramos que leyese más de 100 palabras o todas, seguramente tendríamos secuencias con mucho padding, y para evitar el problema de este, usariamos máscaras, con sólo cambiar esto (en la versión fácil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer, # 100 [1, 3, 4, 4, 90, ...]\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\", mask_zero = True), # <- la capa  Embedding admite mask, le estamos diciendo que si ve un zero lo ignore\n",
    "  GlobalAveragePooling1D(), # [] 16\n",
    "  Dense(16, activation='relu'), # \n",
    "  Dense(1) # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además la capa de Embedding le pasa la mascara a la siguiente capa (en este caso la GlobalAveragePooling1D) a través del argumento mask. Si la capa tiene ese argumento recibe la máscar y la trata como tenga programado.  \n",
    "En este caso GlobalAveragePooling1D ignora el elemento enmascarado y no lo incluye en la media.   \n",
    "\n",
    "Una capa recurrente si recibe una máscara ignora los elementos a 0 y para ese \"time_step\" para ese valor repite el último valor de hidden_state válido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si el valor a enmascarar no es cero? Podemos usar..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.layers.Masking(mask_value=0.0, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask_value es el valor que queremos \"saltarnos\" o \"enmascarar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second \"equivalent\" model using RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al trabajar con secuencias (de palabras) puede resultar más apropiado o potencialmente más útil, trabajar con redes recurrentes que intenten captar la información de la secuencia. Basta con sustituir dos capas en la red anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential([\n",
    "  vectorize_layer, # 100 [1, 3, 4, 4, 90, ...]\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # 10.000 x 16 --> [[], [], [] ...] 100x16\n",
    "  #GlobalAveragePooling1D(), # [] 16\n",
    "  #Dense(16, activation='relu'),  \n",
    "  tf.keras.layers.GRU(embedding_dim), # Al ser recurrente la GRU no necesita hacer el pooling y de hecho puede obtener información del orden de la secuencia\n",
    "  Dense(1, activation = \"sigmoid\") # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(optimizer='adam',\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 8s 328ms/step - loss: 0.6925 - accuracy: 0.5149 - val_loss: 0.6916 - val_accuracy: 0.5130\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 7s 335ms/step - loss: 0.6856 - accuracy: 0.5868 - val_loss: 0.6846 - val_accuracy: 0.5650\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 7s 333ms/step - loss: 0.6534 - accuracy: 0.6501 - val_loss: 0.6040 - val_accuracy: 0.6794\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 8s 399ms/step - loss: 0.4812 - accuracy: 0.7708 - val_loss: 0.4548 - val_accuracy: 0.8022\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 9s 448ms/step - loss: 0.3457 - accuracy: 0.8541 - val_loss: 0.4133 - val_accuracy: 0.8206\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 10s 490ms/step - loss: 0.2702 - accuracy: 0.8961 - val_loss: 0.4271 - val_accuracy: 0.8214\n",
      "Epoch 7/15\n",
      " 1/20 [>.............................] - ETA: 9s - loss: 0.2682 - accuracy: 0.8984"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_rnn \u001b[39m=\u001b[39m model_rnn\u001b[39m.\u001b[39;49mfit(train_ds,\n\u001b[0;32m      2\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[tensorboard_callback])\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_rnn = model_rnn.fit(train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 160ms/step - loss: 0.8089 - accuracy: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8088986277580261, 0.7763000130653381]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar la parte de modelado, simplemente probemos con menos épocas (porque está overfitteando)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 12s 506ms/step - loss: 0.6924 - accuracy: 0.5196 - val_loss: 0.6915 - val_accuracy: 0.5182\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 10s 479ms/step - loss: 0.6856 - accuracy: 0.5824 - val_loss: 0.6832 - val_accuracy: 0.5646\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 10s 488ms/step - loss: 0.6336 - accuracy: 0.6615 - val_loss: 0.5594 - val_accuracy: 0.7120\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 10s 486ms/step - loss: 0.4565 - accuracy: 0.7922 - val_loss: 0.4487 - val_accuracy: 0.7954\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 10s 481ms/step - loss: 0.3390 - accuracy: 0.8579 - val_loss: 0.4104 - val_accuracy: 0.8234\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 9s 444ms/step - loss: 0.2779 - accuracy: 0.8929 - val_loss: 0.4690 - val_accuracy: 0.8190\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 9s 444ms/step - loss: 0.2450 - accuracy: 0.9082 - val_loss: 0.4509 - val_accuracy: 0.8152\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 9s 447ms/step - loss: 0.2056 - accuracy: 0.9278 - val_loss: 0.4646 - val_accuracy: 0.8156\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 9s 449ms/step - loss: 0.1818 - accuracy: 0.9410 - val_loss: 0.5547 - val_accuracy: 0.8126\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 9s 456ms/step - loss: 0.1882 - accuracy: 0.9360 - val_loss: 0.6219 - val_accuracy: 0.8122\n"
     ]
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience = 5, monitor = \"val_accuracy\", restore_best_weights= True)\n",
    "history_rnn = model_rnn.fit(train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 172ms/step - loss: 0.4472 - accuracy: 0.8056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44719213247299194, 0.8055999875068665]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiQbOJZ2WBFY"
   },
   "source": [
    "Visualize the model metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "_Uanp2YH8RzU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ebbbc05d9dfc2f81\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ebbbc05d9dfc2f81\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvURkGVpXDOy"
   },
   "source": [
    "![embeddings_classifier_accuracy.png](images/embeddings_classifier_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## Retrieve the trained word embeddings and save them to disk\n",
    "\n",
    "Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "If you are running this tutorial in [Colaboratory](https://colab.research.google.com), you can use the following snippet to download these files to your local machine (or use the file browser, *View -> Table of contents -> File browser*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lUsjQOKMIV2z"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXLfFA54Yz-o"
   },
   "source": [
    "## Visualize the embeddings\n",
    "\n",
    "To visualize the embeddings, upload them to the embedding projector.\n",
    "\n",
    "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
    "\n",
    "* Click on \"Load data\".\n",
    "\n",
    "* Upload the two files you created above: `vecs.tsv` and `meta.tsv`.\n",
    "\n",
    "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
    "\n",
    "Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.\n",
    "\n",
    "Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvKiEHjramNh"
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSgAZpwF5xF_"
   },
   "source": [
    "This tutorial has shown you how to train and visualize word embeddings from scratch on a small dataset.\n",
    "\n",
    "* To train word embeddings using Word2Vec algorithm, try the [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) tutorial. \n",
    "\n",
    "* To learn more about advanced text processing, read the [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7600a12950a547366bb7a6732117e300ffd26224351912980486e1126c5d0f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
