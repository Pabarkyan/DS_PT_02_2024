{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPor defecto, keras no tira de GPU\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras (NO NECESARIO YA INTEGRADO EN TENSORFLOW)\n",
    "'''\n",
    "Por defecto, keras no tira de GPU\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[25000,12,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays). Aplano a una dimension cada imagen.\n",
    "# Escalamos ya que vamos a usar gradient descent, y le afecta mucho la escala de las features.\n",
    "# Ejecutar esta celda solo una vez. Sino reescalará\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10,000 samples for validation. Entraran dentro del modelo para validar. No es validacion cruzada\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICION/CONSTRUCCION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax.  \n",
    "\n",
    "Es decir vamos a volver a montar esta arquitectura:  \n",
    "<img src=\"./img/mlp_clasification.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Un poco más sobre la activación softmax:    \n",
    "\n",
    "Fórmula:  \n",
    "<img src=\"./img/softmax_function.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Función de transferencia:  \n",
    "<img src=\"./img/softmax_activation.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Ejemplo de funcionamiento:  \n",
    "<img src=\"./img/softmax_example.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una manera de declarar la red neuronal\n",
    "\n",
    "# Siempre hay que declarar la capa sequential para empezar a declarar la red\n",
    "# Se trata de la API sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Flatten, aplana en un unico vector. Y especificamos el tamaño de la entrada\n",
    "# Es como si hiciese un .reshape(-1, 28*28)\n",
    "# \"kernel_initializer\" o \"bias_initializer\" No lo usamos pero seria para inicializar los pesos de otra manera\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Capas de la red. Dense es la capa de neuronas. Necesitamos numero y activacion\n",
    "model.add(keras.layers.Dense(units = 300, # Numero de neuronas de la capa\n",
    "                             activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(100,\n",
    "                             activation='relu'))\n",
    "\n",
    "# Capa de salida, con tamaño del número de clases\n",
    "# Suele ir aqui un softmax. Para multiclase guay. Si es binaria -> sigmoide\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "]\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y una forma mediante encademaniento de funciones (usando lo que se denomina la Functional API)\n",
    "input_layer = keras.layers.Input(shape = (28,28))\n",
    "flatten_layer = keras.layers.Flatten()(input_layer)\n",
    "hidden_1 = keras.layers.Dense(300, activation = \"relu\")(flatten_layer)\n",
    "hidden_2 = keras.layers.Dense(100, activation = \"relu\")(hidden_1)\n",
    "output = keras.layers.Dense(10, activation = \"softmax\")(hidden_2)\n",
    "model = keras.Model(inputs = [input_layer], outputs = [output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La functional API me va a permitir construir redes como estas:\n",
    "\n",
    "<img src=\"./img/otras_arquitecturas.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.reshaping.flatten.Flatten object at 0x000001E66825FF70>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1e66825df60>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x1e66825ff70>,\n",
       " <keras.layers.core.dense.Dense at 0x1e66518b940>,\n",
       " <keras.layers.core.dense.Dense at 0x1e668264a90>,\n",
       " <keras.layers.core.dense.Dense at 0x1e6661e6cb0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[1])\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[2]\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "# 784 features (pixeles de las imagenes) x 300 neuronas\n",
    "# Los pesos están inicializados aleatoriamente\n",
    "weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializadores:  \n",
    "- Los pesos inicializados a cero -> No aprendizaje\n",
    "- Desde siempre se inicializan \"aleatoriamente\", pero no sólo de forma uniforme (todos los valores con la misma probabilidad), sino que se emplean diferentes distribuciones de probabilidad con parámetros que dependen del número de entradas y salidas de la capa. El objetivo esintentar que las varianzas de las entradas sean similares a las de las salidas y evitar el problema del gradiente que se desvanece (\"Vanishing Gradient\" problem):  \n",
    "    *   Glorot inizialization (por defecto la de Keras, con función uniforme de distribución) -> Para cuando tienes funciones de activación (ninguna, tanh, sigmoid, softmax, aunque también se usa por defecto :-) para casi todo) [Xavier Glorot & Yoshua Bengio]\n",
    "    *   He inizialization, -> Para cuando tienes ReLU, Leaky ReLU, ELU, GELU, Swish, Mish [He Kaiming et al.]\n",
    "    *   LeCunn inizialization -> Para cuando tienes SELU [Jean LeCunn]\n",
    "- Es un hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica la configuración del entrenamiento (optimizador, pérdida, métricas):\n",
    "model.compile(\n",
    "    # Stocastic gradient descent. El algoritmo para minimizar la loss function\n",
    "    # El stocastic va haciendo muestreo en cada evaluacion, no usa todo el dataset\n",
    "    # Podemos modificar el learning rate(0.01 por defecto) mediante el parametro lr\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer, con parámetros por defecto\n",
    "    \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    # sparse_categorical_crossentropy cuando tenemos un label en nuna columna\n",
    "    # Si lo tuviesemos en varias tipo dummy, cogeriamos categorical_crossentropy\n",
    "    # binary_crossentropy si es una neurona, clasi binario\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente (... casi, los parámetros del optimizador serán los que tenga por defecto)\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "# La primera hidden layer tiene 784 entradas x 300 salidas\n",
    "# Son los 235500 params = 783x300 + 300 (bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAPAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos construyendo nuestra __lista de capas__ (para guardar en el \"Toolbox\"):\n",
    "\n",
    "__Entrenables__:  \n",
    "__* Dense__ -> Capa completamente conectada a las neuronas de la capa anterior y a la posterior  \n",
    "    Hiperparámetros asociados:     \n",
    "        * units: Number of neurons, dimensionality of the output space  \n",
    "        * activation: Activation function to use. If you don't specify anything, no activation is applied  \n",
    "        * kernel_initializer: Initializer for the kernel weights matrix.  \n",
    "        * bias_initializer: Initializer for the bias vector. (Suelen inicializarse a cero)\n",
    "        * Kernel_regularizar: Los clásicos (L1,L2,...)\n",
    " \n",
    "__Funcionales__:       \n",
    "__* Input__ -> Capa para definir la forma de la entrada (shape), que se puede pasar como input_shape\n",
    "__* Flatten__ -> Capa que aplana (convierte su entrada en un vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones de activación interesantes: SELU (1.67*ELU) y Swish (también SiLU, o Sigmoid linear unit)... No entrar en pánico, vais a usar ReLU, Softmax y no-activation, y en algunos casos (quizás): sigmoid, tanh y las (x)LU (SELU, Siwsh,etc)\n",
    "\n",
    "<img src=\"./img/activation_functions.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMIZADORES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también vamos completando lista de hiperparámetros, estos asociados al \"Optimizador\"/\"Modelo\":  \n",
    "Tipo de optimizador:  \n",
    "* __SGD__, Gradient descent \"genérico\" (puedes añadirle \"momento\", es decir que a la hora de descontar el gradiente tenga en cuenta el vector medio de gradientes pasados)\n",
    "  \n",
    "\n",
    "* __Adagrad__, Hace gradient descent pero ajusta el gradiente para compensar las componentes de mayor valor numérico (es como evitar irse por las pendientes más inclinadas)... Es decir evita irse a mínimos locales al precio de enlentecer el entrenamiento.    \n",
    "\n",
    "* __RMSprop__, Versión de AdaGrad, pero considera principalmente los últimos valores del gradiente. Es decir, busca lo bueno de Adagrad reduciendo sus peligros.    \n",
    "\n",
    "* __Adam__, _Adaptative Moment Estimation_, combina RMSProp y el uso de momento. Es el rey actual (junto con sus versiones) para grandes cantidades de datos.    \n",
    "\n",
    "* __AdamW, Nadam, AdaMax__, variantes del anterior. \n",
    "\n",
    "Comparativa, donde * es malo y *** bueno (extraído del \"Hands-on Machine Learning with....\" de Aurelien Geron, 3a Edicion)\n",
    "\n",
    "<img src=\"./img/Comparativa-optimizadores.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Keras también permite:\n",
    "__Adadelta__ _(variante de Adagrad)_, __Adafactor__ y __Ftrl__\n",
    "\n",
    "      \n",
    "Hiperparámetros Genéricos:\n",
    "Learning Rate: Coeficiente aplicado al descenso de gradiente, como en otros modelos que ya hemos visto\n",
    "Asociados al Gradient Clipping: clipnorm, clipvalue, global_clipnorm\n",
    "\n",
    "Cada optimizador además puede tener sus propios hiperparámetros (ver: https://keras.io/api/optimizers/)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuciones de pérdida y métricas\n",
    "__Función de perdida__: La función a minimizar durante el entrenamiento (son las mismas que en otros modelos no Deep)  \n",
    "- Clasificación: En clases Keras -> __BinaryCrossEntropy, CategoricalCrossEntropy, SparseCategoricalCrossEntropy__  \n",
    "- Regresión: En clases Keras -> __MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, CosineSimilarity__   \n",
    "\n",
    "__Métricas__:  \n",
    "- Regresión: __MAE, MSE, MAPE__ :-)  \n",
    "- Clasificación: __Accuracy, Precision, Recall, f1, AuRoC__  \n",
    "\n",
    "¿Cuál es la diferencia entre Categorical y Sparse? ¿Por qué las funciones de pérdida son diferentes a las métricas en Clasificación? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El __batch_size__ es la cantidad de muestras que utiliza el SGD, y las __epochs__ son las iteraciones que realiza en el entrenamiento. (Son hiperparámetros de entrenamiento)    \n",
    "\n",
    "En una epoch se entrenan tantos batches como sea necesario para recorrer todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8929 - accuracy: 0.7747 - val_loss: 0.3990 - val_accuracy: 0.8945\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3726 - accuracy: 0.8979 - val_loss: 0.3075 - val_accuracy: 0.9129\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3071 - accuracy: 0.9136 - val_loss: 0.2677 - val_accuracy: 0.9231\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2728 - accuracy: 0.9228 - val_loss: 0.2418 - val_accuracy: 0.9317\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2485 - accuracy: 0.9293 - val_loss: 0.2257 - val_accuracy: 0.9357\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2285 - accuracy: 0.9353 - val_loss: 0.2113 - val_accuracy: 0.9410\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2117 - accuracy: 0.9394 - val_loss: 0.2003 - val_accuracy: 0.9454\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1977 - accuracy: 0.9439 - val_loss: 0.1977 - val_accuracy: 0.9457\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1851 - accuracy: 0.9467 - val_loss: 0.1794 - val_accuracy: 0.9508\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1738 - accuracy: 0.9507 - val_loss: 0.1658 - val_accuracy: 0.9543\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1636 - accuracy: 0.9531 - val_loss: 0.1591 - val_accuracy: 0.9557\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1546 - accuracy: 0.9556 - val_loss: 0.1522 - val_accuracy: 0.9591\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1462 - accuracy: 0.9584 - val_loss: 0.1467 - val_accuracy: 0.9603\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1389 - accuracy: 0.9608 - val_loss: 0.1414 - val_accuracy: 0.9609\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1319 - accuracy: 0.9628 - val_loss: 0.1345 - val_accuracy: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEn el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\\nSi vemos que ya no baja mas, no serán necesarias tantas epochs.\\nImprimera tantas lineas como epochs hayamos puesto\\n\\nTampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\\nUtil para datasets desbalanceados.\\n\\nEl loss que muestra es el categoricalcrossentropy\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64, # numero de muestras empleadas en el entrenamiento de SGD\n",
    "    epochs=15, # 1 por defecto. Insuficiente. Numero de vueltas del backpropagation\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    # En vez de validation data podemos usar el argumento validation_split=0.1\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1256 - accuracy: 0.9644 - val_loss: 0.1363 - val_accuracy: 0.9637\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1197 - accuracy: 0.9664 - val_loss: 0.1295 - val_accuracy: 0.9653\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1143 - accuracy: 0.9682 - val_loss: 0.1243 - val_accuracy: 0.9660\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1093 - accuracy: 0.9696 - val_loss: 0.1209 - val_accuracy: 0.9676\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1045 - accuracy: 0.9713 - val_loss: 0.1162 - val_accuracy: 0.9675\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1002 - accuracy: 0.9724 - val_loss: 0.1183 - val_accuracy: 0.9680\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9734 - val_loss: 0.1118 - val_accuracy: 0.9691\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9745 - val_loss: 0.1110 - val_accuracy: 0.9694\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0889 - accuracy: 0.9752 - val_loss: 0.1067 - val_accuracy: 0.9722\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0855 - accuracy: 0.9768 - val_loss: 0.1049 - val_accuracy: 0.9714\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0824 - accuracy: 0.9777 - val_loss: 0.1065 - val_accuracy: 0.9717\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0791 - accuracy: 0.9784 - val_loss: 0.1029 - val_accuracy: 0.9722\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0765 - accuracy: 0.9791 - val_loss: 0.1026 - val_accuracy: 0.9722\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0734 - accuracy: 0.9802 - val_loss: 0.1018 - val_accuracy: 0.9724\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9811 - val_loss: 0.0953 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e66142d210>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 15, 'steps': 782}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8653184175491333,\n",
       "  0.37718307971954346,\n",
       "  0.31273114681243896,\n",
       "  0.27795034646987915,\n",
       "  0.2524315118789673,\n",
       "  0.23230259120464325,\n",
       "  0.21500366926193237,\n",
       "  0.1999616175889969,\n",
       "  0.18660804629325867,\n",
       "  0.17514637112617493,\n",
       "  0.16470976173877716,\n",
       "  0.15538306534290314,\n",
       "  0.14697298407554626,\n",
       "  0.13917842507362366,\n",
       "  0.13235484063625336],\n",
       " 'accuracy': [0.791159987449646,\n",
       "  0.8963199853897095,\n",
       "  0.9111400246620178,\n",
       "  0.920520007610321,\n",
       "  0.9276400208473206,\n",
       "  0.9338200092315674,\n",
       "  0.9381999969482422,\n",
       "  0.9426400065422058,\n",
       "  0.9466000199317932,\n",
       "  0.9501199722290039,\n",
       "  0.9525600075721741,\n",
       "  0.9553599953651428,\n",
       "  0.9578999876976013,\n",
       "  0.9602400064468384,\n",
       "  0.9619799852371216],\n",
       " 'val_loss': [0.40332141518592834,\n",
       "  0.30681177973747253,\n",
       "  0.2741599977016449,\n",
       "  0.25080054998397827,\n",
       "  0.2291218787431717,\n",
       "  0.21533620357513428,\n",
       "  0.19957205653190613,\n",
       "  0.19031009078025818,\n",
       "  0.17903535068035126,\n",
       "  0.16796830296516418,\n",
       "  0.1651764214038849,\n",
       "  0.15260769426822662,\n",
       "  0.14798428118228912,\n",
       "  0.14240317046642303,\n",
       "  0.1459202766418457],\n",
       " 'val_accuracy': [0.8978000283241272,\n",
       "  0.911899983882904,\n",
       "  0.9210000038146973,\n",
       "  0.9265000224113464,\n",
       "  0.9343000054359436,\n",
       "  0.9377999901771545,\n",
       "  0.9441999793052673,\n",
       "  0.9472000002861023,\n",
       "  0.9506000280380249,\n",
       "  0.9537000060081482,\n",
       "  0.9550999999046326,\n",
       "  0.9577000141143799,\n",
       "  0.9595000147819519,\n",
       "  0.9610999822616577,\n",
       "  0.9602000117301941]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Podemos ver como evoluciona el entrenamiento, en funcion de los epochs\n",
    "# Validacion y training estan muy cerca, no hay overfitting!\n",
    "# Todavia no ha acabado de coverger ya que el loss en validacion sigue bajando,\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9716\n",
      "test loss, test acc: [0.0947565957903862, 0.9715999960899353]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el \"score\" a partir del conjunto de test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "\n",
    "# Metodo evaluate para que nos de el error vs las metricas elegidas en la funcion compile\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\glezr\\AppData\\Local\\Temp\\ipykernel_23048\\1468152043.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "predictions shape: (1, 10)\n",
      "[0.    0.    0.    0.002 0.    0.    0.    0.997 0.    0.   ]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ojo aqui viene slicing xq presupone que le entran varios inputs\n",
    "Nos da las probabilidades de pertenecer a una clase u otra.\n",
    "'''\n",
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona.\n",
    "     \n",
    "Vamos a configurar una red como esta:  \n",
    "<img src=\"./img/mlp_regresion.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8444 - root_mean_squared_error: 0.9189 - val_loss: 0.5773 - val_root_mean_squared_error: 0.7598\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5912 - root_mean_squared_error: 0.7689 - val_loss: 0.5255 - val_root_mean_squared_error: 0.7249\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4993 - root_mean_squared_error: 0.7066 - val_loss: 0.4911 - val_root_mean_squared_error: 0.7008\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4837 - root_mean_squared_error: 0.6955 - val_loss: 0.4753 - val_root_mean_squared_error: 0.6895\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4984 - root_mean_squared_error: 0.7060 - val_loss: 0.4604 - val_root_mean_squared_error: 0.6786\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4561 - root_mean_squared_error: 0.6753 - val_loss: 0.4551 - val_root_mean_squared_error: 0.6746\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4474 - root_mean_squared_error: 0.6689 - val_loss: 0.4428 - val_root_mean_squared_error: 0.6654\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4369 - root_mean_squared_error: 0.6610 - val_loss: 0.4343 - val_root_mean_squared_error: 0.6590\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4377 - root_mean_squared_error: 0.6616 - val_loss: 0.4310 - val_root_mean_squared_error: 0.6565\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4259 - root_mean_squared_error: 0.6526 - val_loss: 0.4279 - val_root_mean_squared_error: 0.6542\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4345 - root_mean_squared_error: 0.6591 - val_loss: 0.4223 - val_root_mean_squared_error: 0.6499\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4175 - root_mean_squared_error: 0.6462 - val_loss: 0.4196 - val_root_mean_squared_error: 0.6477\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4160 - root_mean_squared_error: 0.6449 - val_loss: 0.4117 - val_root_mean_squared_error: 0.6417\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4068 - root_mean_squared_error: 0.6378 - val_loss: 0.4038 - val_root_mean_squared_error: 0.6355\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4125 - root_mean_squared_error: 0.6422 - val_loss: 0.4090 - val_root_mean_squared_error: 0.6395\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4063 - root_mean_squared_error: 0.6374 - val_loss: 0.3991 - val_root_mean_squared_error: 0.6317\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3999 - root_mean_squared_error: 0.6324 - val_loss: 0.3952 - val_root_mean_squared_error: 0.6286\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3938 - root_mean_squared_error: 0.6276 - val_loss: 0.3956 - val_root_mean_squared_error: 0.6290\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3968 - root_mean_squared_error: 0.6299 - val_loss: 0.3897 - val_root_mean_squared_error: 0.6243\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3941 - root_mean_squared_error: 0.6278 - val_loss: 0.3845 - val_root_mean_squared_error: 0.6200\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3721 - root_mean_squared_error: 0.6100\n",
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = [\"RootMeanSquaredError\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.3720649480819702, 0.609971284866333]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7822 - root_mean_squared_error: 0.8844 - mean_absolute_error: 0.6157 - val_loss: 0.5050 - val_root_mean_squared_error: 0.7107 - val_mean_absolute_error: 0.5051\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4596 - root_mean_squared_error: 0.6779 - mean_absolute_error: 0.4842 - val_loss: 3.0211 - val_root_mean_squared_error: 1.7381 - val_mean_absolute_error: 0.5014\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4249 - root_mean_squared_error: 0.6519 - mean_absolute_error: 0.4667 - val_loss: 0.5558 - val_root_mean_squared_error: 0.7455 - val_mean_absolute_error: 0.4587\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4005 - root_mean_squared_error: 0.6328 - mean_absolute_error: 0.4527 - val_loss: 0.5774 - val_root_mean_squared_error: 0.7599 - val_mean_absolute_error: 0.4566\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4000 - root_mean_squared_error: 0.6325 - mean_absolute_error: 0.4510 - val_loss: 0.5919 - val_root_mean_squared_error: 0.7693 - val_mean_absolute_error: 0.4503\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3911 - root_mean_squared_error: 0.6254 - mean_absolute_error: 0.4454 - val_loss: 9.4587 - val_root_mean_squared_error: 3.0755 - val_mean_absolute_error: 0.4971\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3806 - root_mean_squared_error: 0.6169 - mean_absolute_error: 0.4402 - val_loss: 0.4087 - val_root_mean_squared_error: 0.6393 - val_mean_absolute_error: 0.4401\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3865 - root_mean_squared_error: 0.6217 - mean_absolute_error: 0.4385 - val_loss: 0.4787 - val_root_mean_squared_error: 0.6919 - val_mean_absolute_error: 0.4345\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3704 - root_mean_squared_error: 0.6086 - mean_absolute_error: 0.4325 - val_loss: 0.4557 - val_root_mean_squared_error: 0.6751 - val_mean_absolute_error: 0.4300\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3653 - root_mean_squared_error: 0.6044 - mean_absolute_error: 0.4287 - val_loss: 0.4046 - val_root_mean_squared_error: 0.6361 - val_mean_absolute_error: 0.4413\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3619 - root_mean_squared_error: 0.6016 - mean_absolute_error: 0.4267 - val_loss: 0.3635 - val_root_mean_squared_error: 0.6030 - val_mean_absolute_error: 0.4201\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3593 - root_mean_squared_error: 0.5994 - mean_absolute_error: 0.4242 - val_loss: 1.1444 - val_root_mean_squared_error: 1.0698 - val_mean_absolute_error: 0.4391\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3936 - root_mean_squared_error: 0.6274 - mean_absolute_error: 0.4288 - val_loss: 0.8275 - val_root_mean_squared_error: 0.9097 - val_mean_absolute_error: 0.4364\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3598 - root_mean_squared_error: 0.5998 - mean_absolute_error: 0.4237 - val_loss: 0.3665 - val_root_mean_squared_error: 0.6054 - val_mean_absolute_error: 0.4232\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3552 - root_mean_squared_error: 0.5960 - mean_absolute_error: 0.4212 - val_loss: 0.5040 - val_root_mean_squared_error: 0.7099 - val_mean_absolute_error: 0.4218\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - root_mean_squared_error: 0.5963 - mean_absolute_error: 0.4202 - val_loss: 0.4757 - val_root_mean_squared_error: 0.6897 - val_mean_absolute_error: 0.4204\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3613 - root_mean_squared_error: 0.6011 - mean_absolute_error: 0.4211 - val_loss: 0.5700 - val_root_mean_squared_error: 0.7550 - val_mean_absolute_error: 0.4250\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3506 - root_mean_squared_error: 0.5921 - mean_absolute_error: 0.4180 - val_loss: 1.0658 - val_root_mean_squared_error: 1.0324 - val_mean_absolute_error: 0.4347\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3527 - root_mean_squared_error: 0.5939 - mean_absolute_error: 0.4178 - val_loss: 1.7464 - val_root_mean_squared_error: 1.3215 - val_mean_absolute_error: 0.4480\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3494 - root_mean_squared_error: 0.5911 - mean_absolute_error: 0.4166 - val_loss: 0.6268 - val_root_mean_squared_error: 0.7917 - val_mean_absolute_error: 0.4196\n",
      "162/162 [==============================] - 0s 831us/step - loss: 0.4404 - root_mean_squared_error: 0.6636 - mean_absolute_error: 0.4259\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E6611093F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    }
   ],
   "source": [
    "#Otra forma pure-Keras:\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "\n",
    "norm_layer = keras.layers.Normalization(input_shape = X_train.shape[1:]) # Es una Standardization\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    norm_layer,\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=optimizer, metrics = [\"RootMeanSquaredError\",\"MeanAbsoluteError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.36620524525642395, 0.60514897108078, 0.41656115651130676]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva capa al toolbox:\n",
    "\n",
    "Funcionales:  \n",
    "__Normalize__: keras.layers.Normalization -> Nos hace la standardizacion de la entrada \n",
    "Hay que ejecutar el metodo Adapt antes de llamar al fit del modelo que incluya la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es emplear el formato TensorFlow. En este caso crea un directorio con varios ficheros que facilita el despliegue en algunas aplicaciones (ojo, que habría que llevar a producción todo el directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k.save(\"my_model_tf_format\", save_format= \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model_k= keras.models.load_model(\"my_model_tf_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3507 - root_mean_squared_error: 0.5922 - mean_absolute_error: 0.4171\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3453 - root_mean_squared_error: 0.5876 - mean_absolute_error: 0.4148\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - root_mean_squared_error: 0.5955 - mean_absolute_error: 0.4169\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3465 - root_mean_squared_error: 0.5886 - mean_absolute_error: 0.4140\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3442 - root_mean_squared_error: 0.5867 - mean_absolute_error: 0.4121\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3421 - root_mean_squared_error: 0.5849 - mean_absolute_error: 0.4118\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3754 - root_mean_squared_error: 0.6127 - mean_absolute_error: 0.4169\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3395 - root_mean_squared_error: 0.5827 - mean_absolute_error: 0.4103\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.3402 - root_mean_squared_error: 0.5832 - mean_absolute_error: 0.4101\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3387 - root_mean_squared_error: 0.5820 - mean_absolute_error: 0.4082\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3472 - root_mean_squared_error: 0.5892 - mean_absolute_error: 0.4113 - val_loss: 0.9012 - val_root_mean_squared_error: 0.9493 - val_mean_absolute_error: 0.4161\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3401 - root_mean_squared_error: 0.5832 - mean_absolute_error: 0.4089 - val_loss: 0.6783 - val_root_mean_squared_error: 0.8236 - val_mean_absolute_error: 0.4164\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3580 - root_mean_squared_error: 0.5984 - mean_absolute_error: 0.4117 - val_loss: 0.4596 - val_root_mean_squared_error: 0.6780 - val_mean_absolute_error: 0.4186\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3347 - root_mean_squared_error: 0.5786 - mean_absolute_error: 0.4063 - val_loss: 0.4403 - val_root_mean_squared_error: 0.6635 - val_mean_absolute_error: 0.4105\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3343 - root_mean_squared_error: 0.5782 - mean_absolute_error: 0.4052 - val_loss: 0.3436 - val_root_mean_squared_error: 0.5862 - val_mean_absolute_error: 0.3989\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3337 - root_mean_squared_error: 0.5776 - mean_absolute_error: 0.4052 - val_loss: 0.3440 - val_root_mean_squared_error: 0.5865 - val_mean_absolute_error: 0.4078\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3322 - root_mean_squared_error: 0.5763 - mean_absolute_error: 0.4043 - val_loss: 0.3934 - val_root_mean_squared_error: 0.6272 - val_mean_absolute_error: 0.4067\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3494 - root_mean_squared_error: 0.5911 - mean_absolute_error: 0.4074 - val_loss: 0.5454 - val_root_mean_squared_error: 0.7385 - val_mean_absolute_error: 0.4021\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3356 - root_mean_squared_error: 0.5793 - mean_absolute_error: 0.4059 - val_loss: 1.1986 - val_root_mean_squared_error: 1.0948 - val_mean_absolute_error: 0.4176\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3392 - root_mean_squared_error: 0.5824 - mean_absolute_error: 0.4067 - val_loss: 1.5413 - val_root_mean_squared_error: 1.2415 - val_mean_absolute_error: 0.4130\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3368 - root_mean_squared_error: 0.5803 - mean_absolute_error: 0.4053 - val_loss: 0.4166 - val_root_mean_squared_error: 0.6454 - val_mean_absolute_error: 0.4024\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3381 - root_mean_squared_error: 0.5815 - mean_absolute_error: 0.4050 - val_loss: 1.6075 - val_root_mean_squared_error: 1.2679 - val_mean_absolute_error: 0.4194\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - root_mean_squared_error: 0.5786 - mean_absolute_error: 0.4038 - val_loss: 0.4026 - val_root_mean_squared_error: 0.6345 - val_mean_absolute_error: 0.4053\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - root_mean_squared_error: 0.5725 - mean_absolute_error: 0.4015 - val_loss: 0.8785 - val_root_mean_squared_error: 0.9373 - val_mean_absolute_error: 0.4091\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.3659 - root_mean_squared_error: 0.6049 - mean_absolute_error: 0.4077 - val_loss: 5.7623 - val_root_mean_squared_error: 2.4005 - val_mean_absolute_error: 0.4450\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3332 - root_mean_squared_error: 0.5772 - mean_absolute_error: 0.4044 - val_loss: 12.4290 - val_root_mean_squared_error: 3.5255 - val_mean_absolute_error: 0.4571\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4006 - root_mean_squared_error: 0.6329 - mean_absolute_error: 0.4199 - val_loss: 1.2226 - val_root_mean_squared_error: 1.1057 - val_mean_absolute_error: 0.4144\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.3307 - root_mean_squared_error: 0.5751 - mean_absolute_error: 0.4034 - val_loss: 0.4321 - val_root_mean_squared_error: 0.6573 - val_mean_absolute_error: 0.4056\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3435 - root_mean_squared_error: 0.5861 - mean_absolute_error: 0.4047 - val_loss: 0.3588 - val_root_mean_squared_error: 0.5990 - val_mean_absolute_error: 0.4020\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - root_mean_squared_error: 0.5794 - mean_absolute_error: 0.4038 - val_loss: 0.3392 - val_root_mean_squared_error: 0.5824 - val_mean_absolute_error: 0.3914\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - root_mean_squared_error: 0.5733 - mean_absolute_error: 0.4013 - val_loss: 0.5852 - val_root_mean_squared_error: 0.7650 - val_mean_absolute_error: 0.4080\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3264 - root_mean_squared_error: 0.5713 - mean_absolute_error: 0.4009 - val_loss: 0.8560 - val_root_mean_squared_error: 0.9252 - val_mean_absolute_error: 0.4109\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3250 - root_mean_squared_error: 0.5701 - mean_absolute_error: 0.4002 - val_loss: 0.4237 - val_root_mean_squared_error: 0.6509 - val_mean_absolute_error: 0.4013\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3296 - root_mean_squared_error: 0.5741 - mean_absolute_error: 0.4011 - val_loss: 3.3994 - val_root_mean_squared_error: 1.8437 - val_mean_absolute_error: 0.4250\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - root_mean_squared_error: 0.5710 - mean_absolute_error: 0.4003 - val_loss: 0.3760 - val_root_mean_squared_error: 0.6132 - val_mean_absolute_error: 0.3941\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3278 - root_mean_squared_error: 0.5726 - mean_absolute_error: 0.3996 - val_loss: 0.3306 - val_root_mean_squared_error: 0.5750 - val_mean_absolute_error: 0.3906\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3371 - root_mean_squared_error: 0.5806 - mean_absolute_error: 0.4016 - val_loss: 0.6167 - val_root_mean_squared_error: 0.7853 - val_mean_absolute_error: 0.4027\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3238 - root_mean_squared_error: 0.5690 - mean_absolute_error: 0.3995 - val_loss: 0.9647 - val_root_mean_squared_error: 0.9822 - val_mean_absolute_error: 0.4071\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3257 - root_mean_squared_error: 0.5707 - mean_absolute_error: 0.3997 - val_loss: 0.6116 - val_root_mean_squared_error: 0.7821 - val_mean_absolute_error: 0.4010\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3195 - root_mean_squared_error: 0.5652 - mean_absolute_error: 0.3968 - val_loss: 0.6452 - val_root_mean_squared_error: 0.8033 - val_mean_absolute_error: 0.4026\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3229 - root_mean_squared_error: 0.5683 - mean_absolute_error: 0.3987 - val_loss: 0.4284 - val_root_mean_squared_error: 0.6545 - val_mean_absolute_error: 0.4111\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "10 esta bien. Lo pondemos a 5 para el ejercicio\n",
    "¿Qué considera como dejar de mejorar? parametros min_delta y baseline\n",
    "'''\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, \n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guía \"casera\":\n",
    "\n",
    "1- Recetas (adaptada de \"Hands-on...\")  PARA MLPs!!!!    \n",
    "    * Capas:   \n",
    "        - Empezar con una capa oculta e ir añadiendo (dependiendo de la complejidad del problema, probar wide & deep)\n",
    "        - Si pocas features -> más neuronas  (aumentar la combinación de features) (num_features < 100) [Orientativo]  \n",
    "        - Si muchas features  -> menos neuronas (proyección tipo PCA) (num_features > 1000) [Orientativo] e ir aumentando en capas sucesivas\n",
    "        - O empezar con muchas (doble de tus features e ir \"estrechando los pantalones\")  \n",
    "        - Construcción en prisma o pirámide (para empezar)  \n",
    "        - Inicialización: Empezar con Glorot, cambiar a He  \n",
    "        - Activación: ReLU salvo la última, si muchas capas probar -> SELU o Swish (con el inicializador a LeCunn) \n",
    "    * Optimizadores:   \n",
    "        - Si muchos datos*features -> Adam o AdamW con sus valores por defecto  \n",
    "        - Si no, SGD con Nesterov activado, y momento a 0.9  \n",
    "        - Learning rate -> 0.001-0.0001 para empezar e ir creciendo (learning-rate warm-up) (Si te atreves, buscar adaptative learning rate y optimizar con esto)  \n",
    "    * Entrenamiento:  \n",
    "        - Epoch, probar con pocas para ver duración -> Epochs altas y Callback de Early Stop activado  \n",
    "        - Batch_Size -> 32, si tienes muchos datos y una GPU a mano puedes subir mucho 64,128,256...\n",
    "    * Regularización (lo veremos):  \n",
    "        - Dropout al 0.25-0.5 (sin SELU)\n",
    "\n",
    "\n",
    "\n",
    "2- Pasos  \n",
    "    - Si overfitting -> Regularizar: Earlystopping, Dropout (lo veremos en la siguiente sección)  \n",
    "    - Comprobar underfitting -> Aumentar epochs, aumentar batch_size  \n",
    "    - Jugar con optimizador: learning rate (de pequeño a grande), tipo de optimizador   \n",
    "    - Jugar con número de capas (ojo al overfitting) y las funciones de activación y la inicialización de pesos  \n",
    "    - Jugar con el número de neuronas por capa (suele ser piramide o prisma, pero puedes jugar a expandir dimensiones)  \n",
    "    - Combinar los dos anteriores  \n",
    "\n",
    "3- Keras Tuner:\n",
    "    https://keras.io/guides/keras_tuner/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Keras tiene implementado un dashboard para monitorizar las ejecuciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crea este directorio\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# Guarda una carpeta nueva con la fecha de la ejecucion\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3426 - root_mean_squared_error: 0.5853 - mean_absolute_error: 0.4023 - val_loss: 0.3316 - val_root_mean_squared_error: 0.5758 - val_mean_absolute_error: 0.3944\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3236 - root_mean_squared_error: 0.5689 - mean_absolute_error: 0.3985 - val_loss: 0.3906 - val_root_mean_squared_error: 0.6250 - val_mean_absolute_error: 0.3997\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3322 - root_mean_squared_error: 0.5764 - mean_absolute_error: 0.3997 - val_loss: 0.3530 - val_root_mean_squared_error: 0.5941 - val_mean_absolute_error: 0.3966\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3268 - root_mean_squared_error: 0.5716 - mean_absolute_error: 0.3990 - val_loss: 0.6532 - val_root_mean_squared_error: 0.8082 - val_mean_absolute_error: 0.4061\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3210 - root_mean_squared_error: 0.5666 - mean_absolute_error: 0.3979 - val_loss: 0.3530 - val_root_mean_squared_error: 0.5942 - val_mean_absolute_error: 0.3943\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3196 - root_mean_squared_error: 0.5653 - mean_absolute_error: 0.3973 - val_loss: 0.5185 - val_root_mean_squared_error: 0.7201 - val_mean_absolute_error: 0.4008\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3246 - root_mean_squared_error: 0.5698 - mean_absolute_error: 0.3988 - val_loss: 0.3324 - val_root_mean_squared_error: 0.5765 - val_mean_absolute_error: 0.4023\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3243 - root_mean_squared_error: 0.5694 - mean_absolute_error: 0.3986 - val_loss: 0.3725 - val_root_mean_squared_error: 0.6103 - val_mean_absolute_error: 0.3983\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3193 - root_mean_squared_error: 0.5650 - mean_absolute_error: 0.3967 - val_loss: 0.3415 - val_root_mean_squared_error: 0.5844 - val_mean_absolute_error: 0.3978\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3217 - root_mean_squared_error: 0.5672 - mean_absolute_error: 0.3965 - val_loss: 0.3313 - val_root_mean_squared_error: 0.5756 - val_mean_absolute_error: 0.3986\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3169 - root_mean_squared_error: 0.5630 - mean_absolute_error: 0.3954 - val_loss: 0.3381 - val_root_mean_squared_error: 0.5814 - val_mean_absolute_error: 0.4007\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3316 - root_mean_squared_error: 0.5758 - mean_absolute_error: 0.3986 - val_loss: 0.3912 - val_root_mean_squared_error: 0.6255 - val_mean_absolute_error: 0.3944\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3274 - root_mean_squared_error: 0.5722 - mean_absolute_error: 0.3984 - val_loss: 0.3435 - val_root_mean_squared_error: 0.5861 - val_mean_absolute_error: 0.4016\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3204 - root_mean_squared_error: 0.5660 - mean_absolute_error: 0.3967 - val_loss: 0.3579 - val_root_mean_squared_error: 0.5982 - val_mean_absolute_error: 0.3991\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3254 - root_mean_squared_error: 0.5704 - mean_absolute_error: 0.3960 - val_loss: 0.6375 - val_root_mean_squared_error: 0.7984 - val_mean_absolute_error: 0.4443\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3446 - root_mean_squared_error: 0.5870 - mean_absolute_error: 0.3989 - val_loss: 0.3655 - val_root_mean_squared_error: 0.6046 - val_mean_absolute_error: 0.3948\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3192 - root_mean_squared_error: 0.5650 - mean_absolute_error: 0.3962 - val_loss: 0.3241 - val_root_mean_squared_error: 0.5693 - val_mean_absolute_error: 0.3914\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3159 - root_mean_squared_error: 0.5621 - mean_absolute_error: 0.3956 - val_loss: 0.3749 - val_root_mean_squared_error: 0.6123 - val_mean_absolute_error: 0.3983\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3193 - root_mean_squared_error: 0.5651 - mean_absolute_error: 0.3957 - val_loss: 0.6950 - val_root_mean_squared_error: 0.8337 - val_mean_absolute_error: 0.4059\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - root_mean_squared_error: 0.5697 - mean_absolute_error: 0.3969 - val_loss: 0.4134 - val_root_mean_squared_error: 0.6430 - val_mean_absolute_error: 0.3945\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3252 - root_mean_squared_error: 0.5702 - mean_absolute_error: 0.3957 - val_loss: 0.4990 - val_root_mean_squared_error: 0.7064 - val_mean_absolute_error: 0.3946\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3166 - root_mean_squared_error: 0.5627 - mean_absolute_error: 0.3942 - val_loss: 0.4007 - val_root_mean_squared_error: 0.6330 - val_mean_absolute_error: 0.4013\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3203 - root_mean_squared_error: 0.5660 - mean_absolute_error: 0.3954 - val_loss: 0.4133 - val_root_mean_squared_error: 0.6429 - val_mean_absolute_error: 0.4031\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3333 - root_mean_squared_error: 0.5773 - mean_absolute_error: 0.3994 - val_loss: 0.5785 - val_root_mean_squared_error: 0.7606 - val_mean_absolute_error: 0.3990\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3308 - root_mean_squared_error: 0.5752 - mean_absolute_error: 0.3981 - val_loss: 0.4523 - val_root_mean_squared_error: 0.6725 - val_mean_absolute_error: 0.4032\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3141 - root_mean_squared_error: 0.5604 - mean_absolute_error: 0.3937 - val_loss: 0.3241 - val_root_mean_squared_error: 0.5693 - val_mean_absolute_error: 0.3895\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3184 - root_mean_squared_error: 0.5643 - mean_absolute_error: 0.3962 - val_loss: 0.3284 - val_root_mean_squared_error: 0.5730 - val_mean_absolute_error: 0.3870\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3119 - root_mean_squared_error: 0.5585 - mean_absolute_error: 0.3929 - val_loss: 0.3767 - val_root_mean_squared_error: 0.6138 - val_mean_absolute_error: 0.3924\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3132 - root_mean_squared_error: 0.5596 - mean_absolute_error: 0.3925 - val_loss: 0.3307 - val_root_mean_squared_error: 0.5750 - val_mean_absolute_error: 0.3883\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3123 - root_mean_squared_error: 0.5588 - mean_absolute_error: 0.3924 - val_loss: 0.4302 - val_root_mean_squared_error: 0.6559 - val_mean_absolute_error: 0.3922\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3113 - root_mean_squared_error: 0.5580 - mean_absolute_error: 0.3921 - val_loss: 0.4257 - val_root_mean_squared_error: 0.6524 - val_mean_absolute_error: 0.3933\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3117 - root_mean_squared_error: 0.5583 - mean_absolute_error: 0.3921 - val_loss: 0.3827 - val_root_mean_squared_error: 0.6186 - val_mean_absolute_error: 0.3935\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3131 - root_mean_squared_error: 0.5595 - mean_absolute_error: 0.3920 - val_loss: 0.5692 - val_root_mean_squared_error: 0.7544 - val_mean_absolute_error: 0.3982\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3094 - root_mean_squared_error: 0.5562 - mean_absolute_error: 0.3909 - val_loss: 0.4725 - val_root_mean_squared_error: 0.6874 - val_mean_absolute_error: 0.3921\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3091 - root_mean_squared_error: 0.5560 - mean_absolute_error: 0.3906 - val_loss: 0.3202 - val_root_mean_squared_error: 0.5658 - val_mean_absolute_error: 0.3902\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3107 - root_mean_squared_error: 0.5574 - mean_absolute_error: 0.3913 - val_loss: 0.3450 - val_root_mean_squared_error: 0.5874 - val_mean_absolute_error: 0.3892\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3092 - root_mean_squared_error: 0.5561 - mean_absolute_error: 0.3904 - val_loss: 1.7706 - val_root_mean_squared_error: 1.3306 - val_mean_absolute_error: 0.4071\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3133 - root_mean_squared_error: 0.5597 - mean_absolute_error: 0.3922 - val_loss: 0.7380 - val_root_mean_squared_error: 0.8591 - val_mean_absolute_error: 0.3934\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3095 - root_mean_squared_error: 0.5563 - mean_absolute_error: 0.3906 - val_loss: 0.3655 - val_root_mean_squared_error: 0.6046 - val_mean_absolute_error: 0.3957\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3133 - root_mean_squared_error: 0.5598 - mean_absolute_error: 0.3898 - val_loss: 0.8575 - val_root_mean_squared_error: 0.9260 - val_mean_absolute_error: 0.4076\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3080 - root_mean_squared_error: 0.5550 - mean_absolute_error: 0.3893 - val_loss: 0.3990 - val_root_mean_squared_error: 0.6317 - val_mean_absolute_error: 0.3950\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3083 - root_mean_squared_error: 0.5552 - mean_absolute_error: 0.3897 - val_loss: 0.3589 - val_root_mean_squared_error: 0.5991 - val_mean_absolute_error: 0.3890\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3093 - root_mean_squared_error: 0.5561 - mean_absolute_error: 0.3899 - val_loss: 0.3210 - val_root_mean_squared_error: 0.5666 - val_mean_absolute_error: 0.3921\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3124 - root_mean_squared_error: 0.5589 - mean_absolute_error: 0.3906 - val_loss: 0.6128 - val_root_mean_squared_error: 0.7828 - val_mean_absolute_error: 0.3933\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3090 - root_mean_squared_error: 0.5559 - mean_absolute_error: 0.3893 - val_loss: 0.3593 - val_root_mean_squared_error: 0.5994 - val_mean_absolute_error: 0.3902\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3061 - root_mean_squared_error: 0.5532 - mean_absolute_error: 0.3881 - val_loss: 0.4245 - val_root_mean_squared_error: 0.6515 - val_mean_absolute_error: 0.3947\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3155 - root_mean_squared_error: 0.5617 - mean_absolute_error: 0.3905 - val_loss: 1.0757 - val_root_mean_squared_error: 1.0372 - val_mean_absolute_error: 0.3999\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3069 - root_mean_squared_error: 0.5540 - mean_absolute_error: 0.3891 - val_loss: 0.3848 - val_root_mean_squared_error: 0.6203 - val_mean_absolute_error: 0.3909\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3055 - root_mean_squared_error: 0.5528 - mean_absolute_error: 0.3872 - val_loss: 0.3313 - val_root_mean_squared_error: 0.5756 - val_mean_absolute_error: 0.3939\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3063 - root_mean_squared_error: 0.5534 - mean_absolute_error: 0.3883 - val_loss: 0.3223 - val_root_mean_squared_error: 0.5677 - val_mean_absolute_error: 0.3968\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPara lanzarlo desde el jupyter notebook\\n%load_ext tensorboard\\n%tensorboard --logdir=./my_logs --port=6006\\n\\nPara lanzarlo desde el terminal, hay que estar en la carpeta de los logs\\ntensorboard --logdir=./my_logs --port=6006\\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Para lanzarlo desde el jupyter notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "Para lanzarlo desde el terminal, hay que estar en la carpeta de los logs\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "167a7833a0358ac30a26ad970c5914014f41a5348f3dc652232a762d6e3283fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
